[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "Packages & contributions\n\n\n\n\n\n\n\nR package system requirements\n\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenius (retired)\n\n\n\npackage\n\n\nmusic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nh3o for H3 indexing\n\n\n\nspatial\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspdep (contributor)\n\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nThings I’ve written\n\n\n\n\n\n\n\nR for Progressive Campaigns\n\n\n\nwriting\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban Informatics Toolkit\n\n\n\nwriting\n\n\nurban-informatics\n\n\ntextbook\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmirrr - song genre classification\n\n\n\nwriting\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "projects"
    ]
  },
  {
    "objectID": "projects/writing/mirr.html",
    "href": "projects/writing/mirr.html",
    "title": "mirrr - song genre classification",
    "section": "",
    "text": "Tidy Music Information Retrieval was a bookdown project I wrote back in 2019 that created a stacked ensemble model that predicted musical genre from song audio features and song lyrics.\nI created 3 models. The first utilized LDA text classification outputs from song lyrics as inputs into a classification model. The second used song audio features from spotify. The third model used the outputs of both to create a stacked ensemble model."
  },
  {
    "objectID": "projects/pkgs/genius.html",
    "href": "projects/pkgs/genius.html",
    "title": "genius (retired)",
    "section": "",
    "text": "genius was my first R package and my first digital child. genius provided a way to programatically access song lyrics from genius.com. This included ways to fetch single songs, albums, and track lists. It was, at one point, integrated with the spotifyr package.\nThis R package was the basis of much of my learning of the data science ecosystem. Including APIs and Docker. Creating stacked ensemble machine learning models using LDA outputs as model inputs (see online guide)\n\n GitHub"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html",
    "href": "posts/2023-11-10-enums-in-r/index.html",
    "title": "Enums in R: towards type safe R",
    "section": "",
    "text": "Hadley Wickham has recently dropped a new draft section of his book Tidy Design Principles on enumerations and their use in R.\nIn short, enumerations enumerate (list out) the possible values that something might take on. In R we see this most often in function signatures where an argument takes a scalar value but all possible values are listed out."
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#enums-in-r",
    "href": "posts/2023-11-10-enums-in-r/index.html#enums-in-r",
    "title": "Enums in R: towards type safe R",
    "section": "Enums in R",
    "text": "Enums in R\nA good example is the cor() function from the base package stats.\n\nargs(cor)\n\nfunction (x, y = NULL, use = \"everything\", method = c(\"pearson\", \n    \"kendall\", \"spearman\")) \nNULL\n\n\nThe possible values for method are \"pearson\", \"kendall\", or \"spearman\" but all values are listed inside of the function definition.\nInside of the function, though, match.arg(method) is used to ensure that the provided value to the method argument is one of the provided values.\nHadley makes the argument that we should prefer an enumeration to a boolean flag such as TRUE or FALSE. I agree!\nA real world example\nA post on mastodon makes a point that the function sf::st_make_grid() has an argument square = TRUE where when set to FALSE hexagons are returned.\n\n\nIn this case, it’s very clear that an enum would be better! For example we can improve the signature like so:\nst_make_grid &lt;- function(x, grid_shape = c(\"square\", \"hexagon\"), ...) {\n  # ensure only one of the provided grid shapes are used\n  match.arg(grid_shape)\n  # ... rest of function \n}"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#enums-in-rust",
    "href": "posts/2023-11-10-enums-in-r/index.html#enums-in-rust",
    "title": "Enums in R: towards type safe R",
    "section": "Enums in Rust",
    "text": "Enums in Rust\nWhen I first started using rust enums made no sense to me. In Rust, enums are a first class citizen that are treated as their own thing.\n\n\nI’m not really sure what to call things in Rust. Are they all objects?\nWe make them by defining the name of the enum and the variants they may take on.\nenum GridShape {\n  Square,\n  Hexagon\n}\nNow you can use this enum GridShape to specify one of two types: Square or Hexagon. Syntactically, this is written GridShape::Square and GridShape::Hexagon.\nEnums are very nice because we can match on the variants and do different things based on them. For example we can have a function like so:\nfn which_shape(x: GridShape) {\n    match x {\n        GridShape::Square =&gt; println!(\"We have a square!\"),\n        GridShape::Hexagon =&gt; println!(\"Hexagons are the bestagons\")\n    }\n}\nIt takes an argument x which is a GridShape enum. We match on the possible variants and then do something.\n\n\nInside of the match statement each of the possible variants of the enum have to be written out. These are called match arms. The left side lists the variant where as the right portion (after =&gt;) indicates what will be executed if the left side is matched (essentially if the condition is true).\nWith this function we can pass in specific variants and get different behavior.\n\n\nGridShape::Hexagon\nGridShape::Square\n\n\n\nwhich_shape(GridShape::Hexagon)\n#&gt; Hexagons are the bestagons\n\n\nwhich_shape(GridShape::Square)\n#&gt; We have a square!"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#making-an-s7-enum-object-in-r",
    "href": "posts/2023-11-10-enums-in-r/index.html#making-an-s7-enum-object-in-r",
    "title": "Enums in R: towards type safe R",
    "section": "Making an S7 enum object in R",
    "text": "Making an S7 enum object in R\nI think R would benefit from having a “real” enum type object. Having a character vector of valid variants and checking against them using match.arg() or rlang::arg_match() is great but I think we can go further.\n\n\nSince learning Rust, I think having more strictness can make our code much better and more robust. I think adding enums would be a good step towards that\nI’ve prototyped an Enum type in R using the new S7 object system that might point us towards what an enum object in the future might look like for R users.\nDesign of an Enum\nFor an enum we need to know what the valid variants are and what the current value of the enum is. These would be the two properties.\nAn enum S7 object must also make sure that a value of an Enum is one of the valid variants. Using the GridShape enum the valid variants would be \"Square\" and \"Hexagon\". A GridShape enum could not take, for example, \"Circle\" since it is not a listed variant.\nUsing an abstract class\nTo start, we will create an abstract S7 class called Enum.\n\n“_an abstract class is a generic class (or type of object) used as a basis for creating specific objects that conform to its protocol, or the set of operations it supports” — Source\n\nThe Enum class will be used to create other Enum objects.\n\nlibrary(S7)\n\n# create a new Enum abstract class\nEnum &lt;- new_class(\n  \"Enum\",\n  properties = list(\n    Value = class_character,\n    Variants = class_character\n  ),\n  validator = function(self) { \n    if (length(self@Value) != 1L) {\n      \"enum value's are length 1\"\n    } else if (!(self@Value %in% self@Variants)) {\n      \"enum value must be one of possible variants\"\n    }\n  }, \n  abstract = TRUE\n)\n\nIn this code chunk we specify that there are 2 properties: Value and Variant each must be a character type. Value will be the value of the enum. It would be the right hand side of GridShape::Square in Rust’s enum, for example. Variants is a character vector of all of the possible values it may be able to take on. The validator ensures that Value must only have 1 value. It also ensures that Value is one of the enumerated Variants. This Enum class will be used to generate other enums and cannot be instantiated by itself.\nWe can create a new enum factory function with the arguments:\n\n\nenum_class the class of the enum we are creating\n\nvariants a character vector of the valid variant values\n\n\n# create a new enum constructor \nnew_enum_class &lt;- function(enum_class, variants) {\n  new_class(\n    enum_class,\n    parent = Enum,\n    properties = list(\n      Value = class_character,\n      Variants = new_property(class_character, default = variants)\n    ),\n    constructor = function(Value) {\n      new_object(S7_object(), Value = Value, Variants = variants)\n    }\n  )\n}\n\n\n\nNote that the constructor here only takes a Value argument. We do this so that users cannot circumvent the pre-defined variants.\nWith this we can now create a GridShape enum in R!\n\nGridShape &lt;- new_enum_class(\n  \"GridShape\",\n  c(\"Square\", \"Hexagon\")\n)\n\nGridShape\n\n&lt;GridShape&gt; class\n@ parent     : &lt;Enum&gt;\n@ constructor: function(Value) {...}\n@ validator  : &lt;NULL&gt;\n@ properties :\n $ Value   : &lt;character&gt;\n $ Variants: &lt;character&gt;\n\n\nThis new object will construct new GridShape enums for us.\n\nGridShape(\"Square\")\n\n&lt;GridShape&gt;\n @ Value   : chr \"Square\"\n @ Variants: chr [1:2] \"Square\" \"Hexagon\"\n\n\nWhen we try to create a GridShape that is not one of the valid variants we will get an error.\n\nGridShape(\"Triangle\")\n\nError: &lt;GridShape&gt; object is invalid:\n- enum value must be one of possible variants\n\n\nMaking a print method\nFor fun, I would like Enum objects to print like how I would use them in Rust. To do this we can create a custom print method\n\n# print method for enums\n# since this is an abstract class we get the first class (super)\n# to print\nprint.Enum &lt;- function(x, ...) {\n  cat(class(x)[1], \"::\", x@Value, sep = \"\")\n  invisible(x)\n}\n\nSince Enums will only ever be a sub-class we can confidently grab the first element of the class(enum_obj) which is the super-class of the enum. We paste that together with the value of the enum.\n\nsquare  &lt;- GridShape(\"Square\")\nsquare\n\nGridShape::Square"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#drawing-even-more-from-rust",
    "href": "posts/2023-11-10-enums-in-r/index.html#drawing-even-more-from-rust",
    "title": "Enums in R: towards type safe R",
    "section": "Drawing even more from Rust",
    "text": "Drawing even more from Rust\nRust enums are even more powerful than what I briefly introduced. Each variant of an enum can actually be typed!!! Take a look at the example from The Book™.\nenum Message {\n    Quit,\n    Move { x: i32, y: i32 },\n    Write(String),\n    ChangeColor(i32, i32, i32),\n}\nIn this enum there are 4 variants. The first Quit doesn’t have any associated data with it. But the other three do! The second one Move has two fields x and y which contain integer values. Write is a tuple with a string in it and ChangeColor has 3 integer values in its tuple. These can be extracted.\nA silly example function that illustrates how each value can be used can be\nfn which_msg(x: Message) {\n    match x {\n        Message::Quit =&gt; println!(\"I'm a quitter\"),\n        Message::Move { x, y } =&gt;  println!(\"Move over {x} and up {y}\"),\n        Message::Write(msg) =&gt; println!(\"your message is: {msg}\"),\n        Message::ChangeColor(r, g, b) =&gt;  println!(\"Your RGB ({r}, {g}, {b})\"),\n    }\n}\nWhen a variant with data is passed in the values can be used. For example\nwhich_msg(Message::ChangeColor(0, 155, 200));\n#&gt; Your RGB (0, 155, 200)\nExtending it to R\nWhat would this look like if we extended it to an R based enum object? I suspect the Variants would be a list of prototypes such as those from {vctrs}. The Value would have to be validated against all of the provided prototypes to ensure that it is one of the provided types.\nI’m not sure how I would code this up, but I think that would be a great thing to have."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html",
    "href": "posts/2023-07-06-r-is-still-fast.html",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "",
    "text": "There’s this new blog post making the rounds making some claims about why they won’t put R into production. Most notably they’re wheeling the whole “R is slow thing” again. And there are few things that grind my gears more than that type of sentiment. It’s almost always ill informed. I find that to be the case here too.\nI wouldn’t have known about this had it 1) not mentioned my own Rust project Valve and 2) a kind stranger inform me about it on mastodon.\nI’ve collected my reactions below as notes and sundry bench marks and bullet points."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#tldr",
    "href": "posts/2023-07-06-r-is-still-fast.html#tldr",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "TL;DR",
    "text": "TL;DR\n\nThere is a concurrent web server for R and I made it Valve\n\nPython is really fast at serializing json and R is slower\nPython is really slow at parsing json and R is so so soooo much faster\nTo handle types appropriately, sometimes you have to program\nThere are mock REST API testing libraries {httptest} and {webmockr}\n\nDemand your service providers to make the tools you want\nAsk and you shall receive\nR can go into production\nPLEASE JUST TRY VALVE YOU’LL LOVE IT"
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#production-services",
    "href": "posts/2023-07-06-r-is-still-fast.html#production-services",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Production Services",
    "text": "Production Services\nThere are so many people using R in production in so many ways across the world. I wish Posit did a better job getting these stories out. As a former RStudio employee, I personally met people putting R in production in most amazing ways. From the US Department of State, Defense, Biotech companies, marketing agencies, national lotteries, and so much more. The one that sticks out the most is that Payam M., when at Tabcorp massively scaled their system using Plumber APIs and Posit Connect to such a ridiculous scale I couldn’t even believe.\nGunicorn, Web Servers, and Concurrency\n\n“R has no widely-used web server to help it run concurrently.”\n\nThe premise of this whole blog post stems from the fact that there is no easily concurrent web server for R. Which is true and is the reason I built Valve. It doesn’t meet the criteria of widely used because no one has used it. In part, because of posts like this that discourage people from using R in production.\nTypes and Conversion\nThere’s this weird bit about how 1 and c(1, 2) are treated as the same class and unboxing of json. They provide the following python code as a desirable pattern for processing data.\nx = 1\ny = [1, 2]\n\njson.dump(x, sys.stdout)\n#&gt; 1\njson.dump(y, sys.stdout)\n#&gt; [1, 2]\nThey want scalars to be unboxed and lists to remain lists. This is the same behavior as jsonlite, though.\n\njsonlite::toJSON(1, auto_unbox = TRUE)\n\n1 \n\njsonlite::toJSON(1:2, auto_unbox = TRUE)\n\n[1,2] \n\n\nThere’s a difference here: one that the author fails to recognize is that a length 1 vector is handled appropriately. What the author is saying is that they don’t like that R doesn’t behave the same way as Python. You, as a developer should be able to guarantee that a value is length 1. It’s easy. length(x) == 1, or if you want is_scalar &lt;- function(x) length(x) == 1. This is the type system in R and json libraries handle the “edge case” appropriately. There is nothing wrong here. The reprex is the same as the python library.\n\n“R (and Plumber) also do not enforce types of parameters to your API, as opposed to FastAPI, for instance, which does via the use of pydantic.”\n\nPython does not type check nor does FastAPI. You opt in to type checking with FastAPI. You can do the same with Plumber. A quick perusal of the docs will show you this. Find the @param section. There is some concessions here, though. The truthful part here is the type annotations do type conversion for only dynamic routes. Which, I don’t know if FastAPI does. Type handling for static parameters is an outstanding issue of mine for plumber since 2021.\nI’ve followed up on the issue above and within minutes the maintainer responded. There is an existing PR to handle this issue.\nThis just goes to show if that you want something done in the open source world, just ask for it. More than likely its already there or just waiting for the slight nudge from someone else.\nWhile I know it’s not “seemless” adding an as.integer() and a stopifnot(is.integer(n)) isn’t the wildest thing for a developer to do.\nThere is a comparison between type checking in R and Python with the python example using type hints which are, again, opt-in. An unfair comparison when you say “if you don’t use the opt-in features of plumber but use the opt-in features of FastAPI, FastAPI is better.”\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/types\")\nasync def types(n: int) -&gt; int:\n  return n * 2\nClients and Testing\nI haven’t done much testing of API endpoints but I do know that there are two de facto packages for this:\n\n\n{httptest} and\n\n{webmockr}.\n\nThese are pretty easy to find. Not so sure why they weren’t mentioned or even tested.\nPerformance\nJSON serialization is a quite interesting thing to base performance off of. I’ve never seen how fast pandas serialization is. Quite impressive! But, keep with me, because you’ll see, this is fibbing with benchmarks.\nI do have thoughts on the use of jsonlite and it’s ubiquity. jsonlite is slow. I don’t like it. My belief is that everyone should use {jsonify} when creating json. It’s damn good.\nSo, when I run these bench marks on my machine for parsing I get:\n\nmicrobenchmark::microbenchmark(\n  jsonify = jsonify::to_json(iris),\n  jsonlite = jsonlite::toJSON(iris),\n  unit = \"ms\", \n  times = 1000\n)\n\nWarning in microbenchmark::microbenchmark(jsonify = jsonify::to_json(iris), :\nless accurate nanosecond times to avoid potential integer overflows\n\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\nUnit: milliseconds\n     expr      min       lq      mean    median       uq      max neval cld\n  jsonify 0.258218 0.265024 0.3224672 0.2698005 0.280850 35.20330  1000  a \n jsonlite 0.346245 0.360759 0.4169715 0.3719110 0.399012 20.00181  1000   b\n\n\nA very noticable difference in using jsonify over jsonlite. The same benchmark using pandas is holy sh!t fast!\nfrom timeit import timeit\nimport pandas as pd\n\niris = pd.read_csv(\"fastapi-example/iris.csv\")\n\nN = 1000\n\nprint(\n  \"Mean runtime:\", \n  round(1000 * timeit('iris.to_json(orient = \"records\")', globals = locals(), number = N) / N, 4), \n  \"milliseconds\"\n)\n#&gt; Mean runtime: 0.0721 milliseconds\nNow, this is only half the story. This is serialization. What about the other part? Where you ingest it.\nHere, I will also say, again, that you shouldn’t use jsonlite because it is slow. Instead, you should use {RcppSimdJson}. Because its\n\n\nLet’s run another benchmark\n\njsn &lt;- jsonify::to_json(iris)\n\nmicrobenchmark::microbenchmark(\n  simd = RcppSimdJson::fparse(jsn),\n  jsonlite = jsonlite::fromJSON(jsn),\n  unit = \"ms\",\n  times = 1000\n)\n\nUnit: milliseconds\n     expr      min        lq       mean   median        uq      max neval cld\n     simd 0.052275 0.0551040 0.06672631 0.057933 0.0634885 4.316275  1000  a \n jsonlite 0.433165 0.4531525 0.48931155 0.467359 0.4919795 4.352232  1000   b\n\n\nRcppSimdJson is ~8 times faster than jsonlite.\nLet’s do a similar benchmark in python.\njsn = iris.to_json(orient = \"records\")\n\nprint(\n  \"Mean runtime:\", \n  round(1000 * timeit('pd.read_json(jsn)', globals = locals(), number = N) / N, 4), \n  \"milliseconds\"\n)\n#&gt; Mean runtime: 1.2629 milliseconds\nPython is 3x slower than jsonlite in this case and 25x slower than RcppSimdJson. Which is very slow. While serializing is an important thing to be fast in, so is parsing the incoming json you are receiving. How nice it is to show only half the story! Use RcppSimdJson and embarrass pandas’ json parsing.\nIntegration with Tooling\nI have literally no idea about any of these except Launchdarkly because one of my close homies worked there for years. These are all paid services so I’m not sure how they work :)\nI would say to checkout Posit Connect for deploying R and python into production. But if your only use case is to deploy a single model, then yeah, I’d say that’s overkill.\nI wish more companies would create tooling for R and their services. The way to do this, is to lean into using R in production and demanding (not asking) providers to make wrappers for them. When you pay for a service, you have leverage. Use it. I think too many people fall over when what they need isn’t there immediately. Be sure to be the squeeky wheel that makes change.\nI also think that if you’re in the position where you can make a wrapper for something, you should. I did this when using Databricks in my last role and provided them with a lot of feedback. Have they taken it? I’m not sure. I’m not there to harass them anymore."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#workarounds",
    "href": "posts/2023-07-06-r-is-still-fast.html#workarounds",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Workarounds",
    "text": "Workarounds\nThese are good workarounds. I would suggest looking at ndexr.io as a way to scale these R based services as well. They utilize the NGINX approach described here."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#addenda",
    "href": "posts/2023-07-06-r-is-still-fast.html#addenda",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Addenda",
    "text": "Addenda\nClearly, this is where I care a lot. I am the author of Valve. Valve is exactly what the author was clamoring for in the beginning of the blog post. It is a web server that runs Plumber APIs in parallel written in Rust using Tokio, Axum, and Deadpool. Valve auto-scales on its own up to a maximum number of worker threads. So it’s not always taking up space and running more compute than it needs.\nValve overview:\n\nConcurrent webserver to auto-scale plumber APIs\nwritten in Rust using Tokio, Axum, and Deadpool\nspawns and kills plumber APIs based on demand\nintegration with {vetiver} of of the box\n\nFirst things first, I want to address “it’s not on CRAN.” You’re right. That’s because it is a Rust crate. Crates don’t go on CRAN. I’ve made an R package around it to lower the bar to entry. But it is a CLI tool at the core.\nObviously, it is new. It is untested. I wish I could tell everyone to use it, but I can’t. I think anyone who used it would be floored by its performance and ease of use. It is SO simple.\nI’ll push it to crates.io and CRAN in the coming weeks. Nothing like h8rs to inspire."
  },
  {
    "objectID": "posts/2019-08-04-my-parts.html",
    "href": "posts/2019-08-04-my-parts.html",
    "title": "∑ { my parts }",
    "section": "",
    "text": "library(tidyverse)\n\nterrorists &lt;- readr::read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSqNhpFX_69klKgJCVobc3fjHYVE9pNosrDi9h6irLlCtSSLpR704iu9VqI7CxdRi0iKt3p1FDYbu8Y/pub?gid=956062857&single=true&output=csv\")\n\n\n\n\n\nterrorist_by_race\n#&gt; # A tibble: 7 × 6\n#&gt;   race                n fatalities injured total_victims   `%`\n#&gt;   &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 white              63        554    1067          1621 69.3 \n#&gt; 2 other               5         90     115           205  8.77\n#&gt; 3 black              19        108      89           197  8.43\n#&gt; 4 asian               8         77      33           110  4.70\n#&gt; 5 unclear             6         40      61           101  4.32\n#&gt; 6 latino             10         44      33            77  3.29\n#&gt; 7 native american     3         19       8            27  1.15\n\n\n\n\n\nterrorist_by_gender\n#&gt; # A tibble: 3 × 6\n#&gt;   gender            n fatalities injured total_victims    `%`\n#&gt;   &lt;chr&gt;         &lt;int&gt;      &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 male            110        903    1380          2283 97.6  \n#&gt; 2 male & female     1         14      21            35  1.50 \n#&gt; 3 female            3         15       5            20  0.855\n\n\nterrorist &lt;- c(\"angry\", \"white\", \"male\")\nmy_parts &lt;- c(\"angry\", \"white\", \"male\")\n\n\nmy_parts == terrorist\n#&gt; [1] TRUE TRUE TRUE\n\n\n`I am` &gt; sum(my_parts)\n\n\n#&gt; [1] TRUE\n\n\n`I am` == sum(terrorist)\n\n\nFALSE\n#&gt; [1] FALSE\n\n\nwhite_males &lt;- filter(terrorists,\n                      race == \"white\",\n                      tolower(gender) == \"male\",\n                      !is.na(name))\n\npull(white_males, name)\n#&gt;  [1] \"Jordan Witmer\"             \"Zephen A. Xaver\"           \"Robert D. Bowers\"         \n#&gt;  [4] \"Jarrod W. Ramos\"           \"Dimitrios Pagourtzis\"      \"Travis Reinking\"          \n#&gt;  [7] \"Nikolas J. Cruz\"           \"Timothy O'Brien Smith\"     \"Kevin Janson Neal\"        \n#&gt; [10] \"Devin Patrick Kelley\"      \"Scott Allen Ostrem\"        \"Stephen Craig Paddock\"    \n#&gt; [13] \"Randy Stair\"               \"Thomas Hartless\"           \"Jason B. Dalton\"          \n#&gt; [16] \"Robert Lewis Dear\"         \"Noah Harpham\"              \"Dylann Storm Roof\"        \n#&gt; [19] \"Elliot Rodger\"             \"John Zawahri\"              \"Kurt Myers\"               \n#&gt; [22] \"Adam Lanza\"                \"Andrew Engeldinger\"        \"Wade Michael Page\"        \n#&gt; [25] \"James Holmes\"              \"Ian Stawicki\"              \"Scott Evans Dekraai\"      \n#&gt; [28] \"Jared Loughner\"            \"Robert Stewart\"            \"Wesley Neal Higdon\"       \n#&gt; [31] \"Steven Kazmierczak\"        \"Robert A. Hawkins\"         \"Tyler Peterson\"           \n#&gt; [34] \"Sulejman Talović\\u0087\"    \"Charles Carl Roberts\"      \"Kyle Aaron Huff\"          \n#&gt; [37] \"Terry Michael Ratzmann\"    \"Nathan Gale\"               \"Douglas Williams\"         \n#&gt; [40] \"Michael McDermott\"         \"Larry Gene Ashbrook\"       \"Day trader Mark O. Barton\"\n#&gt; [43] \"Eric Harris\"               \"Kipland P. Kinkel\"         \"Mitchell Scott Johnson\"   \n#&gt; [46] \"Matthew Beck\"              \"Dean Allen Mellberg\"       \"Kenneth Junior French\"    \n#&gt; [49] \"Gian Luigi Ferri\"          \"John T. Miller\"            \"Eric Houston\"             \n#&gt; [52] \"Thomas McIlvane\"           \"George Hennard\"            \"Joseph T. Wesbecker\"      \n#&gt; [55] \"Patrick Purdy\"             \"Richard Farley\"            \"William Cruse\"            \n#&gt; [58] \"Patrick Sherrill\"          \"James Oliver Huberty\"      \"Abdelkrim Belachheb\"      \n#&gt; [61] \"Carl Robert Brown\"\n\n\nam_i &lt;- function(terrorist) {\n  msg &lt;- paste(\"am i ==\", terrorist)\n  print(msg)\n  print(`am i` == terrorist)\n}\n\n\npull(white_males, name) %&gt;% \n  walk(~am_i(.))\n#&gt; [1] \"`am i` == Jordan Witmer\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Zephen A. Xaver\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert D. Bowers\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Jarrod W. Ramos\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Dimitrios Pagourtzis\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Travis Reinking\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Nikolas J. Cruz\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Timothy O'Brien Smith\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kevin Janson Neal\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Devin Patrick Kelley\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Scott Allen Ostrem\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Stephen Craig Paddock\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Randy Stair\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Thomas Hartless\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Jason B. Dalton\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert Lewis Dear\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Noah Harpham\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Dylann Storm Roof\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Elliot Rodger\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == John Zawahri\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kurt Myers\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Adam Lanza\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Andrew Engeldinger\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Wade Michael Page\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == James Holmes\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Ian Stawicki\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Scott Evans Dekraai\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Jared Loughner\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert Stewart\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Wesley Neal Higdon\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Steven Kazmierczak\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert A. Hawkins\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Tyler Peterson\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Sulejman Talović\\u0087\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Charles Carl Roberts\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kyle Aaron Huff\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Terry Michael Ratzmann\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Nathan Gale\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Douglas Williams\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Michael McDermott\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Larry Gene Ashbrook\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Day trader Mark O. Barton\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Eric Harris\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kipland P. Kinkel\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Mitchell Scott Johnson\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Matthew Beck\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Dean Allen Mellberg\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kenneth Junior French\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Gian Luigi Ferri\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == John T. Miller\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Eric Houston\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Thomas McIlvane\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == George Hennard\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Joseph T. Wesbecker\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Patrick Purdy\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Richard Farley\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == William Cruse\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Patrick Sherrill\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == James Oliver Huberty\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Abdelkrim Belachheb\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Carl Robert Brown\"\n#&gt; [1] FALSE\n\n\n`I am` &gt; sum(my_parts)"
  },
  {
    "objectID": "posts/2021-03-28-python-r.html",
    "href": "posts/2021-03-28-python-r.html",
    "title": "Python & R in production — the API way",
    "section": "",
    "text": "In my previous post I discussed how we can alter the R & Python story to be predicated on APIs as a way to bridge the language divide. The R & Python love story feels almost like unrequited love (h/t). Much of the development towards integrating the two languages has been heavily focused on the R user experience. While the developments with respect to reticulate have been enormous and cannot go understated, it might be worthwhile exploring another way in which R & Python and, for that matter, Python & R can be utilized together.\nBy shifting from language based tools that call the other language and translate their objects like reticulate and rpy2, to APIs we can develop robust language agnostic data science pipelines. I want to provide two motivating examples that explore the interplay between R & Python."
  },
  {
    "objectID": "posts/2021-03-28-python-r.html#calling-python-from-r-without-reticulate",
    "href": "posts/2021-03-28-python-r.html#calling-python-from-r-without-reticulate",
    "title": "Python & R in production — the API way",
    "section": "Calling Python from R (without reticulate)",
    "text": "Calling Python from R (without reticulate)\nWhen we talk about R & Python we typically are referring to reticulate, whether that be through python code chunks, the {tensorflow} package, or reticulate itself. However, as discussed in my previous post, another way that we can do this is via API. Flask can be used to create RESTful APIs.\nOn the RStudio Connect demo server there is a Flask app which provides historical stock prices for a few tickers. We can create a simple windowed summary visualization utilizing the Flask app, httr, dplyr, and ggplot2. Let’s break this down. First we use the httr library to send an HTTP request to the Flask app.\nlibrary(httr)\nlibrary(tidyverse)\n\nflask_call &lt;- \"https://colorado.rstudio.com/rsc/flask-stock-service/stocks/AAPL/history\"\n\naapl &lt;- GET(flask_call) %&gt;% \n  content(as = \"text\", encoding = \"utf-8\") %&gt;% \n  jsonlite::fromJSON() %&gt;% \n  as_tibble()\nBy sending this HTTP request, we are then kicking off a Python process which returns our dataset. We can then use dplyr to aggregate our dataset as we normally would.\naapl_yearly &lt;- aapl %&gt;% \n  group_by(year = lubridate::year(date)) %&gt;% \n  summarise(avg_adj = mean(adjusted)) \n\nhead(aapl_yearly)\n#&gt; # A tibble: 6 x 2\n#&gt;    year avg_adj\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1  2010    24.9\n#&gt; 2  2011    34.8\n#&gt; 3  2012    56.1\n#&gt; 4  2013    53.1\n#&gt; 5  2014    84.3\n#&gt; 6  2015   113.\nFinally we utilize ggplot2 to create the simple visualization.\nggplot(aapl_yearly, aes(year, avg_adj)) +\n  geom_line() + \n  labs(title = \"AAPL stock growth\", x = \"\", y = \"Average Adjusted\") +\n  scale_y_continuous(labels = scales::dollar) + \n  theme_minimal()\n\n\n\nThat was simple, right? In the above code chunks we utilized both R and python while only interacting and writing R code. That’s the brilliance of this approach."
  },
  {
    "objectID": "posts/2021-03-28-python-r.html#calling-r-from-python",
    "href": "posts/2021-03-28-python-r.html#calling-r-from-python",
    "title": "Python & R in production — the API way",
    "section": "Calling R from Python",
    "text": "Calling R from Python\nThe less often discussed part of this love story—hence unrequited love story—is how can Python users utilize R within their own workflows. Often machine learning engineers will use Python in combination with Scikit Learn to create their models. To illustrate how we can let both R and Python users shine I wanted to adapt the wonderful Bike Prediction example from the Solutions Engineering team at RStudio.\n\n\n\nThe Bike Prediction project is an example of orchestrating a number of data science artifacts into a holistic system on RStudio Connect that all work in unity. This example could just as well have been written entirely with Python. It could even be written as a combination of both R and Python. And that is what I’d like to illustrate.\nThe bike prediction app utilizes a custom R package and the power of dbplyr to perform scheduled ETL jobs. It is effective, efficient, and already deployed. Say one has a colleague who would like to create a new machine learning model using the same data how can we enable them to do so? The example works within the context of its own R Markdown that retrains the model. Rather than making a one time export of the data from the ETL process, we can make the data available consistently through a RESTful API hosted here.\n\n\n\nThe training and testing data have been made available through a plumber API that is hosted on RStudio Connect. With the data being available through an API, all that is needed to interact with it is the requests library. Everything else is as one would anticipate!\nimport requests\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nIn the below code chunk we call the Plumber API using an HTTP request which kicks off an R process. That R process utilizes dbplyr and lubridate to extract and partition data for training and testing.\n# Fetch data from Plumber API \ntest_dat_raw = requests.get(\"https://colorado.rstudio.com/rsc/bike-data-api/testing-data\")\ntest_dat = pd.read_json(test_dat_raw.text)\n\ntrain_dat_raw = requests.get(\"https://colorado.rstudio.com/rsc/bike-data-api/training-data\")\ntrain_dat = pd.read_json(train_dat_raw.text)\nNow that the data have been processed by R and loaded as a pandas dataframe the model training can continue as standard.\n# partition data and one hot encode day of week\ntrain_x = pd.concat([train_dat[[\"hour\", \"month\", \"lat\", \"lon\"]], pd.get_dummies(train_dat.dow)], axis = 1)\ntrain_y = train_dat[\"n_bikes\"]\n\n# instantiate xgb model object\nmodel = XGBRegressor()\n \n# fit the model with the training data\nmodel.fit(train_x,train_y)\n\n# predict the target on the test dataset\ntest_dow = pd.get_dummies(pd.Categorical(test_dat.dow, categories = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday']))\ntest_x = pd.concat([test_dat[[\"hour\", \"month\", \"lat\", \"lon\"]], test_dow], axis = 1)\ntest_y = test_dat.n_bikes\n# predict the target on the test dataset\npredict_test = model.predict(test_x)\n \n# MSE on test dataset\nmean_squared_error(test_y,predict_test, squared = False)\n#&gt; 4.502217132673415\nThrough the API both R and Python were able to flourish all the while building extensible infrastructure that can be utilized beyond their own team. The API approach enables the R and Python user to extend their tools beyond their direct team without having to adopt a new toolkit."
  },
  {
    "objectID": "posts/2021-03-28-python-r.html#adopting-apis-for-cross-language-collaboration",
    "href": "posts/2021-03-28-python-r.html#adopting-apis-for-cross-language-collaboration",
    "title": "Python & R in production — the API way",
    "section": "Adopting APIs for cross language collaboration",
    "text": "Adopting APIs for cross language collaboration\nWhile data scientists may usually think of APIs as something that they use to interact with SaaS products or extract data, they are also a tool that can be utilized to build out the data science infrastructure of a team. Through Flask, Plumber, and other libraries that turn code into RESTful APIs, data scientists can bridge language divides with exceptional ease. I think we ought to begin to transition the ways in which we think about language divides. We ought to utilize the universal language of HTTP more thoroughly. By creating these APIs we not only can aid other data scientists, but entirely other teams. A React JS web development can then tap into your API to either serve up predictions, extract data, send files, or whatever else you can dream up. Let’s not limit ourselves to one language. Let’s build out APIs to enable all languages to thrive.\nDisclaimer: This is a personal opinion and not endorsed or published by RStudio. My statements represent no one but myself—sometimes not even that."
  },
  {
    "objectID": "posts/2018-11-28-function-methods.html",
    "href": "posts/2018-11-28-function-methods.html",
    "title": "[Not so] generic functions",
    "section": "",
    "text": "Lately I have been doing more of my spatial analysis work in R with the help of the sf package. One shapefile I was working with had some horrendously named columns, and naturally, I tried to clean them using the clean_names() function from the janitor package. But lo, an egregious error occurred. To this end, I officially filed my complaint as an issue. The solution presented was to simply create a method for sf objects.\nYeah, methods, how tough can those be? Apparently the process isn’t at all difficult. But figuring out the process? That was difficult. This post will explain how I went about the process for converting the clean_names() function into a generic (I’ll explain this in a second), and creating a method for sf and tbl_graph objects.\n\nThe Jargon\nOkay, I want to address the jargon. What the hell is a generic function, and what is a method? But first, I want to give a quick tl;dr on what a function is. I define as function as bit of code that takes an input, changes it in some way, and produces an output. Even simpler, a function takes an input and creates an output.\n\nGeneric Functions\nNow, what is a generic function? My favorite definition that I’ve seen so far comes from LispWorks Ltd (their website is a historic landmark, I recommend you give it a look for a reminder of what the internet used to be). They define a generic function as\n\na function whose behavior depends on the classes or identities of the arguments supplied to it.\n\nThis means that we have to create a function that looks at the class of an object and perform an operation based on the object class. That means if there is \"numeric\" or \"list\" object, they will be treated differently. These are called methods. Note: you can find the class of an object by using the class() function on any object.\n\n\nMethods\nTo steal from LispWorks Ltd again, a method is\n\npart of a generic function which provides information about how that generic function should behave [for] certain classes.\n\nThis means that a method is part of a generic function and has to be defined separately. Imagine we have a generic function called f with methods for list and numeric objects. The way that we would denote these methods is by putting a period after the function name and indicating the type of object the function is to be used on. These would look like f.list and f.numeric respectively.\nBut to save time you can always create a default method which will be dispatched (used) on any object that it hasn’t been explicitly told how to operate on (by a specific method).\nNow that the intuition of what generic functions and methods R, we can begin the work of actually creating them. This tutorial will walk through the steps I took in changing the clean_names() from a standard function into a generic function with methods for sf objects and tbl_graph objects from the sf and tidygraph packages respectively.\nA brief overview of the process:\n\nDefine the generic function\nCreate a default method\nCreate additional methods\n\nA quick note: The code that follows is not identical to that of the package. I will be changing it up to make it simpler to read and understand what is happening.\n\n\n\nThe Generic Method\nThe first step, as described above, is to create a generic function. Generic functions are made by creating a new function with the body containing only a call to the UseMethod() function. The only argument to this is the name of your generic function—this should be the same as the name of the function you are making. This tells R that you are creating a generic function. Additionally, you should add any arguments that will be necessary for your function. Here, there are two arguments: dat and case. These indicate the data to be cleaned and the preferred style for them to be cleaned according to.\nI am not setting any default values for dat to make it required, whereas I am setting case to \"snake\".\n\nclean_names &lt;- function(dat, case = \"snake\") {\n  UseMethod(\"clean_names\")\n}\n\nNow we have created a generic function. But this function doesn’t know how to run on any given object types. In other words, there are no methods associated with it. To illustrate this try using the clean_names() function we just defined on objects of different types.\nclean_names(1) # numeric \nclean_names(\"test\") # character \nclean_names(TRUE) # logical \n\n#&gt; [1] \"no applicable method for 'clean_names' applied to an object of class \\\"c('double', 'numeric')\\\"\"\n\n\n#&gt; [1] \"no applicable method for 'clean_names' applied to an object of class \\\"character\\\"\"\n\n\n#&gt; [1] \"no applicable method for 'clean_names' applied to an object of class \\\"logical\\\"\"\n\nThe output of these calls say no applicable method for 'x' applied to an object of [class]. In order to prevent this from happening, we can create a default method. A default method will always be used if the function doesn’t have a method for the provided object type.\n\n\nThe Default Method\nRemember that methods are indicated by writing function.method. It is also important to note that the method should indicate an object class. To figure out what class an object is you can use the class() function. For example class(1) tells you that the number 1 is “numeric”.\nIn this next step I want to create a default method that will be used on every object that there isn’t a method explicitly for. To do this I will create a function called clean_names.default.\nAs background, the clean_names() function takes a data frame and changes column headers to fit a given style. clean_names() in the development version is based on the function make_clean_names() which takes a character vector and makes each value match a given style (the default is snake, and you should only use snake case because everything else is wrong * sarcasm * ).\n\nlibrary(janitor)\n\nNow let’s see how this function works. For this we will use the ugliest character vector I have ever seen from the tests for clean_names() (h/t @sfirke for making this).\n\nugly_names &lt;- c(\n  \"sp ace\", \"repeated\", \"a**^@\", \"%\", \"*\", \"!\",\n  \"d(!)9\", \"REPEATED\", \"can\\\"'t\", \"hi_`there`\", \"  leading spaces\",\n  \"€\", \"ação\", \"Farœ\", \"a b c d e f\", \"testCamelCase\", \"!leadingpunct\",\n  \"average # of days\", \"jan2009sales\", \"jan 2009 sales\"\n)\n\nugly_names\n#&gt;  [1] \"sp ace\"            \"repeated\"          \"a**^@\"             \"%\"                \n#&gt;  [5] \"*\"                 \"!\"                 \"d(!)9\"             \"REPEATED\"         \n#&gt;  [9] \"can\\\"'t\"           \"hi_`there`\"        \"  leading spaces\"  \"€\"                \n#&gt; [13] \"ação\"              \"Farœ\"              \"a b c d e f\"       \"testCamelCase\"    \n#&gt; [17] \"!leadingpunct\"     \"average # of days\" \"jan2009sales\"      \"jan 2009 sales\"\n\nNow to see how this function works:\n\nmake_clean_names(ugly_names)\n#&gt;  [1] \"sp_ace\"                 \"repeated\"               \"a\"                     \n#&gt;  [4] \"percent\"                \"x\"                      \"x_2\"                   \n#&gt;  [7] \"d_9\"                    \"repeated_2\"             \"cant\"                  \n#&gt; [10] \"hi_there\"               \"leading_spaces\"         \"x_3\"                   \n#&gt; [13] \"acao\"                   \"faroe\"                  \"a_b_c_d_e_f\"           \n#&gt; [16] \"test_camel_case\"        \"leadingpunct\"           \"average_number_of_days\"\n#&gt; [19] \"jan2009sales\"           \"jan_2009_sales\"\n\nTrès magnifique!\nThe body of the default method will take column names from a dataframe, clean them, and reassign them. Before we can do this, a dataframe is needed!\n\n# create a data frame with 20 columns\ntest_df &lt;- as_tibble(matrix(sample(100, 20), ncol = 20))\n\n# makes the column names the `ugly_names` vector\nnames(test_df) &lt;- ugly_names\n\n# print the data frame.\ntest_df\n#&gt; # A tibble: 1 × 20\n#&gt;   `sp ace` repeated `a**^@`   `%`   `*`   `!` `d(!)9` REPEATED `can\"'t` hi_\\th…¹   lea…²\n#&gt;      &lt;int&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt; 1       78       18      99    28    26    38      43       73       51       33      23\n#&gt; # … with 9 more variables: `€` &lt;int&gt;, ação &lt;int&gt;, Farœ &lt;int&gt;, `a b c d e f` &lt;int&gt;,\n#&gt; #   testCamelCase &lt;int&gt;, `!leadingpunct` &lt;int&gt;, `average # of days` &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, `jan 2009 sales` &lt;int&gt;, and abbreviated variable names\n#&gt; #   ¹​`hi_\\`there\\``, ²​`  leading spaces`\n#&gt; # ℹ Use `colnames()` to see all variable names\n\nThe process for writing this function is:\n\ntake a dataframe\ntake the old column names and clean them\nreassign the column names as the new clean names\nreturn the object\n\n\nclean_names.default &lt;- function(dat, case = \"snake\") { \n  # retrieve the old names\n  old_names &lt;- names(dat)\n  # clean the old names\n  new_names &lt;- make_clean_names(old_names, case = case)\n  # assign the column names as the clean names vector\n  names(dat) &lt;- new_names\n  # return the data\n  return(dat)\n  }\n\nNow that the default method has been defined. Try running the function on our test dataframe!\n\nclean_names(test_df)\n#&gt; # A tibble: 1 × 20\n#&gt;   sp_ace repeated     a percent     x   x_2   d_9 repeated_2  cant hi_th…¹ leadi…²   x_3\n#&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     78       18    99      28    26    38    43         73    51      33      23    27\n#&gt; # … with 8 more variables: acao &lt;int&gt;, faroe &lt;int&gt;, a_b_c_d_e_f &lt;int&gt;,\n#&gt; #   test_camel_case &lt;int&gt;, leadingpunct &lt;int&gt;, average_number_of_days &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, jan_2009_sales &lt;int&gt;, and abbreviated variable names ¹​hi_there,\n#&gt; #   ²​leading_spaces\n#&gt; # ℹ Use `colnames()` to see all variable names\n\nOh, my gorsh. Look at that! We can try replicating this with a named vector to see how the default method dispatched on unknown objects!\n\n# create a vector with 20 elements\ntest_vect &lt;- c(1:20)\n\n# name each element with the ugly_names vector \nnames(test_vect) &lt;- ugly_names\n\n# try cleaning!\nclean_names(test_vect)\n#&gt;                 sp_ace               repeated                      a \n#&gt;                      1                      2                      3 \n#&gt;                percent                      x                    x_2 \n#&gt;                      4                      5                      6 \n#&gt;                    d_9             repeated_2                   cant \n#&gt;                      7                      8                      9 \n#&gt;               hi_there         leading_spaces                    x_3 \n#&gt;                     10                     11                     12 \n#&gt;                   acao                  faroe            a_b_c_d_e_f \n#&gt;                     13                     14                     15 \n#&gt;        test_camel_case           leadingpunct average_number_of_days \n#&gt;                     16                     17                     18 \n#&gt;           jan2009sales         jan_2009_sales \n#&gt;                     19                     20\n\nIt looks like this default function works super well with named objects! Now, we will broach the problem I started with, sf objects.\n\n\nsf method\nThis section will go over the process for creating the sf method. If you have not ever used the sf package, I suggest you give it a try! It makes dataframe objects with spatial data associated with it. This allows you to perform many of the functions from the tidyverse to spatial data.\nBefore getting into it, I want to create a test object to work with. I will take the test_df column, create longitude and latitude columns, and then convert it into an sf object. The details of sf objects is out of the scope of this post.\n\nlibrary(sf)\n\ntest_sf &lt;- test_df %&gt;%\n  # create xy columns\n  mutate(long = -80, \n         lat = 40) %&gt;% \n  # convert to sf object \n  st_as_sf(coords = c(\"long\", \"lat\"))\n\n# converting geometry column name to poor style\nnames(test_sf)[21] &lt;- \"Geometry\"\n\n# telling sf which column is now the geometry\nst_geometry(test_sf) &lt;- \"Geometry\"\n\ntest_sf\n#&gt; Simple feature collection with 1 feature and 20 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -80 ymin: 40 xmax: -80 ymax: 40\n#&gt; CRS:           NA\n#&gt; # A tibble: 1 × 21\n#&gt;   `sp ace` repeated `a**^@`   `%`   `*`   `!` `d(!)9` REPEATED `can\"'t` hi_\\th…¹   lea…²\n#&gt;      &lt;int&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt; 1       78       18      99    28    26    38      43       73       51       33      23\n#&gt; # … with 10 more variables: `€` &lt;int&gt;, ação &lt;int&gt;, Farœ &lt;int&gt;, `a b c d e f` &lt;int&gt;,\n#&gt; #   testCamelCase &lt;int&gt;, `!leadingpunct` &lt;int&gt;, `average # of days` &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, `jan 2009 sales` &lt;int&gt;, Geometry &lt;POINT&gt;, and abbreviated\n#&gt; #   variable names ¹​`hi_\\`there\\``, ²​`  leading spaces`\n#&gt; # ℹ Use `colnames()` to see all variable names\n\nThe sf object has been created. But now how does our default method of the clean_names() function work on this object? There is only one way to know, try it.\nclean_names(test_sf)\n\nError in st_geometry.sf(x) : attr(obj, \"sf_column\") does not point to a geometry column. Did you rename it, without setting st_geometry(obj) &lt;- \"newname\"?\nNotice how it fails. sf noticed that I changed the name of the geometry column without explicitly telling it I did so. Since the geometry column is almost always the last column of an sf object, we can use the make_clean_names() function on every column but the last one! To do this we will use the rename_at() function from dplyr. This function allows you rename columns based on their name or position, and a function that renames it (in this case, make_clean_names()).\nFor this example dataset, say I wanted to clean the first column. How would I do that? Note that the first column is called sp ace. How this works can be seen in a simple example. In the below function call we are using the rename_at() function (for more, go here), selecting the first column name, and renaming it using the make_clean_names() function.\n\nrename_at(test_df, .vars = vars(1), .funs = make_clean_names)\n#&gt; # A tibble: 1 × 20\n#&gt;   sp_ace repea…¹ `a**^@`   `%`   `*`   `!` `d(!)9` REPEA…² can\"'…³ hi_\\t…⁴   lea…⁵   `€`\n#&gt;    &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     78      18      99    28    26    38      43      73      51      33      23    27\n#&gt; # … with 8 more variables: ação &lt;int&gt;, Farœ &lt;int&gt;, `a b c d e f` &lt;int&gt;,\n#&gt; #   testCamelCase &lt;int&gt;, `!leadingpunct` &lt;int&gt;, `average # of days` &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, `jan 2009 sales` &lt;int&gt;, and abbreviated variable names\n#&gt; #   ¹​repeated, ²​REPEATED, ³​`can\"'t`, ⁴​`hi_\\`there\\``, ⁵​`  leading spaces`\n#&gt; # ℹ Use `colnames()` to see all variable names\n\nNotice how only the first column has been cleaned. It went from sp ace to sp_ace. The goal is to replicate this for all columns except the last one.\nTo write the sf method, the above line of code can be adapted to select columns 1 through the number of columns minus 1 (so geometry isn’t selected). In order to make this work, we need to identify the second to last column—this will be supplied as the ending value of our selected variables.\n\nclean_names.sf &lt;- function(dat, case = \"snake\") {\n  # identify last column that is not geometry\n  last_col_to_clean &lt;- ncol(dat) - 1\n  # create a new dat object\n  dat &lt;- rename_at(dat, \n                   # rename the first up until the second to last\n                   .vars = vars(1:last_col_to_clean), \n                   # clean using the make_clean_names\n                   .funs = make_clean_names)\n  return(dat)\n}\n\nVoilà! Our first non-default method has been created. This means that when an sf object is supplied to our generic function clean_names() it looks at the class of the object—class(sf_object)—notices it’s an sf object, then dispatches (uses) the clean_names.sf() method instead of the default.\n\nclean_names(test_sf)\n#&gt; Simple feature collection with 1 feature and 20 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -80 ymin: 40 xmax: -80 ymax: 40\n#&gt; CRS:           NA\n#&gt; # A tibble: 1 × 21\n#&gt;   sp_ace repeated     a percent     x   x_2   d_9 repeated_2  cant hi_th…¹ leadi…²   x_3\n#&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     78       18    99      28    26    38    43         73    51      33      23    27\n#&gt; # … with 9 more variables: acao &lt;int&gt;, faroe &lt;int&gt;, a_b_c_d_e_f &lt;int&gt;,\n#&gt; #   test_camel_case &lt;int&gt;, leadingpunct &lt;int&gt;, average_number_of_days &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, jan_2009_sales &lt;int&gt;, Geometry &lt;POINT&gt;, and abbreviated\n#&gt; #   variable names ¹​hi_there, ²​leading_spaces\n#&gt; # ℹ Use `colnames()` to see all variable names\n\nHere we see that it worked exactly as we hoped. Every column but the last has been altered. This allows sf to name it’s geometry columns whatever it would like without disrupting it.\nShortly after this addition was added to the package I became aware of another type of object that had problems using clean_names(). This is the tbl_graph object from the tidygraph package from Thomas Lin Pederson.\n\n\ntbl_graph method\nIn issue #252 @gvdr noted that calling clean_names() on a tbl_graph doesn’t execute. Thankfully @Tazinho noted that you could easily clean the column headers by using the rename_all() function from dplyr.\nHere the solution was even easier than above. As a reminder, in order to make the tbl_graph method, we need to specify the name of the generic followed by the object class.\n\nclean_names.tbl_graph &lt;- function(dat, case = \"snake\") { \n  # rename all columns\n  dat &lt;- rename_all(dat, make_clean_names)\n  return(dat)\n  }\n\nIn order to test the function, we will need a graph to test it on. This example draws on the example used in the issue.\n\nlibrary(tidygraph)\n# create test graph to test clean_names\ntest_graph &lt;- play_erdos_renyi(0, 0.5) %&gt;% \n  # attach test_df as columns \n  bind_nodes(test_df)\n\ntest_graph\n#&gt; # A tbl_graph: 1 nodes and 0 edges\n#&gt; #\n#&gt; # A rooted tree\n#&gt; #\n#&gt; # Node Data: 1 × 20 (active)\n#&gt;   sp ace… repeat… `a**^@`   `%`   `*`   `!` `d(!)9` REPEAT… can\"'t… hi_\\th…   lead…\n#&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n#&gt; 1      78      18      99    28    26    38      43      73      51      33      23\n#&gt; # … with 9 more variables: `€` &lt;int&gt;, ação &lt;int&gt;, Farœ &lt;int&gt;, `a b c d e f` &lt;int&gt;,\n#&gt; #   testCamelCase &lt;int&gt;, `!leadingpunct` &lt;int&gt;, `average # of days` &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, `jan 2009 sales` &lt;int&gt;\n#&gt; #\n#&gt; # Edge Data: 0 × 2\n#&gt; # … with 2 variables: from &lt;int&gt;, to &lt;int&gt;\n\nHere we see that there is a graph with only 1 node and 0 edges (relations) with bad column headers (for more, visit the GitHub page). Now we can test this as well.\n\nclean_names(test_graph)\n#&gt; # A tbl_graph: 1 nodes and 0 edges\n#&gt; #\n#&gt; # A rooted tree\n#&gt; #\n#&gt; # Node Data: 1 × 20 (active)\n#&gt;   sp_ace repeat…     a percent     x   x_2   d_9 repeat…  cant hi_the… leadin…   x_3\n#&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     78      18    99      28    26    38    43      73    51      33      23    27\n#&gt; # … with 8 more variables: acao &lt;int&gt;, faroe &lt;int&gt;, a_b_c_d_e_f &lt;int&gt;,\n#&gt; #   test_camel_case &lt;int&gt;, leadingpunct &lt;int&gt;, average_number_of_days &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, jan_2009_sales &lt;int&gt;\n#&gt; #\n#&gt; # Edge Data: 0 × 2\n#&gt; # … with 2 variables: from &lt;int&gt;, to &lt;int&gt;\n\nIt worked as anticipated!\n\n\nReview (tl;dr)\nIn the preceding sections we learned what generic functions and methods are. How to create a generic function, a default method, and methods for objects of different classes.\n\ngeneric function: “A generic function is a function whose behavior depends on the classes or identities of the arguments supplied to it”\ngeneric function method: “part of a generic function and which provides information about how that generic function should behave [for] certain classes”\n\nThe process to create a function with a method is to:\n\nCreate a generic function with:\n\nf_x &lt;- function() { UseMethod(\"f_x\") }\n\nDefine the default method with:\n\nf_x.default &lt;- function() { do something }\n\nDefine object class specific methods with:\n\nf_x.class &lt;- function() { do something else}\n\n\n\nNotes\nIf you have not yet encountered the janitor package it will help you tremendously with various data cleaning processes. Clearly, clean_names() is my favorite function as it helps me enforce my preferred style (and the only). If you are not aware of “proper” R style, I suggest you read the style guide in Advanced R.\nWhile on the subject of Advanced R, I suggest you read the “Creating new methods and generics” section of it. I struggled comprehending it at first because I didn’t even know what a method was. However, if after reading this you feel like you want more, that’s the place to go.\nI’d like to thank @sfirke for being exceptionally helpful in guiding my contributions to the janitor package."
  },
  {
    "objectID": "posts/2023-03-01-learning-rust.html",
    "href": "posts/2023-03-01-learning-rust.html",
    "title": "learning rust",
    "section": "",
    "text": "I have been wanting to learn a low level language for quite some time. Due to the power and prominence of Rcpp I had thought I wanted to learn C++. Every fast package in R uses C++, right? But Rust kept popping up. Rust, is fast. Rust is safe. Linux is going to be rewritten in Rust. Rust is the most loved language for 7 years in a row. Rust is easily multithreaded. Rust. Rust. Rust. I then heard a bit about rextendr a way to incorporate Rust into R packages. With that Rust became a real candidate language to learn.\nThe @hrbrmstr provided some sweet words of encouragement over twitter, an example repo to reference, and provided ideas on how to start learning Rust.\nI spent some time with “The Book” to wrap my head around the basics. I then spent some time writing a chess FEN parser. I wrote a FEN parser. I wrote a whole program in Rust. That was crazy. The reason why I was able to continue learning Rust is because it is so easy to use (in comparison to C++). Once you wrap your head around the basics it rocks.\nRust is a compiled language. Working with a compiled language is a huge paradigm shift. R is an interpreted language. Interpreted languages are super cool because we can run one line of code, do some stuff, and then run another line. Compiled languages have to take everything in at once. So there is no running a line, printing an object, then running another line. But that’s actually okay because of the Rust compiler is the best teacher I’ve ever had.\nWhen you make a mistake in Rust, the compiler will tell you exactly where that mistake is coming from—literally the line and column position. It will also often tell you exactly what code you need to change and how to change it to make your code run. So rather than running line by line, you can compile line by line.\nLearning Rust has made me a better R programmer for many reasons. Here are a few:\n\nI am conscious of type conversions and consistency\nI am conscientious of memory consumption\nI am a glutton for speed now\nI have a better understanding / framework for thinking about inheritance\n\nProgramming in Rust has made me think of ways that R can be improved. Mostly in that scalar classes are missing from R (and from vctrs). We also lack the ability to use 64 bit integers which is a bit of a problem. I also think R packages should be designed to be extended. This would be done by exposing generic s3 functions that can be extended for your class. If the method exists for your class you inherit the functionality. I employed a prototype of this idea in the sdf package."
  },
  {
    "objectID": "posts/csr.html",
    "href": "posts/csr.html",
    "title": "Complete spatial randomness",
    "section": "",
    "text": "&lt; this is a cliche about Tobler’s fist law and things being related in space&gt;. Because of Tobler’s first law, spatial data tend to not follow any specific distribution. So, p-values are sort of…not all that accurate most of the time. P-values in spatial statistics often take a “non-parametric” approach instead of an “analytical” one.\nConsider the t-test. T-tests make the assumption that data are coming from a normal distribution. Then p-values are derived from the cumulative distribution function. The alternative hypothesis, then, is that the true difference in means is not 0.\nIn the spatial case, our alternative hypothesis is generally “the observed statistic different than what we would expect under complete spatial randomness?” But what really does that mean? To know, we have to simulate spatial randomness.\nThere are two approaches to simulating spatial randomness that I’ll go over. One is better than the other. First, I’m going to describe the less good one: bootstrap sampling.\nLoad the super duper cool packages. We create queen contiguity neighbors and row-standardized weights.\nlibrary(sf)\nlibrary(sfdep)\nlibrary(tidyverse)\n\ngrid &lt;- st_make_grid(cellsize = c(1, 1), n = 12, offset = c(0, 0)) |&gt; \n  as_tibble() |&gt; \n  st_as_sf() |&gt; \n  mutate(\n    id = row_number(),\n    nb = st_contiguity(geometry),\n    wt = st_weights(nb)\n    )\nLet’s generate some spatially autocorrelated data. This function is a little slow, but it works.\nnb &lt;- grid[[\"nb\"]]\nwt &lt;- grid[[\"wt\"]]\n\nx &lt;-  geostan::sim_sar(w = wt_as_matrix(nb, wt), rho = 0.78)"
  },
  {
    "objectID": "posts/csr.html#bootstrap-sampling",
    "href": "posts/csr.html#bootstrap-sampling",
    "title": "Complete spatial randomness",
    "section": "Bootstrap sampling",
    "text": "Bootstrap sampling\nUnder the bootstrap approach we are sampling from existing spatial configurations. In our case there are 144 existing neighborhoods. For our simulations, we will randomly sample from existing neighborhoods and then recalculate our statistic. It helps us by imposing randomness into our statistic. We can then repeat the process nsim times. There is a limitation, however. It is that there are only n - 1 possible neighborhood configurations per location.\nHere we visualize a random point and it’s neighbors.\n\n# for a given location create vector indicating position of\n# neighbors and self\ncolor_block &lt;- function(i, nb) {\n  res &lt;- ifelse(1:length(nb) %in% nb[[i]], \"neighbors\", NA)\n  res[i] &lt;- \"self\"\n  res\n}\n\nPlot a point and its neighbors\n\ngrid |&gt; \n  mutate(block = color_block(sample(1:n(), 1), nb)) |&gt; \n  ggplot(aes(fill = block)) +\n  geom_sf() +\n  labs(title = \"Point and it's neighbors\")\n\n\n\n\nFor bootstrap we grab a point and then the neighbors from another point. This function will randomize a nb list object.\n\ncolor_sample_block &lt;- function(i, nb) {\n  index &lt;- 1:length(nb)\n  not_i &lt;- index[-i]\n  \n  sample_block_focal &lt;- sample(not_i, 1)\n  \n  res &lt;- rep(NA, length(index))\n  \n  res[nb[[sample_block_focal]]] &lt;- \"neighbors\"\n  res[i] &lt;- \"self\"\n  res\n}\n\n# visualize it\ngrid |&gt; \n  mutate(block = color_sample_block(sample(1:n(), 1), nb)) |&gt; \n  ggplot(aes(fill = block)) +\n  geom_sf() +\n  labs(title = \"Point and random point's neighbors\")\n\n\n\n\nOften, we will want to create a reference distribution by creating a large number of simulations—typically 999. As the simulations increase in size, we are limited in the amount of samples we can draw. The number of neighborhoods becomes limiting!\nSay we want to look at income distribution in Boston and the only data we have is at the census tract level. I happen to know that Boston has 207 tracts. If we want to do 999 simulations, after the 206th simulation, we will likely have gone through all over the neighborhood configurations!\nHow can we do this sampling? For each observation, we can sample another location, grab their neighbors, and assign them as the observed location’s neighbors."
  },
  {
    "objectID": "posts/csr.html#bootstrap-simulations",
    "href": "posts/csr.html#bootstrap-simulations",
    "title": "Complete spatial randomness",
    "section": "Bootstrap simulations",
    "text": "Bootstrap simulations\nIn sfdep, we use spdep’s nb object. These are lists that store the row position of the neighbors as integer vectors at each element.\n\nIf you want to learn more about neighbors I gave a talk at NY Hackr MeetUp a few months ago that might help.\n\nHere I define a function that samples from the positions (index), then uses that sample to shuffle up the existing neighborhoods and return a shuffled nb object. Note that I add the nb class back to the list.\n\nbootstrap_nbs &lt;- function(nb) {\n  # create index\n  index &lt;- 1:length(nb)\n  # create a resampled index\n  resampled_index &lt;- sample(index, replace = TRUE)\n  # shuffle the neighbors and reassign class\n  structure(nb[resampled_index], class = c(\"nb\", \"list\"))\n}\n\nLet’s compare some observations\n\nnb[1:3]\n\n[[1]]\n[1]  2 13 14\n\n[[2]]\n[1]  1  3 13 14 15\n\n[[3]]\n[1]  2  4 14 15 16\n\nbootstrap_nbs(nb)[1:3]\n\n[[1]]\n[1]  90  91  92 102 104 114 115 116\n\n[[2]]\n[1] 29 30 31 41 43 53 54 55\n\n[[3]]\n[1] 15 16 17 27 29 39 40 41\n\n\nHere we can see the random pattern. Look’s like there is fair amount of clustering of like values.\n\ngrid |&gt; \n  mutate(x = classInt::classify_intervals(x, 7)) |&gt; \n  ggplot(aes(fill = x)) +\n  geom_sf(color = NA, lwd = 0) +\n  scale_fill_brewer(type = \"div\", palette = 5, direction = -1) +\n  theme_void() \n\n\n\n\nWith the weights and the neighbors we can calculate the global Moran. I’ll refer to this as the “observed.” Store it into an object called obs. We’ll need this to calculate a simulated p-value later.\n\nobs &lt;- global_moran(x, nb, wt)\nobs[[\"I\"]]\n\n[1] 0.465776\n\n\n0.47 is a fair amount of positive spatial autocorrelation indicating that like values tend to cluster. But is this due to random chance, or does it depend on where these locations are? Now that we have the observed value of Moran’s I, we can simulate the value under spatial randomness using the bootstrapped sampling. To do so, we bootstrap sample our neighbors, recalculate the weights and then the global Moran. Now, if you’ve read my vignette on conditional permutation, you know what is coming next. We need to create a reference distribution of the global Moran under spatial randomness. To do that, we apply our boot strap nsim times and recalculate the global Moran with each new neighbor list. I love the function replicate() for these purposes.\n\nnsim = 499 \n\n\nAlso, a thing I’ve started doing is assigning scalars / constants with an equals sign because they typically end up becoming function arguments.\n\n\nreps &lt;- replicate(\n  nsim, {\n    nb_sim &lt;- bootstrap_nbs(nb)\n    wt_sim &lt;- st_weights(nb_sim)\n    global_moran(x, nb_sim, wt_sim)[[\"I\"]]\n  }\n)\n\nhist(reps, xlim = c(min(reps), obs[[\"I\"]]))\nabline(v = obs[[\"I\"]], lty = 2)\n\n\n\n\nBootstrap limitations\nThat’s all well and good, but let’s look at this a bit more. Since we’re using the bootstrap approach, we’re limited in the number of unique combinations that are possible. Let’s try something. Let’s calculate the spatial lag nsim times and find the number of unique values that we get.\n\nlags &lt;- replicate(\n  nsim, {\n    # resample the neighbors list\n    nb_sim &lt;- bootstrap_nbs(nb)\n    # recalculate the weights\n    wt_sim &lt;- st_weights(nb_sim)\n    # calculate the lag\n    st_lag(x, nb_sim, wt_sim)\n  }\n)\n\n# cast from matrix to vector\nlags_vec &lt;- as.numeric(lags)\n\n# how many are there?\nlength(lags_vec)\n\n[1] 71856\n\n# how many unique?\nlength(unique(lags_vec))\n\n[1] 144\n\n\nSee this? There are only 144 unique value! That isn’t much! Don’t believe me? Run table(lags_vec). For each location there are only a limited number of combinations that can occur."
  },
  {
    "objectID": "posts/csr.html#conditional-permutation",
    "href": "posts/csr.html#conditional-permutation",
    "title": "Complete spatial randomness",
    "section": "Conditional Permutation",
    "text": "Conditional Permutation\nNow, here is where I want to introduce what I view to be the superior alternative: conditional permutation. Conditional permutation was described by Luc Anselin in his seminal 1995 paper. The idea is that we hold an observation constant, then we randomly assign neighbors. This is like the bootstrap approach but instead of grabbing a random observation’s neighborhood we create a totally new one. We do this be assigning the neighbors randomly from all possible locations.\nLet’s look at how we can program this. For each location we need to sample from an index that excludes the observation’s position. Further we need to ensure that there are the same number of neighbors in each location (cardinality).\n\npermute_nb &lt;- function(nb) {\n  # first get the cardinality\n  cards &lt;- st_cardinalties(nb)\n  # instantiate empty list to fill\n  nb_perm &lt;- vector(mode = \"list\", length = length(nb))\n  \n  # instantiate an index\n  index &lt;- seq_along(nb)\n  \n  # iterate through and full nb_perm\n  for (i in index) {\n    # remove i from the index, then sample and assign\n    nb_perm[[i]] &lt;- sample(index[-i], cards[i])\n  }\n  \n  structure(nb_perm, class = \"nb\")\n}\n\n\nnb[1:3]\n\n[[1]]\n[1]  2 13 14\n\n[[2]]\n[1]  1  3 13 14 15\n\n[[3]]\n[1]  2  4 14 15 16\n\npermute_nb(nb)[1:3]\n\n[[1]]\n[1]  83  98 120\n\n[[2]]\n[1] 124  54 111  69  93\n\n[[3]]\n[1]  49 108  28   4  18\n\n\nNow, let’s repeat the same exercise using conditional permutation.\n\nlags2 &lt;- replicate(\n  nsim, {\n    nb_perm &lt;- permute_nb(nb)\n    st_lag(x, nb_perm, st_weights(nb_perm))\n  }\n)\n\nlags2_vec &lt;- as.numeric(lags2)\n  \nlength(unique(lags2_vec))\n\n[1] 71855\n\n\nThere are farrrrr more unique values. In fact, there is a unique value for each simulation - location pair. If we look at the histograms, the difference is even more stark. The conditional permutation approach actually begins to represent a real distribution.\n\npar(mfrow = c(1, 2))\nhist(lags_vec, breaks = 20) \nhist(lags2_vec, breaks = 20)\n\n\n\n\nSo, this is all for me to say that bootstrapping isn’t it for creating simulated distributions for which to calculate your p-values."
  },
  {
    "objectID": "posts/2023-01-19-raw-strings-in-r.html",
    "href": "posts/2023-01-19-raw-strings-in-r.html",
    "title": "Raw strings in R",
    "section": "",
    "text": "The one thing about Python I actually really like is the ability to use raw strings. Raw strings are super helpful for me because at work I use a windows machine. And windows machines use a silly file path convention. The \\ back slack character is used as the file separator as opposed to the linux / unix / forward slash.\nUsing the backslash is so annoying because it’s also an escape character. In python I can write the following to hard code a file path.\nWhereas in R typically you would have to write:\nSince \\ is an escape character you have to escape it first using itself. So, its annoying. And file.path(\"nav\", \"to\", \"file\", \"path.ext\", fsep = \"\\\\\") is a wee bit cumbersome sometimes."
  },
  {
    "objectID": "posts/2023-01-19-raw-strings-in-r.html#aight.",
    "href": "posts/2023-01-19-raw-strings-in-r.html#aight.",
    "title": "Raw strings in R",
    "section": "Aight.",
    "text": "Aight.\nSo like, you can use raw strings today.\nHow can I get the R-devel news? I’m on the mailing list and get it once a week and it’s like “Re: memory leak in png() ` not this stuff. Tips?\nIt was announced in the news for version 4.0.0.\nThey write:\n\nThere is a new syntax for specifying raw character constants similar to the one used in C++: r”(…)” with … any character sequence not containing the sequence ‘⁠)“⁠’. This makes it easier to write strings that contain backslashes or both single and double quotes. For more details see ?Quotes.\n\nYou can write raw strings using the following formats:\n\nr\"( ... )\"\nr\"{ ... }\"\nr\"[ ... ]\"\nR\"( ... )\"\nR\"{ ... }\"\nR\"[ ... ]\"\n\nYou can even trickier by adding dashes between the quote and the delimter. The dashes need to be symmetrical though. So the following is also valid.\n\nr\"-{ ... }\"-\nr\"--{ ... }--\"\nr\"--{ * _ * }--\"\n\nIt kinda looks like a crab\nAlright so back to the example\n\n r\"{nav\\to\\file\\path.ext}\"\n\n[1] \"nav\\\\to\\\\file\\\\path.ext\"\n\n\nHot damn. Thats nice.\nI freaked out at first though because R prints two backslashes. But if you cat the result they go away. So do not worry.\n\n r\"{nav\\to\\file\\path.ext}\" |&gt; \n  cat()\n\nnav\\to\\file\\path.ext"
  },
  {
    "objectID": "posts/2022-11-23-youtube-videos.html",
    "href": "posts/2022-11-23-youtube-videos.html",
    "title": "YouTube Videos & what not",
    "section": "",
    "text": "Please vote or post a comment in this discuss on what would be helpful for you.\n\nI first made R programming videos when I had the opportunity to teach a remote and asynchronous course called Big Data for Cities. I used the videos as alternative learning material besides a text-book and other required readings.\nI just recently noticed that my video Making your R Markdown Pretty has 16 thousand views. I never thought it would reach that many people! So, if I’ve helped even 0.1% of those people it will have been worth it.\nI’ve started to record some more videos on spatial analaysis in R. I think the spatial anlaysis / statistics videos on youtube are lacking in diversity—with the noteable exception of @GeostatsGuyLectures but he’s more of a environmental scientist :) I will note, though, that the lectures of Luc Anselin are one of the only reason why I am where I am in my knowledge and abilities.\n\n\nThe thing about Anselin’s videos, though, is that they are not about the how of doing it. But they focus more on the theory and the math that sits behind the statistics themselves."
  },
  {
    "objectID": "posts/2022-11-23-youtube-videos.html#i-ask-you",
    "href": "posts/2022-11-23-youtube-videos.html#i-ask-you",
    "title": "YouTube Videos & what not",
    "section": "I ask you!",
    "text": "I ask you!\nWhat do you want to see? What is actually helpful? I’m sure me stumbling and mumbling for 18 minutes on spatial lags can’t be too helpful.\nPlease vote or leave a comment in this discussion."
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html",
    "href": "posts/2020-01-13-gs4-auth.html",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "",
    "text": "This repository contains an example of an R Markdown document that uses googlesheets4 to read from a private Google Sheet and is deployed to RStudio Connect.\nThe path of least resistance for Google auth is to sit back and respond to some interactive prompts, but this won’t work for something that is deployed to a headless machine. You have to do some advance planning to provide your deployed product with a token.\nThe gargle vignette Non-interactive auth is the definitive document for how to do this. The gargle package handles auth for several packages, such as bigrquery, googledrive, gmailr, and googlesheets4.\nThis repo provides a detailed example for the scenario where you are using an OAuth2 user token for a product deployed on RStudio Connect (see vignette section Project-level OAuth cache from which this was adapted). Note that service account tokens are the preferred strategy for a deployed product, but sometimes there are reasons to use a user token."
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html#authenticating",
    "href": "posts/2020-01-13-gs4-auth.html#authenticating",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "Authenticating",
    "text": "Authenticating\nLoad the googlesheets4 package.\n\nlibrary(googlesheets4)\n\nBy default, gargle uses a central token store, outside of the project, which isn’t going to work for us. Instead we specify a project level directory .secrets which will contain our Google token. We will set the gargle_oauth_cache option to refer to this .secrets directory. We can check where the token will be cached with gargle::gargle_oauth_cache().\n\n# designate project-specific cache\noptions(gargle_oauth_cache = \".secrets\")\n\n# check the value of the option, if you like\ngargle::gargle_oauth_cache()\n\nNext we will have to perform the interactive authentication just once. Doing this will generate the token and store it for us. You will be required to select an email account to authenticate with.\n\n# trigger auth on purpose --&gt; store a token in the specified cache\n# a broswer will be opened\ngooglesheets4::sheets_auth()\n\nNow that you have completed the authentication and returned to R, we can double check that the token was cached in .secrets.\n\n# see your token file in the cache, if you like\nlist.files(\".secrets/\")\n\nVoila! Let’s deauthorize in our session so we can try authenticating once more, but this time without interactivity.\n\n# deauth\nsheets_deauth()\n\nIn sheets_auth() we can specify where the token is cached and which email we used to authenticate.\n\n# sheets reauth with specified token and email address\nsheets_auth(\n  cache = \".secrets\",\n  email = \"josiah@email.com\"\n  )\n\nAlternatively, we can specify these in the options() and run the authentication without an arguments supplied. Let’s first deauth in our session to try authenticating again.\n\n# deauth again\nsheets_deauth()\n\n# set values in options\noptions(\n  gargle_oauth_cache = \".secrets\",\n  gargle_oauth_email = \"josiah@email.com\"\n)\n\n# run sheets auth\nsheets_auth()\n\nNow that we are sure that authorization works without an interactive browser session, we should migrate the options into an .Rprofile file. This way, when an R session is spun up the options will be set from session start. Meaning, if you use sheets_auth() within your R Markdown document it will knit without having to open the browser."
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html#deploying-to-connect",
    "href": "posts/2020-01-13-gs4-auth.html#deploying-to-connect",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "Deploying to Connect",
    "text": "Deploying to Connect\nIn order for the deployment to RStudio Connect to work, the .secrets directory and .Rprofile files need to be in the bundle. Be sure to do this from the Add Files button. If you cannot see the files because they are hidden from Finder you cran press cmnd + shift + .. Then publish!"
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html#other-google-platforms",
    "href": "posts/2020-01-13-gs4-auth.html#other-google-platforms",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "Other Google Platforms",
    "text": "Other Google Platforms\nThis same process can be replicated for other packages that use gargle authentication. By virtue of having gargle as the central auth package for Google APIs, the workflow outlined here, and the others in the non-interactive auth vignette, can can be utilized for other google API packages (i.e. googledrive).\n\n# authenticate with googledrive and create a token\ngoogledrive::drive_auth()\n\nThank you to Jenny Bryan for her help editing this!"
  },
  {
    "objectID": "posts/2020-06-12-red-queen.html",
    "href": "posts/2020-06-12-red-queen.html",
    "title": "The Red Queen Effect",
    "section": "",
    "text": "The Red Queen and maintenance of state and society\nIt’s Monday morning. You’re back at work after a few days off. Your inbox is a lot more full than you hoped with 70 emails. Time to get reading and sending. It’s been an hour and you’ve read and sent at least 20 emails but your inbox is still at 70. You’ve been working hard and yet it feels like you’ve gone nowhere. This idea of working really hard but feeling like you’ve gone nowhere is at the center of the Red Queen Effect.\nThere are a number of “Red Queen Effect”s in the scientific literature all of which are inspired from the Red Queen’s race from Lewis Carrols’s Through the Looking Glass.\n\n“Well, in our country,” said Alice, still panting a little, “you’d generally get to somewhere else—if you run very fast for a long time, as we’ve been doing.”\n“A slow sort of country!” said the Queen. “Now, here, you see, it takes all the running you can do, to keep in the same place. If you want to get somewhere else, you must run at least twice as fast as that!” \n\nAlice is running and running and getting nowhere. Much like how as you read emails you get new ones. Daron Acemoglu and James A. Robinson adapt this concept to the evolution of states and connect it to their idea of the Narrow Corridor. The path to a “successful” society is a race between the power of the people and the power of the state. States in which the pace of growth in both the peoples’ power and the power / ability of the state are similar often produce more liberal (in the sense of liberty) nations.\n\nThis graphic is meant to illustrate this race. Getting into the corridor is a game of chase between the power of the state and society. Keeping the balance is delicate act—one that no nation has perfected—which requires society to check the power of the state and the state to provide checks to society. They define the Red Queen as\n\n“The process of competition, struggle and cooperation between state and society”"
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html",
    "href": "posts/2019-03-30-plumber-genius-api.html",
    "title": "genius Plumber API",
    "section": "",
    "text": "get started here\nSince I created genius, I’ve wanted to make a version for python. But frankly, that’s a daunting task for me seeing as my python skills are intermediate at best. But recently I’ve been made aware of the package plumber. To put it plainly, plumber takes your R code and makes it accessible via an API.\nI thought this would be difficult. I was so wrong."
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#using-plumber",
    "href": "posts/2019-03-30-plumber-genius-api.html#using-plumber",
    "title": "genius Plumber API",
    "section": "Using plumber",
    "text": "Using plumber\nPlumber works by using roxygen like comments (#*). Using a single comment, you can define the request type and the end point. Following that you define a function. The arguments to the funciton become the query parameters.\nThe main genius functions only require two main arguments artist and album or song. Making these accessible by API is as simple as:\n#* @get /track\nfunction(artist, song) {\n  genius::genius_lyrics(artist, song)\n}\nWith this line of code I created an endpoint called track to retrieve song lyrics. The two parameters as defined by the anonymous function are artist and song. This means that song lyrics are accessible with a query looking like http://hostname/track?artist=artist_name&song=song_name.\nBut as it stands, this isn’t enough to host the API locally. Save your functions with plumber documentation into a file (I named mine plumber.R).\n\nCreating the API\nCreating the API is probably the easiest part. It takes quite literally, two lines of code. The function plumb() takes two arguments, the file which contains your plumber commented code, and the directory that houses it.\nplumb() creates a router which is “responsible for taking an incoming request, submitting it through the appropriate filters.”\nI created a plumber router which would be used to route income queries.\npr &lt;- plumb(\"plumber.R\")\nThe next step is to actually run the router. Again, this is quite simple by calling the run() method of the pr object. All I need to do is specify the port that the API will listen on, and optionally the host address.\npr$run(port = 80, host = \"0.0.0.0\")\nNow I can construct queries in my browser. An example query is http://localhost/track?artist=andrew%20bird&song=proxy%20war. Sending this request produces a very friendly json output.\n[\n{\"track_title\":\"Proxy War\",\"line\":1,\"lyric\":\"He don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":2,\"lyric\":\"She don't to have get over him\"},\n{\"track_title\":\"Proxy War\",\"line\":3,\"lyric\":\"With all their words preserved forevermore\"},\n{\"track_title\":\"Proxy War\",\"line\":4,\"lyric\":\"You don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":5,\"lyric\":\"She don't have to get over you\"},\n{\"track_title\":\"Proxy War\",\"line\":6,\"lyric\":\"It's true these two have never met before\"},\n{\"track_title\":\"Proxy War\",\"line\":7,\"lyric\":\"At least not in real life\"},\n{\"track_title\":\"Proxy War\",\"line\":8,\"lyric\":\"Where your words cut like a knife\"},\n{\"track_title\":\"Proxy War\",\"line\":9,\"lyric\":\"Conjuring blood, biblical floods\"},\n{\"track_title\":\"Proxy War\",\"line\":10,\"lyric\":\"Looks that stop time\"},\n{\"track_title\":\"Proxy War\",\"line\":11,\"lyric\":\"You don't have to remember\"},\n{\"track_title\":\"Proxy War\",\"line\":12,\"lyric\":\"We forget what memories are for\"},\n{\"track_title\":\"Proxy War\",\"line\":13,\"lyric\":\"Now we store them in the atmosphere\"},\n{\"track_title\":\"Proxy War\",\"line\":14,\"lyric\":\"If you don't want to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":15,\"lyric\":\"You don't have to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":16,\"lyric\":\"It's just what we're calling peer-to-peer\"},\n{\"track_title\":\"Proxy War\",\"line\":17,\"lyric\":\"You don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":18,\"lyric\":\"You don't have to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":19,\"lyric\":\"We store them in the atmosphere\"},\n{\"track_title\":\"Proxy War\",\"line\":20,\"lyric\":\"If you don't want to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":21,\"lyric\":\"She don't have to get over you\"},\n{\"track_title\":\"Proxy War\",\"line\":22,\"lyric\":\"It's true these two have never met before\"},\n{\"track_title\":\"Proxy War\",\"line\":23,\"lyric\":\"At least not in real life\"},\n{\"track_title\":\"Proxy War\",\"line\":24,\"lyric\":\"Where your words cut like a knife\"},\n{\"track_title\":\"Proxy War\",\"line\":25,\"lyric\":\"Conjuring blood, biblical floods\"},\n{\"track_title\":\"Proxy War\",\"line\":26,\"lyric\":\"Looks that stop time\"},\n{\"track_title\":\"Proxy War\",\"line\":27,\"lyric\":\"If you don't want to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":28,\"lyric\":\"You don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":29,\"lyric\":\"If you don't want to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":30,\"lyric\":\"You don't have to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":31,\"lyric\":\"If you want to remember\"},\n{\"track_title\":\"Proxy War\",\"line\":32,\"lyric\":\"If you don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":33,\"lyric\":\"If you don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":34,\"lyric\":\"If you don't want to get over\"}\n]"
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#writing-a-python-wrapper",
    "href": "posts/2019-03-30-plumber-genius-api.html#writing-a-python-wrapper",
    "title": "genius Plumber API",
    "section": "Writing a Python Wrapper",
    "text": "Writing a Python Wrapper\nOne of the appeals of writing an API is that it can be accessed from any language. This was the inspiriation of creating this API. I want to be able to call R using Python. Creating an API is a great intermediary as writing an API wrapper is much easier for me than recreating all of the code that I wrote in R.\nI want to be able to recreate the three main functions of genius. These are genius_lyrics(), genius_album(), and genius_tracklist(). In doing this there are two steps I have to consider. The first is creating query urls, and the second is parsing json.\nTo create the urls, the requests library is used. Next, I created a template for the urls.\nimport requests\nurl_template = \"http://localhost:80/track?artist={}&song={}\"\nThe idea here is that the {} characters will be filled with provided parameters by using the .format() method.\nFor example, if I wanted to get lyrics for Proxy War by Andrew Bird, I would supply \"Andrew Bird\" and \"Proxy War\" as the arguments to format(). It’s important to note that these arguments are taken positionally. The url is created using this method.\nurl = url_template.format(\"andrew bird\", \"proxy war\")\nNow I am at the point where I can ping the server to receive the json. This is accomplished by using the .get() method from requests.\nresponse = requests.get(url)\nThis returns an object that contains the json response. Next, in order to get this into a format that can be analysed, it needs to be parsed. I prefer a Pandas DataFrame, and fortunately Pandas has a lovely read_json function. I will call the .content attribute of the response objectm and feed that into the read_json() function.\nimport pandas as pd\n\nproxy_war = pd.read_json(response.content)\n\nproxy_war.head()\n\n    line    lyric                                   track_title\n0   1   He don't have to get over her               Proxy War\n1   2   She don't to have get over him              Proxy War\n2   3   With all their words preserved forevermore  Proxy War\n3   4   You don't have to get over her              Proxy War\n4   5   She don't have to get over you              Proxy War\nBeautiful. Song lyrics are available through this API and can easily be accessed via python. The next step is to generalize this and the other two functions. The below is the code to create the genius_lyrics() function in python. It works almost identically as in R. However, at this moment it does not have the ability to set the info argument. But this can be changed easily in the original plumber.R file.\n# Define genius_lyrics()\ndef genius_lyrics(artist, song):\n\n    url_template = \"http://localhost:80/track?artist={}&song={}\"\n    \n    url = url_template.format(artist, song)\n\n    response = requests.get(url)\n    \n    song = pd.read_json(response.content)\n    \n    return(song)\nAt this point I’m feeling extremely stoked on the fact that I can use genius with python. Who says R and python practitioners can’t work together?"
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#containerize-with-docker",
    "href": "posts/2019-03-30-plumber-genius-api.html#containerize-with-docker",
    "title": "genius Plumber API",
    "section": "Containerize with Docker",
    "text": "Containerize with Docker\n\nTo make the process of setting up this genius API up easier for those who don’t necessarily interact with R, I created a lightweight-ish Docker container. The idea for this was to be able to pull a Docker image, run a command, and then the API will be available on a local port without having to interact with R at all.\nI’m not the most experience person with creating Docker containers but I can borrow code quite well. Fortunately I came across some wonderful slides from rstudio::conf 2019. Heather Nollis and Jacqueline Nolis presented on “API development with R and TensorFlow at T-Mobile”.\nThis container needs two things: a linux environment and an installation of R with plumber, genius, and its dependencies. An organization called The Rocker Project has created a number of Docker images that are stable and easy to install.\nSince genius relies on many packages from the tidyverse, the rocker/tidyverse image was used. To use their wonderful image, only one line is needed in my Dockerfile.\n# Import existing Docker image\nFROM rocker/tidyverse:3.5.2\nNow, not knowing exactly what I was doing, I copied code from Jacqueline and Heather’s sample Dockerfile in their slides. Their comment says that this is necessary to have the “needed linux libraries for plumber”, I went with it.\n# install needed linux libraries for plumber\nRUN apt-get update -qq && apt-get install -y \\\n  libssl-dev \\\n  libcurl4-gnutls-dev\ngenius and plumber are not part of the tidyverse image and have to be installed manually. The following lines tell Docker to run the listed R commands. For some unknown reason there was an issue with installing genius from CRAN so the repos argument was stated explicitly.\n# Install R packages\nRUN R -e \"install.packages('genius', repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('plumber')\"\nIn addition to the Dockerfile there are two files in my directory which are used to launch the API. The plumber.R and launch_api.R files. These need to be copied into the container. The line COPY / / copies from the location / in my directory to the location / in the container.\nThe Docker image has the libraries and files needed, but it needs to be able to actually launch the API. Since the plumber.R file specifies that the API will be listening on port 80, I need to expose that port in my Docker image using EXPOSE 80.\nThe last part of this is to run the launch_api.R so the API is available. The ENTRYPOINT command tells Docker what to run when the container is launched. In this case ENTRYPOINT [\"Rscript\", \"launch_api.R\"] tells Docker to run the Rscript command with the argument launch_api.R. And with that, the Dockerfile is complete and read to run.\nThe image needs to be built and ran. The simplest way to do this for me was to work from Dockerhub. Thus to run this container only three lines of code are needed!\ndocker pull josiahparry/genius-api:working\n\ndocker build -t josiahparry/genius-api .\n\ndocker run --rm -p 80:80 josiahparry/genius-api\n\nBoom, now you have an API that will be able to use the functionality of genius. If you wish to use Python with the API, I wrote a simple script which creates a nice tidy wrapper around it.\n\nIf anyone is interested in writing a more stable Python library that can call the functionality described above I’d love your help to make genius more readily available to the python community."
  },
  {
    "objectID": "posts/2020-12-09-secure-package-environment.html",
    "href": "posts/2020-12-09-secure-package-environment.html",
    "title": "Secure R Package Environments",
    "section": "",
    "text": "One of the biggest challenges faced by public sector organizations and other security conscious groups is package management. These groups are typically characterized air gapped network environments—i.e. no internet connectivity to the outside world. The purpose of an air gapped network is to get rid of any possibility of an intrusion in your network from an unwanted visitor. Air gapped installations come with some challenges particularly with package management.\nTypically, when you want to install a new package it comes from The Comprehensive R Archive Network (CRAN). While CRAN has a comprehensive testing system as part of their software development life cycle, security teams are still hesitant to trust any domains outside of their network.\nAt RStudio, our solution to this is our RStudio Package Manager or RSPM for short. “RStudio Package Manager is a repository management server to organize and centralize R packages across your team, department, or entire organization.” With RSPM there are a few different ways of addressing this concern. Here I’ll walk through some different approaches. Each subsequent approach is stricter than the last.\n\n\nThe first approach to do this is to install and configure RSPM in your air gapped network. However, RSPM will need special permission to reach out to our sync service (https://rspm-sync.rstudio.com). In many cases security teams are willing to open up an outbound internet connection to just RStudio’s sync service—we hope you trust us!\nThis solution is the easiest as it is a quick configuration. Moreover, packages will only be installed as they are requested. Giving access to the sync service also enables your team to be able to download the latest versions of packages from CRAN.\n\n\n\nThe limitation of the first approach is that is permits a constant outbound internet connection. For some groups, this is a no go. The next best approach then is to have a completely air-gapped CRAN mirror. To do this you will need an internet connection for a brief amount of time—there’s no way to have data magically appear on your server! During the brief period in which your proxy is open you will have to copy all of CRAN, binaries and source, into your server. RSPM provides a utility tool to do this. Once complete, you can close your network again and be confident that there is no possibility of having any connection with the outside world.\nOnce you’ve completed moving data into your server everything behaves as expected—just ensure your options('repos') is set properly. The one downside to this approach is that you will not be able to have access to the latest versions of packages. To rectify this, you can sync on periodic basis.\n\n\n\nOften there are even further restrictions placed on data scientists which limit what packages can be used for their work. We refer to this as a validated set of packages or a curated CRAN. Packages are often “validated” and through that validation process are promoted to the CRAN repository. The upside to this approach is that teams can be confident in the packages their team are using.\nSome approaches to validating the package environment include selecting the top n packages from CRAN (post on identifying those packages here), having a subject matter expert provide a list of preferred packages, or a ticketing system. The ticketing system is the least scalable, most restrictive, and will likely hinder your work. I don’t recommend it.\nThe limitations with this are rather straight forward: your data scientists do not have too much leeway in utilizing packages that may expedite or even enable their work.\n\n\n\nWith approach 3 there are usually two repositories: 1) a mirror of CRAN and 2) a subset of CRAN. While the subset of CRAN is preferred there is nothing stopping users from using the CRAN repository if they know the URL. To prevent this you can implement strict rules with your proxy to prevent users installing from the CRAN mirror thus forcing users to use the subset. In essence, approach 4 is approach 3 but with an enforcement mechanism."
  },
  {
    "objectID": "posts/2020-12-09-secure-package-environment.html#securing-your-r-package-environment",
    "href": "posts/2020-12-09-secure-package-environment.html#securing-your-r-package-environment",
    "title": "Secure R Package Environments",
    "section": "",
    "text": "One of the biggest challenges faced by public sector organizations and other security conscious groups is package management. These groups are typically characterized air gapped network environments—i.e. no internet connectivity to the outside world. The purpose of an air gapped network is to get rid of any possibility of an intrusion in your network from an unwanted visitor. Air gapped installations come with some challenges particularly with package management.\nTypically, when you want to install a new package it comes from The Comprehensive R Archive Network (CRAN). While CRAN has a comprehensive testing system as part of their software development life cycle, security teams are still hesitant to trust any domains outside of their network.\nAt RStudio, our solution to this is our RStudio Package Manager or RSPM for short. “RStudio Package Manager is a repository management server to organize and centralize R packages across your team, department, or entire organization.” With RSPM there are a few different ways of addressing this concern. Here I’ll walk through some different approaches. Each subsequent approach is stricter than the last.\n\n\nThe first approach to do this is to install and configure RSPM in your air gapped network. However, RSPM will need special permission to reach out to our sync service (https://rspm-sync.rstudio.com). In many cases security teams are willing to open up an outbound internet connection to just RStudio’s sync service—we hope you trust us!\nThis solution is the easiest as it is a quick configuration. Moreover, packages will only be installed as they are requested. Giving access to the sync service also enables your team to be able to download the latest versions of packages from CRAN.\n\n\n\nThe limitation of the first approach is that is permits a constant outbound internet connection. For some groups, this is a no go. The next best approach then is to have a completely air-gapped CRAN mirror. To do this you will need an internet connection for a brief amount of time—there’s no way to have data magically appear on your server! During the brief period in which your proxy is open you will have to copy all of CRAN, binaries and source, into your server. RSPM provides a utility tool to do this. Once complete, you can close your network again and be confident that there is no possibility of having any connection with the outside world.\nOnce you’ve completed moving data into your server everything behaves as expected—just ensure your options('repos') is set properly. The one downside to this approach is that you will not be able to have access to the latest versions of packages. To rectify this, you can sync on periodic basis.\n\n\n\nOften there are even further restrictions placed on data scientists which limit what packages can be used for their work. We refer to this as a validated set of packages or a curated CRAN. Packages are often “validated” and through that validation process are promoted to the CRAN repository. The upside to this approach is that teams can be confident in the packages their team are using.\nSome approaches to validating the package environment include selecting the top n packages from CRAN (post on identifying those packages here), having a subject matter expert provide a list of preferred packages, or a ticketing system. The ticketing system is the least scalable, most restrictive, and will likely hinder your work. I don’t recommend it.\nThe limitations with this are rather straight forward: your data scientists do not have too much leeway in utilizing packages that may expedite or even enable their work.\n\n\n\nWith approach 3 there are usually two repositories: 1) a mirror of CRAN and 2) a subset of CRAN. While the subset of CRAN is preferred there is nothing stopping users from using the CRAN repository if they know the URL. To prevent this you can implement strict rules with your proxy to prevent users installing from the CRAN mirror thus forcing users to use the subset. In essence, approach 4 is approach 3 but with an enforcement mechanism."
  },
  {
    "objectID": "posts/2020-12-09-secure-package-environment.html#review",
    "href": "posts/2020-12-09-secure-package-environment.html#review",
    "title": "Secure R Package Environments",
    "section": "Review",
    "text": "Review\nPackage management isn’t easy. It’s even tougher in an offline environment. You’re not going to be able to know exactly what every package does. You’re going to have to make tradeoffs. You can secure your package environment by migrating packages into your own network. You can implement progressively stricter rules to reduce your exposure to potential packages. RStudio Package Manager is a wonderful tool that will make accomplishing all of this a whole lot easier.\nFeel free to reach out to me via twitter or email and we can talk this through."
  },
  {
    "objectID": "posts/2022-05-11-make-your-r-scripts-databricks-notebooks.html",
    "href": "posts/2022-05-11-make-your-r-scripts-databricks-notebooks.html",
    "title": "Make your R scripts Databricks notebooks",
    "section": "",
    "text": "I’ve never had a good reason to deviate from the canonical .R file extension until today.\nAs you may have seen over the past few month from my numerous rage-tweets and Databricks related threads, I’ve been doing a lot of work getting figuring out Databricks as an R user so we can get onboard with adoption here at NPD.\nOne of my biggest qualms about Databricks is that it’s tailored to their notebooks. The notebooks get magical superpowers that aren’t available anywhere else. Notebooks get root permissions, they have access to dbutils, and are the only thing that can actually be scheduled by Databricks outside of a jar file or SparkSQL code.\nI’ve spent quite a bit of time thinking about how we can schedule R scripts through a notebook. If you’re wondering, have the notebook kickoff the R script with a shell command.\nBut, alas, I’ve learned something today. If you connect your Git repo to Databricks through their \"Repos\", you can have your R scripts be accessible as notebooks with quite literally only two changes.\nFirst, R scripts need to have the less-preferable, though equally functional, file extension .r. Second, the first line of the script should be a comment that says # Databricks notebook source. And that’s it. Then once the git repo has been connected, it will recognize those as notebooks.\nIf you want to create cells in your code write a comment # COMMAND ----------—that’s 10 hyphens at the end.\nIf you create a file main.r which contains the body\n# Databricks notebook source\n\nprint(\"Hello world\")\n\n# COMMAND ---------\n\nprint(\"Databricks! I've figured you out—sorta....\")\n\n# COMMAND ---------\n\nprint(\"I feel powerful.\")\nYou will have an R script that is recognized as a notebook by Databricks that can be scheduled using Databricks’ scheduling wizard."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html",
    "href": "posts/2022-10-03-spacetime-representations.html",
    "title": "spacetime representations aren’t good—yet",
    "section": "",
    "text": "My beliefs can be summarized somewhat succinctly.\nWe should not limit space-time data to dates or timestamps.\nThe R ecosystem should always utilize a normalized approach as described above. Further, a representation should use friendly R objects. The friendliest object is a data frame. A new representation should allow context switching between geometries and temporal data. That new representation should always use time-long formats and the geometries should never be repeated.\nA spacetime representation should give users complete and total freedom to manipulate their data as they see fit (e.g. dplyr or data.table operations).\nThe only time to be strict in the format of spacetime data is when statstics are going to be derived from the data."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#background",
    "href": "posts/2022-10-03-spacetime-representations.html#background",
    "title": "spacetime representations aren’t good—yet",
    "section": "Background",
    "text": "Background\nWhile implementing emerging hotspot analysis in sfdep I encountered the need for a formalized spacetime class in R. As my focus in sfdep has been tidyverse-centric functionality, I desired a “tidy” data frame that could be used as a spacetime representation. Moreover, space (in the spacetime representation) should be represented as an sf or sfc object. In sfdep I introduced the new S3 class spacetime based on Edzer Pebesma’s 2012 article “spacetime: Spatio-Temporal Data in R” and Thomas Lin Pederson’s tidygraph package."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#representations-of-spatial-data",
    "href": "posts/2022-10-03-spacetime-representations.html#representations-of-spatial-data",
    "title": "spacetime representations aren’t good—yet",
    "section": "Representations of Spatial Data",
    "text": "Representations of Spatial Data\nBefore describing my preferences in a spacetime representation in R, I want to review possible representations of spacetime data.\nPebesma (2012) outlines three tabular representations of spatio-temporal data.\n\n“Time-wide: Where different columns reflect different moments in time.\n\nSpace-wide: Where different columns reflect different measurement locations or areas.\n\nLong formats: Where each record reflects a single time and space combination.\n\nThe “long format” is what we may consider “tidy” per Wickham (2014). In this case, both time and space are variables with unique combinations as rows.\nPebesma further qualifies spatial data representation into a “sparse grid” and a “full grid.” Say we have a variable X. In a spatio temporal full grid we will store all combinations of time (t) and locations (i) . If Xi is missing at any of those location and time combinations (Xit is missing), the value of X is recorded as a missing value. Whereas in a sparse grid, if there is any missing data, the observation is omitted. Necessarily, in a full grid there will be i x t number of rows. In a sparse grid there will be fewer than i x t rows.\nVery recently in an r-spatial blog post, “Vector Data Cubes”, Edzer describes another approach to representing spacetime using a database normalization approach. Database normalization is a process that reduces redundancy by creating a number of smaller tables containing IDs and values. These tables can then be joined only when needed. When we consider spacetime data, we have repeating geometries across time. It is inefficient to to keep multiple copies of the geometry. Instead, we can keep track of the unique ID of a geometry and store the geometry in another table."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#sfdep-spacetime-representation",
    "href": "posts/2022-10-03-spacetime-representations.html#sfdep-spacetime-representation",
    "title": "spacetime representations aren’t good—yet",
    "section": "sfdep spacetime representation",
    "text": "sfdep spacetime representation\nThe spacetime class in sfdep is in essence a database normalization approach (see above blog post). It is implemented with the database normalization approach and the ergonomics of tidygraph in mind.\nThe objective of the spacetime class in sfdep is to\n\nallow complete freedom of data manipulation via data.frame objects,\nprevent duplication of geometries,\nand provide leeway in what “time” can be defined as.\n\nSimilar to tidygraph, spacetime provides access to two contexts: data and geometry. The data context is a data frame and the geometry context. These are linked based on a unqie identifie that is present in both contexts.\nR code\n\nlibrary(dplyr)\n\ntimes &lt;- seq(\n  Sys.time(), \n  Sys.time() + lubridate::hours(5),\n  length.out = 5\n)\n\nlocations &lt;- c(\"001\", \"002\")\n\ndata_context &lt;- tidyr::crossing(\n  location = locations,\n  time = times\n) |&gt; \n  mutate(value = rnorm(n())) |&gt; \n  arrange(location)\n\n\nlibrary(sf)\n\nLinking to GEOS 3.9.1, GDAL 3.2.3, PROJ 7.2.1; sf_use_s2() is TRUE\n\ngeometry_context &lt;- st_sfc(\n  list(st_point(c(0, 1)), st_point(c(1, 1)))\n  ) |&gt; \n  st_as_sf() |&gt; \n  mutate(location = c(\"001\", \"002\"))\n\nUse the spacetime constructor\n\nlibrary(sfdep)\nspt &lt;- spacetime(\n  .data = data_context,\n  .geometry = geometry_context, \n  .loc_col = \"location\", \n  .time_col = \"time\"\n)\n\nSwap contexts with activate\nactivate(spt, \"geometry\")\nspacetime ────\nContext:`geometry`\n2 locations `location`\n5 time periods `time`\n── geometry context ────────────────────────────────────────────────────────────\nSimple feature collection with 2 features and 1 field Geometry type: POINT Dimension: XY Bounding box: xmin: 0 ymin: 1 xmax: 1 ymax: 1 CRS: NA x location 1 POINT (0 1) 001 2 POINT (1 1) 002\nOne of my very strong beliefs is that temporal data does not, and should not, always be represented as a date or a timestamp. This paradigm is too limiting. What about panel data where you’re measuring cohorts along periods 1 - 10? Should these be represented as dates? No, definitely not. Because of this, sfdep allows you to utilize any numeric column that can be sorted.\n\nPerhaps I’ve just spent too much time listening to ecometricians…\n\nexample of using integers\n\nspacetime(\n  mutate(data_context, period = row_number()),\n  geometry_context, \n  .loc_col = \"location\",\n  .time_col = \"period\"\n)\n\nspacetime ────\n\n\nContext:`data`\n\n\n2 locations `location`\n\n\n10 time periods `period`\n\n\n── data context ────────────────────────────────────────────────────────────────\n\n\n# A tibble: 10 × 4\n   location time                 value period\n * &lt;chr&gt;    &lt;dttm&gt;               &lt;dbl&gt;  &lt;int&gt;\n 1 001      2022-11-15 08:23:08 -0.206      1\n 2 001      2022-11-15 09:38:08 -0.289      2\n 3 001      2022-11-15 10:53:08 -0.275      3\n 4 001      2022-11-15 12:08:08  0.136      4\n 5 001      2022-11-15 13:23:08 -1.48       5\n 6 002      2022-11-15 08:23:08  1.01       6\n 7 002      2022-11-15 09:38:08  2.11       7\n 8 002      2022-11-15 10:53:08 -1.68       8\n 9 002      2022-11-15 12:08:08  0.880      9\n10 002      2022-11-15 13:23:08  0.698     10"
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#qualifiers",
    "href": "posts/2022-10-03-spacetime-representations.html#qualifiers",
    "title": "spacetime representations aren’t good—yet",
    "section": "Qualifiers",
    "text": "Qualifiers\nI don’t think my spacetime class is the panacea. I don’t have the technical chops to make a great data format. I also don’t want to have that burden. Additionally, the class is desgned with lattice data in mind. I don’t think it is sufficient for trajectories or point pattern without repeating locations.\nThere’s a new R package called cubble for spatio-temporal data. I’ve not explored it. It may be better suited to your tidy-centric spatio-temporal data."
  },
  {
    "objectID": "posts/2022-02-12-the-heck-is-a-statistical-moment.html",
    "href": "posts/2022-02-12-the-heck-is-a-statistical-moment.html",
    "title": "The heck is a statistical moment??",
    "section": "",
    "text": "I wrote myself a short story to help me remember what the moments are.\n\n“The first moment I looked at the distribution I thought only of the average. The second, I thought of the variance. Soon after, I thought then of the skewness. Only then did I think about the kurtosis.”\n\nThis all started when reading Luc Anselin’s “Spatial Regression Analysis in R: A Workbook”, I encountered the following:\n\n“Under the normal assumption for the null, the theoretical moments of Moran’s I only depend on the characteristics of the weights matrix.”\n\nThe moments? The what? Under the normal assumption of my study habits I would skip over this word and continue to the next sentence. However, this was critical for understanding the formula for Moran’s I: \\(E[I] = \\frac{-1}{n - 1}\\).\nWikipedia was likely written by the same gatekeepers. I turned to the article on “Method of moments (statistics)” which writes\n\n“Those expressions are then set equal to the sample moments. The number of such equations is the same as the number of parameters to be estimated. Those equations are then solved for the parameters of interest. The solutions are estimates of those parameters.”\n\nNaturally, I turned to twitter to vent.\n\n\nThe use of the word \"moment\" in statistics is cruel.\n\n— jos (@JosiahParry) October 23, 2021\n\n\nThanks to Nicole Radziwill for a very helpful link from the US Naval Academy.\nThe method of moments is no more than simple summary statistics from a distribution. There are four “moments”.\n\nMean\nVariance\nSkew\nKurtosis\n\nWhy would one use these words? To gatekeep, of course. Academia uses needlessly complex language quite often.\nRemember, friends. Use clear and concise language. Let’s remove “moments” from our statistical lexicon."
  },
  {
    "objectID": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html",
    "href": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html",
    "title": "Exploratory Spatial Data Analysis in R",
    "section": "",
    "text": "Geospatial data is becoming increasingly common across domains and industries. Spatial data is no longer only in the hands of soil scientists, meteorologists, and criminologists, but in marketing, retail, finance, etc. It is common for spatial data to be treated as any other tabular data set. However, there is information to be drawn from our data’s relation to space. The standard exploratory data analysis toolkit will not always suffice. In this talk I introduce the basics of exploratory spatial data analysis (ESDA) and the {sfdep} package. {sfdep} builds on the shoulders of {spdep} for spatial dependence, emphasizes the use of simple features and the {sf} package, and integrates within your tidyverse-centric workflow. By the end of this talk users will understand the basics of ESDA and know how to start incorporating these skills in their own work."
  },
  {
    "objectID": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#about-the-talk",
    "href": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#about-the-talk",
    "title": "Exploratory Spatial Data Analysis in R",
    "section": "",
    "text": "Geospatial data is becoming increasingly common across domains and industries. Spatial data is no longer only in the hands of soil scientists, meteorologists, and criminologists, but in marketing, retail, finance, etc. It is common for spatial data to be treated as any other tabular data set. However, there is information to be drawn from our data’s relation to space. The standard exploratory data analysis toolkit will not always suffice. In this talk I introduce the basics of exploratory spatial data analysis (ESDA) and the {sfdep} package. {sfdep} builds on the shoulders of {spdep} for spatial dependence, emphasizes the use of simple features and the {sf} package, and integrates within your tidyverse-centric workflow. By the end of this talk users will understand the basics of ESDA and know how to start incorporating these skills in their own work."
  },
  {
    "objectID": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#recording",
    "href": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#recording",
    "title": "Exploratory Spatial Data Analysis in R",
    "section": "Recording",
    "text": "Recording"
  },
  {
    "objectID": "posts/2019-05-25-introducing-trendyy.html",
    "href": "posts/2019-05-25-introducing-trendyy.html",
    "title": "Introducing trendyy",
    "section": "",
    "text": "trendyy is a package for querying Google Trends. It is build around Philippe Massicotte’s package gtrendsR which accesses this data wonderfully.\nThe inspiration for this package was to provide a tidy interface to the trends data."
  },
  {
    "objectID": "posts/2019-05-25-introducing-trendyy.html#getting-started",
    "href": "posts/2019-05-25-introducing-trendyy.html#getting-started",
    "title": "Introducing trendyy",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstallation\nYou can install trendyy from CRAN using install.packages(\"trendyy\").\n\n\nUsage\nUse trendy() to search Google Trends. The only mandatory argument is search_terms. This is a character vector with the terms of interest. It is important to note that Google Trends is only capable of comparing up to five terms. Thus, if your search_terms vector is longer than 5, it will search each term individually. This will remove the direct comparative advantage that Google Trends gives you.\n\nAdditional arguments\n\n\nfrom: The beginning date of the query in \"YYYY-MM-DD\" format.\nto: The end date of the query in \"YYYY-MM-DD\" format.\n... : any additional arguments that would be passed to gtrendsR::gtrends(). Note that it might be useful to indicate the geography of interest. See gtrendsR::countries for list of possible geographies.\n\n\n\nAccessor Functions\n\nget_interest(): Retrieve interest over time\nget_interest_city(): Retrieve interest by city\nget_interest_country(): Retrieve interest by country\nget_interest_dma(): Retrieve interest by DMA\nget_interest_region(): Retrieve interest by region\nget_related_queries(): Retrieve related queries\nget_related_topics(): Retrieve related topics"
  },
  {
    "objectID": "posts/2019-05-25-introducing-trendyy.html#example",
    "href": "posts/2019-05-25-introducing-trendyy.html#example",
    "title": "Introducing trendyy",
    "section": "Example",
    "text": "Example\nSeeing as I found an interest in this due to the relatively pervasive use of Google Trends in political analysis, I will compare the top five polling candidates in the 2020 Democratic Primary. As of May 22nd, they were Joe Biden, Kamala Harris, Beto O’Rourke, Bernie Sanders, and Elizabeth Warren.\nFirst, I will create a vector of my desired search terms. Second, I will pass that vector to trendy() specifying my query date range from the first of 2019 until today (May 25th, 2019).\n\ncandidates &lt;- c(\"Joe Biden\", \"Kamala Harris\", \"Beto O'Rourke\", \"Bernie Sanders\", \"Elizabeth Warren\")\n\ncandidate_trends &lt;- trendy(candidates, from = \"2019-01-01\", to = Sys.Date())\n\nNow that we have a trendy object, we can print it out to get a summary of the trends.\n\ncandidate_trends\n#&gt; ~Trendy results~\n#&gt; \n#&gt; Search Terms: Joe Biden, Kamala Harris, Beto O'Rourke, Bernie Sanders, Elizabeth Warren\n#&gt; \n#&gt; (&gt;^.^)&gt; ~~~~~~~~~~~~~~~~~~~~ summary ~~~~~~~~~~~~~~~~~~~~ &lt;(^.^&lt;)\n#&gt; # A tibble: 5 × 5\n#&gt;   keyword          max_hits min_hits from       to        \n#&gt;   &lt;chr&gt;               &lt;int&gt;    &lt;int&gt; &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 Bernie Sanders         21        1 2019-01-06 2022-11-06\n#&gt; 2 Beto O'Rourke           1        0 2019-01-06 2022-11-06\n#&gt; 3 Elizabeth Warren        8        1 2019-01-06 2022-11-06\n#&gt; 4 Joe Biden             100        1 2019-01-06 2022-11-06\n#&gt; 5 Kamala Harris          48        1 2019-01-06 2022-11-06\n\nIn order to retrieve the trend data, use get_interest(). Note, that this is dplyr friendly.\n\nget_interest(candidate_trends)\n#&gt; # A tibble: 1,005 × 7\n#&gt;    date                 hits keyword   geo   time                  gprop category      \n#&gt;    &lt;dttm&gt;              &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;                 &lt;chr&gt; &lt;chr&gt;         \n#&gt;  1 2019-01-06 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  2 2019-01-13 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  3 2019-01-20 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  4 2019-01-27 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  5 2019-02-03 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  6 2019-02-10 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  7 2019-02-17 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  8 2019-02-24 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  9 2019-03-03 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt; 10 2019-03-10 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt; # … with 995 more rows\n#&gt; # ℹ Use `print(n = ...)` to see more rows\n\n\nPlotting Interest\n\ncandidate_trends %&gt;% \n  get_interest() %&gt;% \n  ggplot(aes(date, hits, color = keyword)) +\n  geom_line() +\n  geom_point(alpha = .2) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"\", \n       y = \"Relative Search Popularity\",\n       title = \"Google Search Popularity\")\n\n\n\nIt is also possible to view the related search queries for a given set of keywords using get_related_queries().\n\ncandidate_trends %&gt;% \n  get_related_queries() %&gt;% \n  group_by(keyword) %&gt;% \n  sample_n(2)\n#&gt; # A tibble: 10 × 5\n#&gt; # Groups:   keyword [5]\n#&gt;    subject  related_queries value                        keyword          category      \n#&gt;    &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;                        &lt;chr&gt;            &lt;chr&gt;         \n#&gt;  1 +3,450%  rising          klobuchar                    Bernie Sanders   All categories\n#&gt;  2 81       top             joe biden                    Bernie Sanders   All categories\n#&gt;  3 32       top             kamala harris                Beto ORourke     All categories\n#&gt;  4 Breakout rising          beto orourke announcement    Beto ORourke     All categories\n#&gt;  5 Breakout rising          elizabeth warren beer video  Elizabeth Warren All categories\n#&gt;  6 40       top             elizabeth warren net worth   Elizabeth Warren All categories\n#&gt;  7 Breakout rising          joe biden stimulus           Joe Biden        All categories\n#&gt;  8 Breakout rising          joe biden senile             Joe Biden        All categories\n#&gt;  9 Breakout rising          kamala harris husbands       Kamala Harris    All categories\n#&gt; 10 30       top             vice president kamala harris Kamala Harris    All categories\n\n\n\nUseful Resources\n\nHow Trends Data Is Adjusted\nPost by Google News Lab"
  },
  {
    "objectID": "posts/2020-09-07-demographic-change-white-fear-and-social-construction-of-race.html",
    "href": "posts/2020-09-07-demographic-change-white-fear-and-social-construction-of-race.html",
    "title": "Demographic Change, White Fear, and Social Construction of Race",
    "section": "",
    "text": "Two or three weeks ago, somewhere between Carter Dome and Mount Hight in the White Mountains of New Hampshire my friend posed a thought experiment to me. It’s one that I have heard dozens of times whether at a bar top, a fire pit, or an inflatable tube on the Pemigewasset River. It goes something like this.\n\nNote that this is rather extreme example and may not be comfortable for some readers. But thought experiments are supposed to be uncomfortable.\n\n“Take the country Iceland, it has a small population of about 350,000. Say, 100,000 Chinese immigrants move to the country within the period of a year. Is it still Iceland?”\n“Yes, of course.”\n“Okay, say this new population brings a massive baby boom. We know the fertility rate in China is much greater than that of Iceland. This new population has parity with the original 350,000 Icelanders. Making 700,000 total. A massive election is held and there is complete overturn of elected officials and each new official is either from the massive Chinese influx or immediate descendants of the Chinese immigrants. This new government enacts laws that greatly resemble China. Is this country no longer Iceland? What about the Icelandic culture? How can it be preserved? Are you okay with the destruction of a culture?”\nAt this point, for some reason, I’ve always found it tough to provide an argument that can persuade him. Upon reflection, it’s likely because the conversation shifts abruptly from one of pure demographic consideration to one of cultural preservation. The thought experiment feels challenging mainly because the idea of an ethnic and cultural Iceland is portrayed as some static, unshifting, unyielding, monolith. And that is what is at the crux of this.\nThere is an extant fear of racial elimination as a product of demographic growth. Research shows that when white individuals learn about a projected demographic shift from being a majority to minority of the population they show racial preferences for their identified race (source). This has consequences for political party preference as well. White Americans who express concern become more “conservative”—a term I increasingly struggle to use or condone the use of—political views and lead to a great partisan divide (source). Rather prescient, right?\nIceland, while they do not maintain official statistics on race, we do know that approximately 94% of the population are ethnically Icelandic. If we take the complement as entirely people of color (POC) that makes Iceland at most 6% POC. It is likely much less. But what does it mean to be ethnically Icelandic?\nIceland is a discovered land. At the time of its settlement by Norwegians in the 9th century, the land was uninhabited. Icelandic settlers, confirmed by genomic study, are largely from the Scandinavian countries, Ireland, and Scotland. Thus, in the one thousand and change years since its inception, ethnic Icelanders were derived from a melange of Northern Europeans. It would be unreasonable to think that sex would only occur between people of the same homeland indefinitely—that small genetic pool would lead to things like the Hapsburg Jaw. This is illustrative of two points pertinent to the thought experiment.\n\nEthnic Icelanders are descendants of other ethnic groups. Or, put another way, ethnicity is a social construction.\nThe movement of people is a constant in human history.\n\nSay, for the sake of the mental experiment, we give way to the idea that there is an Icelandic culture which can be nailed down and is not in flux. When was it in its purest state? Surely, if we take a snapshot of Icelandic culture of today, it would be unrecognizable to people a century ago, or maybe considerably different than even a few decades ago.\nIf, however, we define culture as an artifact of the history of Iceland—as we rightly should—where the past is important in informing the present, then we must be willing to concede that was is happening presently will become context for understanding Icelandic culture in the future. And that what will happen is soon to be the present and, following, the past (time is a construct I still don’t fully grasp). This is all to say is that culture is a constantly changing (in a state of flux) and that the concept of indefinite cultural preservation is unattainable—and, I’d argue, undesirable. We must accept that populations grow and change; that movement of peoples is a constant in human history; that culture is not a monolith and is constantly shifting; and that ethnicity and race is are myths.\nAt the end of the day, his thought experiment isn’t so much a thought experiment but rather an argument against in-migration. People will move across borders—another social construction, but perhaps with more contemporary utility than that of ethnicity—and the directionality is not only in, but it is also out. For every immigration problem there is an emigration problem. Call me a globalist, but I believe international borders should be more open.\nThere is a scene from Parks and Recreation where Leslie Knope refers to Ann Perkins as racially ambiguous. Today, this might be a problematic statement, but it is a reality of the future.\n\nAll of our descendants within a few generation will be ethnically ambiguous and new ethnic and racial identities will emerge. Fearing change solely based on the color of others’ skin and your preconceptions of them is not a good reason for fearing change. In this thought experiment, the concern ought not be about culture and race. But rather the focus of my argument should have been that of infrastructure. How can we ensure that there is enough housing? Nourishment? Education? Opportunity? You know, the things that truly matter to humanity."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "recent posts",
    "section": "",
    "text": "Josiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWriting S3 head() methods\n\n\na note to self for later\n\n\n\npkg-dev\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a DataFusion CSV reader with arrow-extendr\n\n\nextending R with Arrow and Rust\n\n\n\nrust\n\n\npkg-dev\n\n\nextendr\n\n\narrow\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nEnums in R: towards type safe R\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhere am I in the sky?\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhere am I in the sky?\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExport Python functions in R packages\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExport Python functions in R packages\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Science Across Languages\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nValve: putting R in production\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nR is still fast: a salty reaction to a salty blog post\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s so special about arrays?\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFeeling rusty: counting characters\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFeeling rusty: counting characters\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nRust traits for R users\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nlearning rust\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nJHU talk (slides)\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nRaw strings in R\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically Create Formulas in R\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nYouTube Videos & what not\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFishnets and overlapping polygons\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nComplete spatial randomness\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nspacetime representations aren’t good—yet\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMake your R scripts Databricks notebooks\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Spatial Data Analysis in R\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMy new IDE theme: xxEmoCandyLandxx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nActually identifying R package System Requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe heck is a statistical moment??\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Python on my M1 in under 10 minutes\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSLICED! a brief reflection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n{cpcinema} & associated journey\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nOSINT in 7 minutes\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nAPIs: the language agnostic love story\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nPython & R in production — the API way\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nColor Palette Cinema\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSecure R Package Environments\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is critical race theory, anyways?\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nDemographic Change, White Fear, and Social Construction of Race\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMedium Data and Production API Pipeline\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Red Queen Effect\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExcel in pRod\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nDesign Paradigms in R\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nR Security Concerns and 2019 CRAN downloads\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nNon-interactive user tokens with googlesheets4\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFinding an SPSS {haven}\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Tidy Modeling\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWater Quality Analysis\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n∑ { my parts }\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Trends for Campaigns\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWeb-scraping for Campaigns\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing trendyy\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\ngenius tutorial\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\ngenius Plumber API\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Fallacy of one person, one vote\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cost of Gridlock\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nxgboost feature importance\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n[Not so] generic functions\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nUS Representation: Part I\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing: Letters to a layperson\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nChunking your csv\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nReading Multiple csvs as 1 data frame\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nCoursera R-Programming: Week 2 Problems\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing geniusR\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "posts"
    ]
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html",
    "title": "What is critical race theory, anyways?",
    "section": "",
    "text": "TL;DR critical race theory is a mental framework used for understanding racial inequality that focuses on power imbalances.\nTrump recently suggested that all educational institutions stop teaching critical race theory and, if they fail to do so, lose funding. Like most things Trump does I was appalled. But this came from a different place. This came from a fear of educational censorship and suppression of science.\nBut what the hell is critical race theory, anyways? What does it matter?\nTo understand critical race theory we need to understand where it came from. Critical race theory came from the sociological critical theory. And sociological critical theory came from what is called conflict theory. And conflict theory came from—now don’t lose it—Karl Marx. Let’s try and grasp each theory in chronological order from when they were created to understand how each new theory came to be.\nThe very rough timeline looks like the below."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#historical-materialism",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#historical-materialism",
    "title": "What is critical race theory, anyways?",
    "section": "Historical Materialism",
    "text": "Historical Materialism\nOkay, Karl Marx. Sure, sure, he’s considered the father of communism. But he is really so much more than that. His theories have been absolutely critical to the social sciences for decades.\nMarx believed that there were two broad categories of people in a society, the proletariat and the bourgeoisie. Think of them as the workers and the business owners / managers, respectively. These two groups are always in tension with each other. Workers want better pay and better working conditions. Business owners want to save costs by paying workers less to increase their earnings at the margin (margin is jargon for each additional unit of goods and services). In order to get along they each need to concede to achieve something in the middle ground. This meeting in the middle is how progress is supposedly made.\n\n“Meeting in the middle” is a simplification of the German philosopher Hegel’s idea of a dialectic. A dialectic is described as “thesis, antithesis, synthesis.” In normal people words a dialectic is two opposites (thesis and antithesis) creating something (synthesis)."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#conflict-theory",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#conflict-theory",
    "title": "What is critical race theory, anyways?",
    "section": "Conflict Theory",
    "text": "Conflict Theory\nConflict theory is a bit more general than Marx’s Historical Materialism. Conflict theory suggests that social structures are created from power struggles between different groups of people—not just proletariat and bourgeoisie. One group may have more authority and resources at hand and are using it to the detriment of the other group. We can think of students and teacher, homeless and housed, secular and religious, so on and so forth.\nIn sociology, conflict theory is considered the antithesis (the direct opposite) of functionalism. Functionalism states that each social institution exists to serve some purpose. For example, policing serves the social function of reducing crime. Functionalists tend to think that’s a good thing. Conflict theorists are likely to disagree because they see the power imbalance between the oppressed and the oppressor causing crime. These social institutions that we create tend to reinforce the status quo."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-theory",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-theory",
    "title": "What is critical race theory, anyways?",
    "section": "Critical Theory",
    "text": "Critical Theory\nOkay so we’re getting closer to critical race theory. To recap:\n\nWe can trace critical race theory’s origin to Marx’s Historical Materialism.\nHistorical Materialism says that history is a product of economic struggle between workers and business owners.\nConflict theory generalizes Marx’s theory to say that social structures are created by tension between groups based on interests, resources, and power.\n\nCritical theory is famously defined as\n\n“an essential element in historical effort to create a world which satisfied the needs and powers of men…[and] its goal is man’s emancipation from slavery” - Horkheimer\n\nCritical theory is essentially conflict theory but with an embedded social critique. It’s goal is to improve the human condition by illustrating power imbalance and to improve the conditions of the oppressed. Because of this desire to improve society, critical theory is often used by activists.\n\n\n\n\n\n\nThe above diagram attempts to illustrate the hierarchical nature of these theories."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-race-theory",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-race-theory",
    "title": "What is critical race theory, anyways?",
    "section": "Critical Race Theory",
    "text": "Critical Race Theory\nNow making the leap from conflict theory and critical theory to critical race theory isn’t all that difficult. We understand that there is historical conflict between groups of people. This conflict creates social structures and reinforces the relative power of one group to another. The created social structures perpetuate and often exacerbate inequalities. A lot of times the inequalities between groups, from a philosophical standpoint, are incongruent with our beliefs and need to be rectified or improved.\nCritical race theory, then, is the application of critical theory to the concept of race. In the American context we can understand critical race theory going all the way back to the 17th century.\n\nCritical Race Theory in the United States: the really, really, and I cannot stress this enough, really, short version\n(White) Europeans and then Americans enslaved Africans to be their source of labor. Europeans built institutions to maintain slave trading. The economy of the new world was built almost entirely on “free” labor. Social structures were modified to reinforce ownership rights between people and of people.\nThen one day on July 4th, 1776 this really important document was written which said\n\n“We hold these truths to be self-evident, that all men [people] are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.” – Declaration of Independence\n\nBut that wasn’t true at all. Those words were aspirational at best. In the south, slavery was so deeply embedded that a life without it was deemed problematic enough to start a war over. When Mississippi seceded from the United States prior to the Civil War they enumerated their grievances (much like the Declaration of Independence did) to justify their departure from the US and wrote:\n\n“Our position is thoroughly identified with the institution of slavery—the greatest material interest of the world. Its labor supplies the product which constitutes by far the largest and most important portions of commerce of the earth.” – A Declaration of the Immediate Causes which Induce and Justify the Secession of the State of Mississippi from the Federal Union, 1861\n\nIt wasn’t until 1865 when the 13th Amendment was ratified, 1868 for the 14th Amendment and still that wasn’t enough. In 1896 we had the landmark Plessy v. Ferguson court case which paved the way for Jim Crow laws—a.k.a. American apartheid. It wasn’t until 1964 until the Civil Rights Act was passed. But the passing of laws doesn’t doesn’t change our thinking or our behaviors right away. We still have social structures and institutions which condition and alter our thinking. They don’t disappear with the stroke of a pen.\nIn short, sh!t has been f*cked up in the United States for a very long time and things still aren’t perfect. They’re better. But they’re not good. Critical race theory helps us understand how we got here, why we’re still having problems, and suggests some ways that we can improve it.\nIf we suppress critical race theory we’re also tossing aside critical theory, conflict theory, and historical materialism. These are theories that have helped us make sense of the world for over one hundred years. If we allow this censorship we’re giving way to an unjust power imbalance—the very thing these theories help us understand. If we throw away textbooks we’re walking down the same path that was taken in 1930s Germany and in 1960s China. If we allow suppression of free thought we’re giving way to authoritarians who don’t want you to challenge injustices."
  },
  {
    "objectID": "posts/2021-04-04-OSINT-youtube.html",
    "href": "posts/2021-04-04-OSINT-youtube.html",
    "title": "OSINT in 7 minutes",
    "section": "",
    "text": "As I was perusing the bowels of YouTube, as one does, I stumbled across a video titled “Using My Python Skills To Punish Credit Card Scammers”. The video is both whimsical, informative, and largely educational. It teaches us about:\n\nweb scraping / resource discovery\nsending API requests\nscaling requests using process threading.\n\n\n\nEngineer Man received a phishing text message. He opens up the url in an incognito browser and begins to walk us through the steps. He fills out the fake Amazon form and begins toying with the submit form with fake data. Upon reading the error messages, he gets the sense that the form might be utilizing a real service. And doing so is never free. He takes this as an opportunity to exploit the hacker’s use of the service.\nHe does this by taking advantage of my latest favorite thing—API requests. Engineer Man walks us through the process of opening up the web developer tools in a browser and using them to our advantage to understand the network traffic. He identifies the POST endpoint and shows us how to make a request from the information that is provided.\nBut this isn’t where it ends. Engineer Man’s face lights up as he begins to talk about threading. He utilizes the threading library to spin up 50 infinite loops.\nThis video sparked my excitement as it is a wonderful expample of open source intelligence (OSINT). OSINT is the process of collecting publicly all available data to be used by attackers. While attacking someone is not OSINT, the information gathering process is. This video is a wonderful resource for illustrating how to utilize the browser tools and public, though, hidden API endpoints."
  },
  {
    "objectID": "posts/2018-11-15-introducing-letters-to-a-layperson.html",
    "href": "posts/2018-11-15-introducing-letters-to-a-layperson.html",
    "title": "Introducing: Letters to a layperson",
    "section": "",
    "text": "I have been in the world of academia for nearly five years now. During this time I’ve read countless scholarly journal articles that I’ve struggled to wrap my head around. The academic language is riddled with obfuscating words like “milieux” and “nexus” which are often used to explain relatively simple concepts in a not so simple language. I’ve had to train myself to understand the academic language and translate it to regular people (layperson) speak.\nThe academic dialect is often associated with the “elitist media” (see Chomsky) which has recently been blamed for creating a strong divide in American politics—as we’ve seen since the beginning of the 2016 presidential primaries. Many words, phrases, and ideas have been shrouded by this language barrier. I have been trying to break down this barrier for myself for years now. I feel like I’ve only made a small dent. I have been trying to educate myself, a layperson, on these phrases and concepts.\nAs an undergraduate student I studied sociology and anthropology, but I found that I was enamored with economics, political science, urban theory, data science, psychology, and other disciplines. Across these fields there are identical concepts represented by different words or phrases—an ever frustrating thing. This is a barrier to understanding these fields. You must know certain ideas, words, and histories to understand the content.\nI have been collecting notes on these ideas and often revisit them to remind myself of what they are, what they mean, and why they exist. These notes were created for a myself, a layperson.\nIn this series of forthcoming posts, I will write about concepts that I wish I knew better in a language that I can understand. I call this collection of posts Letters To a Layperson, inspired by the phenomenal book Letters to a Young Contrarian by Christopher Hitchens."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html",
    "title": "The Fallacy of one person, one vote",
    "section": "",
    "text": "On October 6, 2018, the US Senate voted 50–48 in favor of the appointment of Associate Justice Brett Kavanaugh. This led many pundits to point out a “disconnect” between the Senate and the body politic. The 50 senators who voted “yea” represent only 44% of the nation’s population. The year prior, Supreme Court Justice Neil Gorsuch was confirmed by 54 senators representing approximately 45% of the population. This trend of increasing control by a decreasing portion of the constituency has been attributed to a rise in partisanship.\nSince the mid 90’s Dr. Frances E. Lee has been developing a body of literature on Senate apportionment, and her book Sizing Up the Senate has become part of the current political milieu (Vox, CNN, New York Times). The book discusses, among many things, the relevant historical context surrounding the creation and organization of the Senate at the constitutional Convention. In her 1998 paper “The Consequences of Senate Apportionment for the Geographic Distribution of Federal Funds” (Lee, 1998), Dr. Lee describes the “representation index”, a measure to quantify the over- or underrepresentation of a state in the US senate. In the formulation described in the paper, “the index is simply the ratio of the state’s actual population to 1/50th of the nation’s population” (Lee, 1998). In the formulation described in the paper, “the index is simply the ratio of the state’s actual population to 1/50th of the nation’s population” (Lee, 1998). It is written mathematically as \\(\\frac{State \\ Population}{1/50 \\ * \\ US \\ Population}\\). This creates a number between \\((0, \\infty)\\). As it is put in Sizing up the Senate,\nMany of Lee’s analyses utilize this index, and it has proved useful in temporal comparisons and modeling. However, it does not seem immediately capable of effectively evaluating other legislative bodies such as the House of Representatives. Here I will put forth an adaptation of this measure. That measure will then be adjusted to evaluate the House of Representatives. The House model will then be generalized to fit any representative body."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#interpretibility",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#interpretibility",
    "title": "The Fallacy of one person, one vote",
    "section": "Interpretibility",
    "text": "Interpretibility\nThe representation index has three main shortcomings, each of which are simple to address. First, the index produces a counter-intuitive number. An index of greater than 1 indicates an underrepresented state, and vice versa. Second, the interpretation of a middle value of 1 is useful for the “one person, one vote” standard. But the index is a divergent measure where 1 is the middle with the bounds (0,∞). Often, when one thinks of divergence, it is from an origin, or 0. Third, the index has a lower limit of 0 and no upper bound. This inhibits comparisons in both directions.\nTo illustrate the point let’s take the populations of California and Wyoming based on 2010 Census figures.\n\n\n\n\nRegion\nPopulation\n\n\n\n\nWY\n563626\n\n\nCA\n37253956\n\n\nUS\n308745538\n\n\n\n\nIf we calculate the representation index for these places, we get:\n\n\n\n\nRegion\nPopulation\nRepresentation Index\n\n\n\n\nWY\n563626\n0.09\n\n\nCA\n37253956\n6.03\n\n\n\n\nIn this example, California has a representation index of 6. This means that it is vastly underrepresented, whereas Wyoming has an index value of nearly 0 meaning it is vastly overrepresented. To interpret this, we must remember that a larger value actually means less representation.\nBut if we invert our formula, we obtain a more informative number.\n\n\n\n\nRegion\nPopulation\nNew Rep. Index\n\n\n\n\nWY\n563626\n10.96\n\n\nCA\n37253956\n0.17\n\n\n\n\nIn this table it is clear that Wyoming is overrepresented and California is underrepresented. But still, in evaluating these numbers we are required to do the mental math to contextualize the divergence from a middle value. California has a value of 0.83 less than the one person, one vote standard. To handle this, we can center the score around 0 by simply subtracting 1.\n\n\n\n\nRegion\nPopulation\nRepresentation Index\nNew Rep. Index\n\n\n\n\nWY\n563626\n0.09\n9.96\n\n\nCA\n37253956\n6.03\n-0.83\n\n\n\n\nThus the formula for the new representation index is \\(\\dfrac{\\dfrac{1}{50} \\ * \\ US \\ Population}{State \\ Population} - 1\\). When the measures are compared, we see that the initial measure used by Dr. Lee emphasizes underrepresentation of California, whereas the measure I have suggested emphasises the overrepresentation of Wyoming. From here on I will refer to these as the underrepresentation index (URI) and the overrepresentation index (ORI), respectively.\nThe URI and ORI are informative, but both are biased in scale. The bounds of the URI are \\((0,\\infty)\\) and the ORI are \\((-1, \\infty)\\). A value is needed that can simultaneously demonstrate the over- and underrepresentation of a state.\nThe ORI can be altered slightly to create this balanced measure. By taking the natural logarithm of the ratio 1/50th of the US population to a state’s population, a divergent scale naturally occurs. When the ratio is equal to 1 (or adhering to the one person, one vote standard), the value becomes 0. When the denominator is less than the numerator (or when the state has a smaller share of population than its share of votes), the value is positive and vice versa. Thus we arrive at the formula \\(\\ln\\Bigg({\\dfrac{\\dfrac{1}{50} \\ * \\ US \\ Population}{State \\ Population}}\\Bigg)\\). The following table compares these three measures.\n\n\n\n\nRegion\nPopulation\nURI\nORI\nNew Rep. Index\n\n\n\n\nWY\n563626\n0.09\n9.96\n2.39\n\n\nCA\n37253956\n6.03\n-0.83\n-1.80\n\n\n\n\nThe new representation index can be generalized to the House of Representatives or any other representative body. In the following sections, the representation index is adapted to the House of Representatives, the California Assembly and Senate, and the New Hampshire House and Senate."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#representation-index-and-the-house-of-representatives",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#representation-index-and-the-house-of-representatives",
    "title": "The Fallacy of one person, one vote",
    "section": "Representation Index and the House of Representatives",
    "text": "Representation Index and the House of Representatives\nRepresentation in the House of Representative is proportional meaning that a state has a number of legislative representatives proportional to its population. For example, if a state were to have 50% of the nation’s population it should represent 50% of the legislative body. This is the principle that the representation index evaluates.\nIn the above adaptation of the representation index, the nation’s population is divided by 50. This would be the population of a single state if every state had the same number of citizens. Then, that number is scaled (divided) by the state’s actual population, and the logarithm of the result is the representation index. Thus if a state’s population is exactly equal to 1/50th of the nation’s population, its representation in the Senate is proportional.\nTo adapt this measure to the House, we must think about how the relationship between proportional representation and population can be expressed numerically. As mentioned above, proportional representation would mean that a state comprising 50% of the national population would likewise comprise 50% of the House’s representatives. The ratio of these two proportions is 1, which creates a similar comparison to Lee’s ratio of 1/50th of national population to state population. This is the motivation for a formula of a representation index for the House of Representatives. The new formula, then, is \\(\\ln\\Big(\\frac{State \\ share \\ of \\ reps}{State \\ share \\ of \\ pop.}\\Big)\\).\n\n\n\n\n\n\nIn this case, if the share of the population is smaller than the share of representatives, the index is inflated, meaning the state is overrepresented. If the share of population is greater than the share of representatives, the index is deflated, meaning the state is underrepresented. This index ranges from \\((-\\infty, \\infty)\\)."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#generalizing-the-representation-index",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#generalizing-the-representation-index",
    "title": "The Fallacy of one person, one vote",
    "section": "Generalizing the Representation Index",
    "text": "Generalizing the Representation Index\nThe representation index for the House of Representatives is written in such a way that it can be adapted for any representative body. The formula evaluates equality of the share of representatives and the share of the total constituency. In general, the formula can be written as \\(\\ln\\Big(\\frac{\\%\\ share \\ of \\ reps}{\\% \\ share \\ of \\ constituency}\\Big)\\).\nTo illustrate, let’s use this formula to calculate the representation index of the Assembly and Senate of California. In 2011, after the most recent census, California redrew its districts. The data used in this demonstration are from the LA Times. The California Assembly and Senate have 80 and 40 members respectively each representing one district.\nFor this example, I consider a difference of 5% in either direction as adhering to the one person, one vote principle. To illustrate this, if the ratio is \\(\\frac{1%}{0.95}\\) the index score is 0.05. Alternatively, if the ratio is \\(\\frac{1}{1.05}\\), the index score is -0.05.\n\n\n\n\n\n\n\n\n\n\n\n\nThe above example demonstrates the use of the representation index for both houses of the California legislature. This is good news as it demonstrates that the state upholds the Equal Protection Clause of the Fourteenth Amendment and adheres to the Supreme Court decision Reynolds v. Sims, in which the court held that state districts must be proportionally drawn (unlike US Senate districts).\nAs Wikipedia states, “[given California]’s large population and relatively small legislature, the Assembly has the largest population per representative ratio of any lower house legislature in the United States; only the federal U.S. House of Representatives has a larger ratio.” California’s representative body differs greatly from that of, for example, New Hampshire.\nNew Hampshire has arguably the most unique lower house legislature of any state: there are 400 representatives from 204 districts. House districts also include what are called floterial districts, areas that represent multiple municipalities. The legality of such districts has been disputed in the state Supreme Court, but nonetheless they persist, and as a result, New Hampshire has one of the smallest constituent-to-representative ratios in the nation. This results in overrepresentation for almost every municipality.\n\n\n\n\n\n\nThe above chart illustrates this phenomenon. Interestingly, the most populous cities and towns in the region are represented according to the one person, one vote paradigm. When applied to the state senate, the results are much different.\n\n\n\n\n\n\nThe representation index for the New Hampshire Senate trends toward underrepresentation. The median value is shown with a dotted red line. It is apparent that the representation of the Senate of New Hampshire is not as equally representative as that of California. The population distribution across the state is highly unequal with a vast majority residents living close to the Maine and Massachusetts borders plausibly contributing to this inequality."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#further-directions",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#further-directions",
    "title": "The Fallacy of one person, one vote",
    "section": "Further Directions",
    "text": "Further Directions\nThe ability to compare representation across governing bodies has large implications for comparative political analysis. Further development of the representation index allows scholars and researchers to compare constituency representation among similar bodies—as demonstrated with the case of California and New Hampshire.\nThe new formulation of the representation index is conducive to inter-governmental body analysis. This is possible by the index’s ability to place bodies of different size on the same scale. A result of this is the ability to perform hypothesis testing among groups. As a motivating example, the representation indexes of states are compared along partisanship lines.\n\n\n\n\n\n\nState Senate representation indexes were calculated using the general representation index formula for all 50 states. A two-sample t-test was performed comparing states with two Republican senators to those with two Democratic senators. In doing so, we fail to reject the null hypothesis \\(( \\ t(39) = 1.117, \\ p = 0.27 \\ )\\) that there is a difference of representation index based solely on partisanship.\n\n\n\n\n\n\n\nParty\nn\nMean\nSD\nSE\n\n\n\n\nDemocrat\n21\n0.22\n0.97\n0.21\n\n\nRepublican\n23\n0.67\n0.99\n0.21\n\n\nSplit / Other\n6\n0.67\n1.28\n0.52\n\n\nTotal\n50\n0.48\n1.02\n0.14\n\n\n\n\nIt has been demonstrated that the representation index is an informative measure that can be utilized to examine over and underrepresentation of a governing body. This new formulation of the representation index is useful in its ability to evaluate both over and under-representation and to compare different political entities. One could imagine, for example, a comparison of constituency representation between the United States and France’s upper and lower legislative houses.\n\nI would like to thank Harley Phleger for his help in editing this piece. The cogency of this writing would be entirely lacking without his superb editing abilities. If you are in need of an editor, give him a message. Also, his poetry is wonderful."
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html",
    "href": "posts/2018-01-27-introducing-geniusr.html",
    "title": "Introducing geniusR",
    "section": "",
    "text": "knitr::opts_chunk$set(eval=FALSE)"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#install-and-load-the-package",
    "href": "posts/2018-01-27-introducing-geniusr.html#install-and-load-the-package",
    "title": "Introducing geniusR",
    "section": "Install and load the package",
    "text": "Install and load the package\n\ndevtools::install_github(\"josiahparry/geniusR\")\n\nLoad the package:\n\nlibrary(geniusR)\nlibrary(tidyverse) # For manipulation"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#whole-albums",
    "href": "posts/2018-01-27-introducing-geniusr.html#whole-albums",
    "title": "Introducing geniusR",
    "section": "Whole Albums",
    "text": "Whole Albums\ngenius_album() allows you to download the lyrics for an entire album in a tidy format. There are two arguments artists and album. Supply the quoted name of artist and the album (if it gives you issues check that you have the album name and artists as specified on Genius).\nThis returns a tidy data frame with three columns:\n\ntitle: track name\ntrack_n: track number\ntext: lyrics\n\n\nemotions_math &lt;- genius_album(artist = \"Margaret Glaspy\", album = \"Emotions and Math\")\nemotions_math"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#multiple-albums",
    "href": "posts/2018-01-27-introducing-geniusr.html#multiple-albums",
    "title": "Introducing geniusR",
    "section": "Multiple Albums",
    "text": "Multiple Albums\nIf you wish to download multiple albums from multiple artists, try and keep it tidy and avoid binding rows if you can. We can achieve this in a tidy workflow by creating a tibble with two columns: artist and album where each row is an artist and their album. We can then iterate over those columns with purrr:map2().\nIn this example I will extract 3 albums from Kendrick Lamar and Sara Bareilles (two of my favotire musicians). The first step is to create the tibble with artists and album titles.\n\nalbums &lt;-  tibble(\n  artist = c(\n    rep(\"Kendrick Lamar\", 3), \n    rep(\"Sara Bareilles\", 3)\n    ),\n  album = c(\n    \"Section 80\", \"Good Kid, M.A.A.D City\", \"DAMN.\",\n    \"The Blessed Unrest\", \"Kaleidoscope Heart\", \"Little Voice\"\n    )\n)\n\nalbums\n\nNo we can iterate over each row using the map2 function. This allows us to feed each value from the artist and album columns to the genius_album() function. Utilizing a map call within a dplyr::mutate() function creates a list column where each value is a tibble with the data frame from genius_album(). We will later unnest this.\n\n## We will have an additional artist column that will have to be dropped\nalbum_lyrics &lt;- albums %&gt;% \n  mutate(tracks = map2(artist, album, genius_album))\n\nalbum_lyrics\n\nNow when you view this you will see that each value within the tracks column is &lt;tibble&gt;. This means that that value is infact another tibble. We expand this using tidyr::unnest().\n\n# Unnest the lyrics to expand \nlyrics &lt;- album_lyrics %&gt;% \n  unnest(tracks) %&gt;%    # Expanding the lyrics \n  arrange(desc(artist)) # Arranging by artist name \n\nhead(lyrics)"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#song-lyrics",
    "href": "posts/2018-01-27-introducing-geniusr.html#song-lyrics",
    "title": "Introducing geniusR",
    "section": "Song Lyrics",
    "text": "Song Lyrics\n\ngenius_lyrics()\nGetting lyrics to a single song is pretty easy. Let’s get in our ELEMENT. and checkout DNA. by Kendrick Lamar. But first, note that the genius_lyrics() function takes two main arguments, artist and song. Be sure to spell the name of the artist and the song correctly.\n\nDNA &lt;- genius_lyrics(artist = \"Kendrick Lamar\", song = \"DNA.\")\n\nDNA\n\nThis returns a tibble with three columns title, text, and line. However, you can specifiy additional arguments to control the amount of information to be returned using the info argument.\n\ninfo = \"title\" (default): Return the lyrics, line number, and song title.\ninfo = \"simple\": Return just the lyrics and line number.\ninfo = \"artist\": Return the lyrics, line number, and artist.\ninfo = \"all\": Return lyrics, line number, song title, artist."
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#tracklists",
    "href": "posts/2018-01-27-introducing-geniusr.html#tracklists",
    "title": "Introducing geniusR",
    "section": "Tracklists",
    "text": "Tracklists\ngenius_tracklist(), given an artist and an album will return a barebones tibble with the track title, track number, and the url to the lyrics.\n\ngenius_tracklist(artist = \"Basement\", album = \"Colourmeinkindness\")"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#nitty-gritty",
    "href": "posts/2018-01-27-introducing-geniusr.html#nitty-gritty",
    "title": "Introducing geniusR",
    "section": "Nitty Gritty",
    "text": "Nitty Gritty\ngenius_lyrics() generates a url to Genius which is fed to genius_url(), the function that does the heavy lifting of actually fetching lyrics.\nI have not figured out all of the patterns that are used for generating the Genius.com urls, so errors are bound to happen. If genius_lyrics() returns an error. Try utilizing genius_tracklist() and genius_url() together to get the song lyrics.\nFor example, say “(No One Knows Me) Like the Piano” by Sampha wasn’t working in a standard genius_lyrics() call.\n\npiano &lt;- genius_lyrics(\"Sampha\", \"(No One Knows Me) Like the Piano\")\n\nWe could grab the tracklist for the album Process which the song is from. We could then isolate the url for (No One Knows Me) Like the Piano and feed that into `genius_url().\n\n# Get the tracklist for \nprocess &lt;- genius_tracklist(\"Sampha\", \"Process\")\n\n# Filter down to find the individual song\npiano_info &lt;- process %&gt;% \n  filter(title == \"(No One Knows Me) Like the Piano\")\n\n# Filter song using string detection\n# process %&gt;% \n#  filter(stringr::str_detect(title, coll(\"Like the piano\", ignore_case = TRUE)))\n\npiano_url &lt;- piano_info$track_url\n\nNow that we have the url, feed it into genius_url().\n\ngenius_url(piano_url, info = \"simple\")"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#generative-functions",
    "href": "posts/2018-01-27-introducing-geniusr.html#generative-functions",
    "title": "Introducing geniusR",
    "section": "Generative functions",
    "text": "Generative functions\nThis package works almost entirely on pattern detection. The urls from Genius are (mostly) easily reproducible (shout out to Angela Li for pointing this out).\nThe two functions that generate urls are gen_song_url() and gen_album_url(). To see how the functions work, try feeding an artist and song title to gen_song_url() and an artist and album title to gen_album_url().\n\ngen_song_url(\"Laura Marling\", \"Soothing\")\n\n\ngen_album_url(\"Daniel Caesar\", \"Freudian\")\n\ngenius_lyrics() calls gen_song_url() and feeds the output to genius_url() which preforms the scraping.\nGetting lyrics for albums is slightly more involved. It first calls genius_tracklist() which first calls gen_album_url() then using the handy package rvest scrapes the song titles, track numbers, and song lyric urls. Next, the song urls from the output are iterated over and fed to genius_url().\nTo make this more clear, take a look inside of genius_album()\n\ngenius_album &lt;- function(artist = NULL, album = NULL, info = \"simple\") {\n\n  # Obtain tracklist from genius_tracklist\n  album &lt;- genius_tracklist(artist, album) %&gt;%\n\n    # Iterate over the url to the song title\n    mutate(lyrics = map(track_url, genius_url, info)) %&gt;%\n\n    # Unnest the tibble with lyrics\n    unnest(lyrics) %&gt;%\n    \n    # Deselect the track url\n    select(-track_url)\n\n\n  return(album)\n}\n\n\nNotes:\nAs this is my first “package” there will be many issues. Please submit an issue and I will do my best to attend to it.\nThere are already issues of which I am present (the lack of error handling). If you would like to take those on, please go ahead and make a pull request. Please contact me on Twitter."
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html",
    "title": "genius tutorial",
    "section": "",
    "text": "knitr::opts_chunk$set(eval = FALSE)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#introducing-genius",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#introducing-genius",
    "title": "genius tutorial",
    "section": "Introducing genius",
    "text": "Introducing genius\nYou want to start analysing song lyrics, where do you go? There have been music information retrieval papers written on the topic of programmatically extracting lyrics from the web. Dozens of people have gone through the laborious task of scraping song lyrics from websites. Even a recent winner of the Shiny competition scraped lyrics from Genius.com.\nI too have been there. Scraping websites is not always the best use of your time. genius is an R package that will enable you to programatically download song lyrics in a tidy format ready for analysis. To begin using the package, it first must be installed, and loaded. In addition to genius, we will need our standard data manipulation tools from the tidyverse.\n\ninstall.packages(\"genius\")\n\n\nlibrary(genius)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#single-song-lyrics",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#single-song-lyrics",
    "title": "genius tutorial",
    "section": "Single song lyrics",
    "text": "Single song lyrics\nThe simplest method of extracting song lyrics is to get just a single song at a time. This is done with the genius_lyrics() function. It takes two main arguments: artist and song. These are the quoted name of the artist and song. Additionally there is a third argument info which determines what extra metadata you can get. The possible values are title, simple, artist, features, and all. I recommend trying them all to see how they work.\nIn this example we will work to retrieve the song lyrics for the upcoming musician Renny Conti.\n\nfloating &lt;- genius_lyrics(\"renny conti\", \"people floating\")\nfloating"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#album-lyrics",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#album-lyrics",
    "title": "genius tutorial",
    "section": "Album Lyrics",
    "text": "Album Lyrics\nNow that you have the intuition for obtaining lyrics for a single song, we can now create a larger dataset for the lyrics of an entire album using genius_album(). Similar to genius_lyrics(), the arguments are artist, album, and info.\nIn the exercise below the lyrics for Snail Mail’s album Lush. Try retrieving the lyrics for an album of your own choosing.\n\nlush &lt;- genius_album(\"Snail Mail\", \"Lush\")\nlush"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#adding-lyrics-to-a-data-frame",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#adding-lyrics-to-a-data-frame",
    "title": "genius tutorial",
    "section": "Adding Lyrics to a data frame",
    "text": "Adding Lyrics to a data frame\n\nMultiple songs\nA common use for lyric analysis is to compare the lyrics of one artist to another. In order to do that, you could potentially retrieve the lyrics for multiple songs and albums and then join them together. This has one major issue in my mind, it makes you create multiple object taking up precious memory. For this reason, the function add_genius() was developed. This enables you to create a tibble with a column for an artists name and their album or song title. add_genius() will then go through the entire tibble and add song lyrics for the tracks and albums that are available.\nLet’s try this with a tibble of three songs.\n\nthree_songs &lt;- tribble(\n  ~ artist, ~ title,\n  \"Big Thief\", \"UFOF\",\n  \"Andrew Bird\", \"Imitosis\",\n  \"Sylvan Esso\", \"Slack Jaw\"\n)\n\nsong_lyrics &lt;- three_songs %&gt;% \n  add_genius(artist, title, type = \"lyrics\")\n\nsong_lyrics %&gt;% \n  count(artist)\n\n\n\n\nMultiple albums\nadd_genius() also extends this functionality to albums.\n\nalbums &lt;- tribble(\n  ~ artist, ~ title,\n  \"Andrew Bird\", \"Armchair Apocrypha\",\n  \"Andrew Bird\", \"Things are really great here sort of\"\n)\n\nalbum_lyrics &lt;- albums %&gt;% \n  add_genius(artist, title, type = \"album\")\n\nalbum_lyrics\n\nWhat is important to note here is that the warnings for this function are somewhat informative. When a 404 error occurs, this may be because that the song does not exist in Genius. Or, that the song is actually an instrumental which is the case here with Andrew Bird.\n\n\nAlbums and Songs\nIn the scenario that you want to mix single songs and lyrics, you can supply a column with the type value of each row. The example below illustrates this. First a tibble with artist, track or album title, and type columns are created. Next, the tibble is piped to add_genius() with the unquote column names for the artist, title, and type columns. This will then iterate over each row and fetch the appropriate song lyrics.\n\nsong_album &lt;- tribble(\n  ~ artist, ~ title, ~ type,\n  \"Big Thief\", \"UFOF\", \"lyrics\",\n  \"Andrew Bird\", \"Imitosis\", \"lyrics\",\n  \"Sylvan Esso\", \"Slack Jaw\", \"lyrics\",\n  \"Movements\", \"Feel Something\", \"album\"\n)\n\nmixed_lyrics &lt;- song_album %&gt;% \n  add_genius(artist, title, type)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#self-similarity",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#self-similarity",
    "title": "genius tutorial",
    "section": "Self-similarity",
    "text": "Self-similarity\nAnother feature of genius is the ability to create self-similarity matrices to visualize lyrical patterns within a song. This idea was taken from Colin Morris’ wonderful javascript based Song Sim project. Colin explains the interpretation of a self-similarity matrix in their TEDx talk. An even better description of the interpretation is available in this post.\nTo use Colin’s example we will look at the structure of Ke$ha’s Tik Tok.\nThe function calc_self_sim() will create a self-similarity matrix of a given song. The main arguments for this function are the tibble (df), and the column containing the lyrics (lyric_col). Ideally this is one line per observation as is default from the output of genius_*(). The tidy output compares every ith word with every word in the song. This measures repetition of words and will show us the structure of the lyrics.\n\ntik_tok &lt;- genius_lyrics(\"Ke$ha\", \"Tik Tok\")\n\ntt_self_sim &lt;- calc_self_sim(tik_tok, lyric, output = \"tidy\")\n\ntt_self_sim\n\ntt_self_sim %&gt;% \n  ggplot(aes(x = x_id, y = y_id, fill = identical)) +\n  geom_tile() +\n  scale_fill_manual(values = c(\"white\", \"black\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        axis.text = element_blank()) +\n  scale_y_continuous(trans = \"reverse\") +\n  labs(title = \"Tik Tok\", subtitle = \"Self-similarity matrix\", x = \"\", y = \"\", \n       caption = \"The matrix displays that there are three choruses with a bridge between the last two. The bridge displays internal repetition.\")"
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "",
    "text": "Installing python has never been an easy task for me. I remember back in 2016 I wanted to learn how to use pyspark and thus python, I couldn’t figure out how to install python so I gave up. In graduate school I couldn’t install python so I used a docker container my professor created and never changed a thing. When working at RStudio I used the Jupyter Lab instance in RStudio Workbench when I couldn’t install it locally.\nNow, I want to compare pysal results to some functionality I’ve written in R. To do that, I need a python installation. I’ve heard extra horror stories about installing Python on the new Mac M1 chip—which I have.\nPrior to installing, I took to twitter for suggestions. I received the phenomenal tweet below encouraging me to install with {reticulate}1 which was absolutely phenomenal advice."
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#installing-python",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#installing-python",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Installing Python",
    "text": "Installing Python\nThe steps to install python, at least for me, was very simple.\n\nInstall reticulate\nInstall miniconda\n\ninstall.packages(\"reticulate\")\nreticulate::install_miniconda()\nThat’s it. That’s all it took."
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#creating-my-first-conda-environment",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#creating-my-first-conda-environment",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Creating my first conda environment",
    "text": "Creating my first conda environment\nAfter installing python, I restarted R, and began building my first conda environment. I created a conda environment called geo for my geospatial work. I installed libpysal, geopandas, and esda. These installed every other dependency I needed–e.g. pandas, and numpy.\nreticulate::conda_create(\"geo\")\nreticulate::use_condaenv(\"geo\")\nreticulate::conda_install(\"geo\", c(\"libpysal\", \"geopandas\", \"esda\"))"
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#using-my-conda-environment",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#using-my-conda-environment",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Using my conda environment",
    "text": "Using my conda environment\nTo begin using my new conda environment, I opened up a fresh R session and a fresh R Markdown document. In my first code chunk I told reticulate which conda environment to use. Then my following code chunks were python which opened up the python repl. Make sure that you start your code chunk with ```{python}\n\nreticulate::use_condaenv(\"geo\")\n\nIn the following example I utilize esda to calculate a local join count.\n\nimport libpysal\nimport geopandas as gpd\nfrom esda.join_counts_local import Join_Counts_Local\n\nfp = libpysal.examples.root + \"/guerry/\" + \"Guerry.shp\" \n\nguerry_ds = gpd.read_file(fp)\nguerry_ds['SELECTED'] = 0\nguerry_ds.loc[(guerry_ds['Donatns'] &gt; 10997), 'SELECTED'] = 1\n\nw = libpysal.weights.Queen.from_dataframe(guerry_ds)\n\nLJC_uni = Join_Counts_Local(connectivity=w).fit(guerry_ds['SELECTED'])\n\nLJC_uni.p_sim\n\n## array([  nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan, 0.435,   nan, 0.025, 0.025,   nan, 0.328,\n##          nan,   nan,   nan,   nan,   nan,   nan, 0.342,   nan, 0.334,\n##          nan,   nan,   nan,   nan,   nan,   nan, 0.329,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan, 0.481,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan, 0.02 ,   nan,   nan,   nan,   nan,   nan, 0.125,\n##          nan, 0.043,   nan,   nan])"
  },
  {
    "objectID": "projects/pkgs/sysreqs.html",
    "href": "projects/pkgs/sysreqs.html",
    "title": "R package system requirements",
    "section": "",
    "text": "GitHub repo\nRelated blog post\n\nThe goal of sysreqs is to make it easy to identify R package system dependencies. There are two components to this package: an “API” and a wrapper package.\nThis API and package is based on rstudio/r-system-requirements and the API client for RStudio Package Manager. The functionality is inspired by pak::pkg_system_requirements()."
  },
  {
    "objectID": "projects/writing/uitk.html",
    "href": "projects/writing/uitk.html",
    "title": "Urban Informatics Toolkit",
    "section": "",
    "text": "The Urban Informatics Toolkit (uitk) is an open text book I wrote with the intention of teaching first year graduate students in Urban Informatics the R programming language.\nWithin it are two years of study in the Urban Informatics Program at Northeastern University, five years of self-directed education in R, two years of teaching R, and innumerable hours learning R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "Welcome to my personal website. I always aspire to keep pumping out content, but I don’t.\nHere you will find my “recent” blog posts, possibly outdated biographical information, my socials, and links to projects I have made or worked on.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na note to self for later\n\n\n\npkg-dev\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n\n\n\n\n\n\nextending R with Arrow and Rust\n\n\n\nrust\n\n\npkg-dev\n\n\nextendr\n\n\narrow\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\n\n\n\n\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\n\n\n\n\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\n\n\n\n\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\n\n\n\n\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\n\n\n\n\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\n\n\n\n\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\n\n\n\n\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\n\n\n\n\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\n\n\n\n\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "index.html#letters-to-a-layperson-myself",
    "href": "index.html#letters-to-a-layperson-myself",
    "title": "Josiah Parry",
    "section": "",
    "text": "Welcome to my personal website. I always aspire to keep pumping out content, but I don’t.\nHere you will find my “recent” blog posts, possibly outdated biographical information, my socials, and links to projects I have made or worked on.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na note to self for later\n\n\n\npkg-dev\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n\n\n\n\n\n\nextending R with Arrow and Rust\n\n\n\nrust\n\n\npkg-dev\n\n\nextendr\n\n\narrow\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\n\n\n\n\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\n\n\n\n\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\n\n\n\n\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\n\n\n\n\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\n\n\n\n\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\n\n\n\n\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\n\n\n\n\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\n\n\n\n\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\n\n\n\n\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "employment: Senior Product Engineer @ Esri\neducation:\n\nMS Urban Informatic, Northeastern University (2020)\nBA Sociology, Plymouth State University\n\nMinor, General Mathematics\nProfessional Certificate GIS\nI am a Senior Product Engineer on the Spatial Analysis team at Esri. Previously, I was at The NPD Group as a Research Analyst where I worked to modernize our data science infrastructure to use Databricks, Docker, and Spark. Before that, I was at RStudio, PBC on the customer success team enabling public sector adoption of data science tools. In 2020 I received my master’s degree in Urban Informatics from Northeastern University following my bachelor’s degree in sociology with focuses in geographic information systems and general mathematics from Plymouth State University in 2018.",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "about",
    "section": "Contact me",
    "text": "Contact me\nIf you want to get in contact with me please send me an email at josiah.parry at gmail dot com.\n\n\ntalks i’ve given\n\n\nExploratory Spatal Data Analysis in the tidyverse\n\nJuly 28th, 2022 rstudio::conf(2022L)\n\nExploratory Spatial Data Analysis in R\n\nRecording\nApril 28th, 2022\n\nAPIs: you’re probably not using them and why you probably should\n\nGovernment Advances in Statistical Programming\nNovember 6th, 2020\n\n“Old Town Road” Rap or Country?: Putting R in Production with Tidymodels, Plumber, and Shiny\n\nBoston useR group\nDecember 10th, 2019\n\nTidy Lyrical Analysis\n\nBoston useR group\nJuly 17th, 2018\n\nNewfound Lake Landscape Value Analysis: Exploring the efficacy of PPGIS, NESTVAL 2016\n\nNew England St. Lawrence River Valley regional American Associations of Geographers Conference\n2016",
    "crumbs": [
      "about"
    ]
  }
]