[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "Over the years I’ve built or contributed to quite a few things. These are just a select few of them.\n\nRust libraries\n\n\n\n\n\n\n\nValve\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narrow_extendr\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nserde_esri\n\n\n\nrust\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsfconversions\n\n\n\nrust\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nPackages & contributions\n\n\n\n\n\n\n\nR package system requirements\n\n\n\npackage\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR-ArcGIS Bridge\n\n\n\nrust\n\n\nr\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenius (retired)\n\n\n\npackage\n\n\nmusic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nh3o for H3 indexing\n\n\n\nrust\n\n\nr\n\n\nspatial\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrsgeo (deprecated)\n\n\n\nrust\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\n\npackage\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspdep (contributor)\n\n\n\npackage\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nThings I’ve written\n\n\n\n\n\n\n\nR for Progressive Campaigns\n\n\n\nwriting\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban Informatics Toolkit\n\n\n\nwriting\n\n\nurban-informatics\n\n\ntextbook\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmirrr - song genre classification\n\n\n\nwriting\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "projects"
    ]
  },
  {
    "objectID": "projects/writing/mirr.html",
    "href": "projects/writing/mirr.html",
    "title": "mirrr - song genre classification",
    "section": "",
    "text": "Tidy Music Information Retrieval was a bookdown project I wrote back in 2019 that created a stacked ensemble model that predicted musical genre from song audio features and song lyrics.\nI created 3 models. The first utilized LDA text classification outputs from song lyrics as inputs into a classification model. The second used song audio features from spotify. The third model used the outputs of both to create a stacked ensemble model."
  },
  {
    "objectID": "projects/pkgs/genius.html",
    "href": "projects/pkgs/genius.html",
    "title": "genius (retired)",
    "section": "",
    "text": "genius was my first R package and my first digital child. genius provided a way to programatically access song lyrics from genius.com. This included ways to fetch single songs, albums, and track lists. It was, at one point, integrated with the spotifyr package.\nThis R package was the basis of much of my learning of the data science ecosystem. Including APIs and Docker. Creating stacked ensemble machine learning models using LDA outputs as model inputs (see online guide)\n\n GitHub"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html",
    "href": "posts/2023-11-10-enums-in-r/index.html",
    "title": "Enums in R: towards type safe R",
    "section": "",
    "text": "Hadley Wickham has recently dropped a new draft section of his book Tidy Design Principles on enumerations and their use in R.\nIn short, enumerations enumerate (list out) the possible values that something might take on. In R we see this most often in function signatures where an argument takes a scalar value but all possible values are listed out."
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#enums-in-r",
    "href": "posts/2023-11-10-enums-in-r/index.html#enums-in-r",
    "title": "Enums in R: towards type safe R",
    "section": "Enums in R",
    "text": "Enums in R\nA good example is the cor() function from the base package stats.\n\nargs(cor)\n\nfunction (x, y = NULL, use = \"everything\", method = c(\"pearson\", \n    \"kendall\", \"spearman\")) \nNULL\n\n\nThe possible values for method are \"pearson\", \"kendall\", or \"spearman\" but all values are listed inside of the function definition.\nInside of the function, though, match.arg(method) is used to ensure that the provided value to the method argument is one of the provided values.\nHadley makes the argument that we should prefer an enumeration to a boolean flag such as TRUE or FALSE. I agree!\nA real world example\nA post on mastodon makes a point that the function sf::st_make_grid() has an argument square = TRUE where when set to FALSE hexagons are returned.\n\n\nIn this case, it’s very clear that an enum would be better! For example we can improve the signature like so:\nst_make_grid &lt;- function(x, grid_shape = c(\"square\", \"hexagon\"), ...) {\n  # ensure only one of the provided grid shapes are used\n  match.arg(grid_shape)\n  # ... rest of function \n}"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#enums-in-rust",
    "href": "posts/2023-11-10-enums-in-r/index.html#enums-in-rust",
    "title": "Enums in R: towards type safe R",
    "section": "Enums in Rust",
    "text": "Enums in Rust\nWhen I first started using rust enums made no sense to me. In Rust, enums are a first class citizen that are treated as their own thing.\n\n\nI’m not really sure what to call things in Rust. Are they all objects?\nWe make them by defining the name of the enum and the variants they may take on.\nenum GridShape {\n  Square,\n  Hexagon\n}\nNow you can use this enum GridShape to specify one of two types: Square or Hexagon. Syntactically, this is written GridShape::Square and GridShape::Hexagon.\nEnums are very nice because we can match on the variants and do different things based on them. For example we can have a function like so:\nfn which_shape(x: GridShape) {\n    match x {\n        GridShape::Square =&gt; println!(\"We have a square!\"),\n        GridShape::Hexagon =&gt; println!(\"Hexagons are the bestagons\")\n    }\n}\nIt takes an argument x which is a GridShape enum. We match on the possible variants and then do something.\n\n\nInside of the match statement each of the possible variants of the enum have to be written out. These are called match arms. The left side lists the variant where as the right portion (after =&gt;) indicates what will be executed if the left side is matched (essentially if the condition is true).\nWith this function we can pass in specific variants and get different behavior.\n\n\nGridShape::Hexagon\nGridShape::Square\n\n\n\nwhich_shape(GridShape::Hexagon)\n#&gt; Hexagons are the bestagons\n\n\nwhich_shape(GridShape::Square)\n#&gt; We have a square!"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#making-an-s7-enum-object-in-r",
    "href": "posts/2023-11-10-enums-in-r/index.html#making-an-s7-enum-object-in-r",
    "title": "Enums in R: towards type safe R",
    "section": "Making an S7 enum object in R",
    "text": "Making an S7 enum object in R\nI think R would benefit from having a “real” enum type object. Having a character vector of valid variants and checking against them using match.arg() or rlang::arg_match() is great but I think we can go further.\n\n\nSince learning Rust, I think having more strictness can make our code much better and more robust. I think adding enums would be a good step towards that\nI’ve prototyped an Enum type in R using the new S7 object system that might point us towards what an enum object in the future might look like for R users.\nDesign of an Enum\nFor an enum we need to know what the valid variants are and what the current value of the enum is. These would be the two properties.\nAn enum S7 object must also make sure that a value of an Enum is one of the valid variants. Using the GridShape enum the valid variants would be \"Square\" and \"Hexagon\". A GridShape enum could not take, for example, \"Circle\" since it is not a listed variant.\nUsing an abstract class\nTo start, we will create an abstract S7 class called Enum.\n\n“_an abstract class is a generic class (or type of object) used as a basis for creating specific objects that conform to its protocol, or the set of operations it supports” — Source\n\nThe Enum class will be used to create other Enum objects.\n\nlibrary(S7)\n\n# create a new Enum abstract class\nEnum &lt;- new_class(\n  \"Enum\",\n  properties = list(\n    Value = class_character,\n    Variants = class_character\n  ),\n  validator = function(self) { \n    if (length(self@Value) != 1L) {\n      \"enum value's are length 1\"\n    } else if (!(self@Value %in% self@Variants)) {\n      \"enum value must be one of possible variants\"\n    }\n  }, \n  abstract = TRUE\n)\n\nIn this code chunk we specify that there are 2 properties: Value and Variant each must be a character type. Value will be the value of the enum. It would be the right hand side of GridShape::Square in Rust’s enum, for example. Variants is a character vector of all of the possible values it may be able to take on. The validator ensures that Value must only have 1 value. It also ensures that Value is one of the enumerated Variants. This Enum class will be used to generate other enums and cannot be instantiated by itself.\nWe can create a new enum factory function with the arguments:\n\n\nenum_class the class of the enum we are creating\n\nvariants a character vector of the valid variant values\n\n\n# create a new enum constructor \nnew_enum_class &lt;- function(enum_class, variants) {\n  new_class(\n    enum_class,\n    parent = Enum,\n    properties = list(\n      Value = class_character,\n      Variants = new_property(class_character, default = variants)\n    ),\n    constructor = function(Value) {\n      new_object(S7_object(), Value = Value, Variants = variants)\n    }\n  )\n}\n\n\n\nNote that the constructor here only takes a Value argument. We do this so that users cannot circumvent the pre-defined variants.\nWith this we can now create a GridShape enum in R!\n\nGridShape &lt;- new_enum_class(\n  \"GridShape\",\n  c(\"Square\", \"Hexagon\")\n)\n\nGridShape\n\n&lt;GridShape&gt; class\n@ parent     : &lt;Enum&gt;\n@ constructor: function(Value) {...}\n@ validator  : &lt;NULL&gt;\n@ properties :\n $ Value   : &lt;character&gt;\n $ Variants: &lt;character&gt;\n\n\nThis new object will construct new GridShape enums for us.\n\nGridShape(\"Square\")\n\n&lt;GridShape&gt;\n @ Value   : chr \"Square\"\n @ Variants: chr [1:2] \"Square\" \"Hexagon\"\n\n\nWhen we try to create a GridShape that is not one of the valid variants we will get an error.\n\nGridShape(\"Triangle\")\n\nError: &lt;GridShape&gt; object is invalid:\n- enum value must be one of possible variants\n\n\nMaking a print method\nFor fun, I would like Enum objects to print like how I would use them in Rust. To do this we can create a custom print method\n\n# print method for enums\n# since this is an abstract class we get the first class (super)\n# to print\nprint.Enum &lt;- function(x, ...) {\n  cat(class(x)[1], \"::\", x@Value, sep = \"\")\n  invisible(x)\n}\n\nSince Enums will only ever be a sub-class we can confidently grab the first element of the class(enum_obj) which is the super-class of the enum. We paste that together with the value of the enum.\n\nsquare  &lt;- GridShape(\"Square\")\nsquare\n\nGridShape::Square"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#drawing-even-more-from-rust",
    "href": "posts/2023-11-10-enums-in-r/index.html#drawing-even-more-from-rust",
    "title": "Enums in R: towards type safe R",
    "section": "Drawing even more from Rust",
    "text": "Drawing even more from Rust\nRust enums are even more powerful than what I briefly introduced. Each variant of an enum can actually be typed!!! Take a look at the example from The Book™.\nenum Message {\n    Quit,\n    Move { x: i32, y: i32 },\n    Write(String),\n    ChangeColor(i32, i32, i32),\n}\nIn this enum there are 4 variants. The first Quit doesn’t have any associated data with it. But the other three do! The second one Move has two fields x and y which contain integer values. Write is a tuple with a string in it and ChangeColor has 3 integer values in its tuple. These can be extracted.\nA silly example function that illustrates how each value can be used can be\nfn which_msg(x: Message) {\n    match x {\n        Message::Quit =&gt; println!(\"I'm a quitter\"),\n        Message::Move { x, y } =&gt;  println!(\"Move over {x} and up {y}\"),\n        Message::Write(msg) =&gt; println!(\"your message is: {msg}\"),\n        Message::ChangeColor(r, g, b) =&gt;  println!(\"Your RGB ({r}, {g}, {b})\"),\n    }\n}\nWhen a variant with data is passed in the values can be used. For example\nwhich_msg(Message::ChangeColor(0, 155, 200));\n#&gt; Your RGB (0, 155, 200)\nExtending it to R\nWhat would this look like if we extended it to an R based enum object? I suspect the Variants would be a list of prototypes such as those from {vctrs}. The Value would have to be validated against all of the provided prototypes to ensure that it is one of the provided types.\nI’m not sure how I would code this up, but I think that would be a great thing to have."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html",
    "href": "posts/2023-07-06-r-is-still-fast.html",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "",
    "text": "There’s this new blog post making the rounds making some claims about why they won’t put R into production. Most notably they’re wheeling the whole “R is slow thing” again. And there are few things that grind my gears more than that type of sentiment. It’s almost always ill informed. I find that to be the case here too.\nI wouldn’t have known about this had it 1) not mentioned my own Rust project Valve and 2) a kind stranger inform me about it on mastodon.\nI’ve collected my reactions below as notes and sundry bench marks and bullet points."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#tldr",
    "href": "posts/2023-07-06-r-is-still-fast.html#tldr",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "TL;DR",
    "text": "TL;DR\n\nThere is a concurrent web server for R and I made it Valve\n\nPython is really fast at serializing json and R is slower\nPython is really slow at parsing json and R is so so soooo much faster\nTo handle types appropriately, sometimes you have to program\nThere are mock REST API testing libraries {httptest} and {webmockr}\n\nDemand your service providers to make the tools you want\nAsk and you shall receive\nR can go into production\nPLEASE JUST TRY VALVE YOU’LL LOVE IT"
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#production-services",
    "href": "posts/2023-07-06-r-is-still-fast.html#production-services",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Production Services",
    "text": "Production Services\nThere are so many people using R in production in so many ways across the world. I wish Posit did a better job getting these stories out. As a former RStudio employee, I personally met people putting R in production in most amazing ways. From the US Department of State, Defense, Biotech companies, marketing agencies, national lotteries, and so much more. The one that sticks out the most is that Payam M., when at Tabcorp massively scaled their system using Plumber APIs and Posit Connect to such a ridiculous scale I couldn’t even believe.\nGunicorn, Web Servers, and Concurrency\n\n“R has no widely-used web server to help it run concurrently.”\n\nThe premise of this whole blog post stems from the fact that there is no easily concurrent web server for R. Which is true and is the reason I built Valve. It doesn’t meet the criteria of widely used because no one has used it. In part, because of posts like this that discourage people from using R in production.\nTypes and Conversion\nThere’s this weird bit about how 1 and c(1, 2) are treated as the same class and unboxing of json. They provide the following python code as a desirable pattern for processing data.\nx = 1\ny = [1, 2]\n\njson.dump(x, sys.stdout)\n#&gt; 1\njson.dump(y, sys.stdout)\n#&gt; [1, 2]\nThey want scalars to be unboxed and lists to remain lists. This is the same behavior as jsonlite, though.\n\njsonlite::toJSON(1, auto_unbox = TRUE)\n\n1 \n\njsonlite::toJSON(1:2, auto_unbox = TRUE)\n\n[1,2] \n\n\nThere’s a difference here: one that the author fails to recognize is that a length 1 vector is handled appropriately. What the author is saying is that they don’t like that R doesn’t behave the same way as Python. You, as a developer should be able to guarantee that a value is length 1. It’s easy. length(x) == 1, or if you want is_scalar &lt;- function(x) length(x) == 1. This is the type system in R and json libraries handle the “edge case” appropriately. There is nothing wrong here. The reprex is the same as the python library.\n\n“R (and Plumber) also do not enforce types of parameters to your API, as opposed to FastAPI, for instance, which does via the use of pydantic.”\n\nPython does not type check nor does FastAPI. You opt in to type checking with FastAPI. You can do the same with Plumber. A quick perusal of the docs will show you this. Find the @param section. There is some concessions here, though. The truthful part here is the type annotations do type conversion for only dynamic routes. Which, I don’t know if FastAPI does. Type handling for static parameters is an outstanding issue of mine for plumber since 2021.\nI’ve followed up on the issue above and within minutes the maintainer responded. There is an existing PR to handle this issue.\nThis just goes to show if that you want something done in the open source world, just ask for it. More than likely its already there or just waiting for the slight nudge from someone else.\nWhile I know it’s not “seemless” adding an as.integer() and a stopifnot(is.integer(n)) isn’t the wildest thing for a developer to do.\nThere is a comparison between type checking in R and Python with the python example using type hints which are, again, opt-in. An unfair comparison when you say “if you don’t use the opt-in features of plumber but use the opt-in features of FastAPI, FastAPI is better.”\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/types\")\nasync def types(n: int) -&gt; int:\n  return n * 2\nClients and Testing\nI haven’t done much testing of API endpoints but I do know that there are two de facto packages for this:\n\n\n{httptest} and\n\n{webmockr}.\n\nThese are pretty easy to find. Not so sure why they weren’t mentioned or even tested.\nPerformance\nJSON serialization is a quite interesting thing to base performance off of. I’ve never seen how fast pandas serialization is. Quite impressive! But, keep with me, because you’ll see, this is fibbing with benchmarks.\nI do have thoughts on the use of jsonlite and it’s ubiquity. jsonlite is slow. I don’t like it. My belief is that everyone should use {jsonify} when creating json. It’s damn good.\nSo, when I run these bench marks on my machine for parsing I get:\n\nmicrobenchmark::microbenchmark(\n  jsonify = jsonify::to_json(iris),\n  jsonlite = jsonlite::toJSON(iris),\n  unit = \"ms\", \n  times = 1000\n)\n\nWarning in microbenchmark::microbenchmark(jsonify = jsonify::to_json(iris), :\nless accurate nanosecond times to avoid potential integer overflows\n\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\nUnit: milliseconds\n     expr      min       lq      mean    median       uq      max neval cld\n  jsonify 0.258218 0.265024 0.3224672 0.2698005 0.280850 35.20330  1000  a \n jsonlite 0.346245 0.360759 0.4169715 0.3719110 0.399012 20.00181  1000   b\n\n\nA very noticable difference in using jsonify over jsonlite. The same benchmark using pandas is holy sh!t fast!\nfrom timeit import timeit\nimport pandas as pd\n\niris = pd.read_csv(\"fastapi-example/iris.csv\")\n\nN = 1000\n\nprint(\n  \"Mean runtime:\", \n  round(1000 * timeit('iris.to_json(orient = \"records\")', globals = locals(), number = N) / N, 4), \n  \"milliseconds\"\n)\n#&gt; Mean runtime: 0.0721 milliseconds\nNow, this is only half the story. This is serialization. What about the other part? Where you ingest it.\nHere, I will also say, again, that you shouldn’t use jsonlite because it is slow. Instead, you should use {RcppSimdJson}. Because its\n\n\nLet’s run another benchmark\n\njsn &lt;- jsonify::to_json(iris)\n\nmicrobenchmark::microbenchmark(\n  simd = RcppSimdJson::fparse(jsn),\n  jsonlite = jsonlite::fromJSON(jsn),\n  unit = \"ms\",\n  times = 1000\n)\n\nUnit: milliseconds\n     expr      min        lq       mean   median        uq      max neval cld\n     simd 0.052275 0.0551040 0.06672631 0.057933 0.0634885 4.316275  1000  a \n jsonlite 0.433165 0.4531525 0.48931155 0.467359 0.4919795 4.352232  1000   b\n\n\nRcppSimdJson is ~8 times faster than jsonlite.\nLet’s do a similar benchmark in python.\njsn = iris.to_json(orient = \"records\")\n\nprint(\n  \"Mean runtime:\", \n  round(1000 * timeit('pd.read_json(jsn)', globals = locals(), number = N) / N, 4), \n  \"milliseconds\"\n)\n#&gt; Mean runtime: 1.2629 milliseconds\nPython is 3x slower than jsonlite in this case and 25x slower than RcppSimdJson. Which is very slow. While serializing is an important thing to be fast in, so is parsing the incoming json you are receiving. How nice it is to show only half the story! Use RcppSimdJson and embarrass pandas’ json parsing.\nIntegration with Tooling\nI have literally no idea about any of these except Launchdarkly because one of my close homies worked there for years. These are all paid services so I’m not sure how they work :)\nI would say to checkout Posit Connect for deploying R and python into production. But if your only use case is to deploy a single model, then yeah, I’d say that’s overkill.\nI wish more companies would create tooling for R and their services. The way to do this, is to lean into using R in production and demanding (not asking) providers to make wrappers for them. When you pay for a service, you have leverage. Use it. I think too many people fall over when what they need isn’t there immediately. Be sure to be the squeeky wheel that makes change.\nI also think that if you’re in the position where you can make a wrapper for something, you should. I did this when using Databricks in my last role and provided them with a lot of feedback. Have they taken it? I’m not sure. I’m not there to harass them anymore."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#workarounds",
    "href": "posts/2023-07-06-r-is-still-fast.html#workarounds",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Workarounds",
    "text": "Workarounds\nThese are good workarounds. I would suggest looking at ndexr.io as a way to scale these R based services as well. They utilize the NGINX approach described here."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#addenda",
    "href": "posts/2023-07-06-r-is-still-fast.html#addenda",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Addenda",
    "text": "Addenda\nClearly, this is where I care a lot. I am the author of Valve. Valve is exactly what the author was clamoring for in the beginning of the blog post. It is a web server that runs Plumber APIs in parallel written in Rust using Tokio, Axum, and Deadpool. Valve auto-scales on its own up to a maximum number of worker threads. So it’s not always taking up space and running more compute than it needs.\nValve overview:\n\nConcurrent webserver to auto-scale plumber APIs\nwritten in Rust using Tokio, Axum, and Deadpool\nspawns and kills plumber APIs based on demand\nintegration with {vetiver} of of the box\n\nFirst things first, I want to address “it’s not on CRAN.” You’re right. That’s because it is a Rust crate. Crates don’t go on CRAN. I’ve made an R package around it to lower the bar to entry. But it is a CLI tool at the core.\nObviously, it is new. It is untested. I wish I could tell everyone to use it, but I can’t. I think anyone who used it would be floored by its performance and ease of use. It is SO simple.\nI’ll push it to crates.io and CRAN in the coming weeks. Nothing like h8rs to inspire."
  },
  {
    "objectID": "posts/2019-08-04-my-parts.html",
    "href": "posts/2019-08-04-my-parts.html",
    "title": "∑ { my parts }",
    "section": "",
    "text": "library(tidyverse)\n\nterrorists &lt;- readr::read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSqNhpFX_69klKgJCVobc3fjHYVE9pNosrDi9h6irLlCtSSLpR704iu9VqI7CxdRi0iKt3p1FDYbu8Y/pub?gid=956062857&single=true&output=csv\")\n\n\n\n\n\nterrorist_by_race\n#&gt; # A tibble: 7 × 6\n#&gt;   race                n fatalities injured total_victims   `%`\n#&gt;   &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 white              63        554    1067          1621 69.3 \n#&gt; 2 other               5         90     115           205  8.77\n#&gt; 3 black              19        108      89           197  8.43\n#&gt; 4 asian               8         77      33           110  4.70\n#&gt; 5 unclear             6         40      61           101  4.32\n#&gt; 6 latino             10         44      33            77  3.29\n#&gt; 7 native american     3         19       8            27  1.15\n\n\n\n\n\nterrorist_by_gender\n#&gt; # A tibble: 3 × 6\n#&gt;   gender            n fatalities injured total_victims    `%`\n#&gt;   &lt;chr&gt;         &lt;int&gt;      &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 male            110        903    1380          2283 97.6  \n#&gt; 2 male & female     1         14      21            35  1.50 \n#&gt; 3 female            3         15       5            20  0.855\n\n\nterrorist &lt;- c(\"angry\", \"white\", \"male\")\nmy_parts &lt;- c(\"angry\", \"white\", \"male\")\n\n\nmy_parts == terrorist\n#&gt; [1] TRUE TRUE TRUE\n\n\n`I am` &gt; sum(my_parts)\n\n\n#&gt; [1] TRUE\n\n\n`I am` == sum(terrorist)\n\n\nFALSE\n#&gt; [1] FALSE\n\n\nwhite_males &lt;- filter(terrorists,\n                      race == \"white\",\n                      tolower(gender) == \"male\",\n                      !is.na(name))\n\npull(white_males, name)\n#&gt;  [1] \"Jordan Witmer\"             \"Zephen A. Xaver\"           \"Robert D. Bowers\"         \n#&gt;  [4] \"Jarrod W. Ramos\"           \"Dimitrios Pagourtzis\"      \"Travis Reinking\"          \n#&gt;  [7] \"Nikolas J. Cruz\"           \"Timothy O'Brien Smith\"     \"Kevin Janson Neal\"        \n#&gt; [10] \"Devin Patrick Kelley\"      \"Scott Allen Ostrem\"        \"Stephen Craig Paddock\"    \n#&gt; [13] \"Randy Stair\"               \"Thomas Hartless\"           \"Jason B. Dalton\"          \n#&gt; [16] \"Robert Lewis Dear\"         \"Noah Harpham\"              \"Dylann Storm Roof\"        \n#&gt; [19] \"Elliot Rodger\"             \"John Zawahri\"              \"Kurt Myers\"               \n#&gt; [22] \"Adam Lanza\"                \"Andrew Engeldinger\"        \"Wade Michael Page\"        \n#&gt; [25] \"James Holmes\"              \"Ian Stawicki\"              \"Scott Evans Dekraai\"      \n#&gt; [28] \"Jared Loughner\"            \"Robert Stewart\"            \"Wesley Neal Higdon\"       \n#&gt; [31] \"Steven Kazmierczak\"        \"Robert A. Hawkins\"         \"Tyler Peterson\"           \n#&gt; [34] \"Sulejman Talović\\u0087\"    \"Charles Carl Roberts\"      \"Kyle Aaron Huff\"          \n#&gt; [37] \"Terry Michael Ratzmann\"    \"Nathan Gale\"               \"Douglas Williams\"         \n#&gt; [40] \"Michael McDermott\"         \"Larry Gene Ashbrook\"       \"Day trader Mark O. Barton\"\n#&gt; [43] \"Eric Harris\"               \"Kipland P. Kinkel\"         \"Mitchell Scott Johnson\"   \n#&gt; [46] \"Matthew Beck\"              \"Dean Allen Mellberg\"       \"Kenneth Junior French\"    \n#&gt; [49] \"Gian Luigi Ferri\"          \"John T. Miller\"            \"Eric Houston\"             \n#&gt; [52] \"Thomas McIlvane\"           \"George Hennard\"            \"Joseph T. Wesbecker\"      \n#&gt; [55] \"Patrick Purdy\"             \"Richard Farley\"            \"William Cruse\"            \n#&gt; [58] \"Patrick Sherrill\"          \"James Oliver Huberty\"      \"Abdelkrim Belachheb\"      \n#&gt; [61] \"Carl Robert Brown\"\n\n\nam_i &lt;- function(terrorist) {\n  msg &lt;- paste(\"am i ==\", terrorist)\n  print(msg)\n  print(`am i` == terrorist)\n}\n\n\npull(white_males, name) %&gt;% \n  walk(~am_i(.))\n#&gt; [1] \"`am i` == Jordan Witmer\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Zephen A. Xaver\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert D. Bowers\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Jarrod W. Ramos\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Dimitrios Pagourtzis\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Travis Reinking\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Nikolas J. Cruz\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Timothy O'Brien Smith\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kevin Janson Neal\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Devin Patrick Kelley\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Scott Allen Ostrem\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Stephen Craig Paddock\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Randy Stair\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Thomas Hartless\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Jason B. Dalton\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert Lewis Dear\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Noah Harpham\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Dylann Storm Roof\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Elliot Rodger\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == John Zawahri\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kurt Myers\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Adam Lanza\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Andrew Engeldinger\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Wade Michael Page\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == James Holmes\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Ian Stawicki\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Scott Evans Dekraai\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Jared Loughner\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert Stewart\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Wesley Neal Higdon\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Steven Kazmierczak\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert A. Hawkins\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Tyler Peterson\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Sulejman Talović\\u0087\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Charles Carl Roberts\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kyle Aaron Huff\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Terry Michael Ratzmann\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Nathan Gale\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Douglas Williams\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Michael McDermott\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Larry Gene Ashbrook\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Day trader Mark O. Barton\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Eric Harris\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kipland P. Kinkel\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Mitchell Scott Johnson\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Matthew Beck\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Dean Allen Mellberg\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kenneth Junior French\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Gian Luigi Ferri\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == John T. Miller\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Eric Houston\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Thomas McIlvane\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == George Hennard\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Joseph T. Wesbecker\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Patrick Purdy\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Richard Farley\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == William Cruse\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Patrick Sherrill\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == James Oliver Huberty\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Abdelkrim Belachheb\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Carl Robert Brown\"\n#&gt; [1] FALSE\n\n\n`I am` &gt; sum(my_parts)"
  },
  {
    "objectID": "posts/2021-03-28-python-r.html",
    "href": "posts/2021-03-28-python-r.html",
    "title": "Python & R in production — the API way",
    "section": "",
    "text": "In my previous post I discussed how we can alter the R & Python story to be predicated on APIs as a way to bridge the language divide. The R & Python love story feels almost like unrequited love (h/t). Much of the development towards integrating the two languages has been heavily focused on the R user experience. While the developments with respect to reticulate have been enormous and cannot go understated, it might be worthwhile exploring another way in which R & Python and, for that matter, Python & R can be utilized together.\nBy shifting from language based tools that call the other language and translate their objects like reticulate and rpy2, to APIs we can develop robust language agnostic data science pipelines. I want to provide two motivating examples that explore the interplay between R & Python."
  },
  {
    "objectID": "posts/2021-03-28-python-r.html#calling-python-from-r-without-reticulate",
    "href": "posts/2021-03-28-python-r.html#calling-python-from-r-without-reticulate",
    "title": "Python & R in production — the API way",
    "section": "Calling Python from R (without reticulate)",
    "text": "Calling Python from R (without reticulate)\nWhen we talk about R & Python we typically are referring to reticulate, whether that be through python code chunks, the {tensorflow} package, or reticulate itself. However, as discussed in my previous post, another way that we can do this is via API. Flask can be used to create RESTful APIs.\nOn the RStudio Connect demo server there is a Flask app which provides historical stock prices for a few tickers. We can create a simple windowed summary visualization utilizing the Flask app, httr, dplyr, and ggplot2. Let’s break this down. First we use the httr library to send an HTTP request to the Flask app.\nlibrary(httr)\nlibrary(tidyverse)\n\nflask_call &lt;- \"https://colorado.rstudio.com/rsc/flask-stock-service/stocks/AAPL/history\"\n\naapl &lt;- GET(flask_call) %&gt;% \n  content(as = \"text\", encoding = \"utf-8\") %&gt;% \n  jsonlite::fromJSON() %&gt;% \n  as_tibble()\nBy sending this HTTP request, we are then kicking off a Python process which returns our dataset. We can then use dplyr to aggregate our dataset as we normally would.\naapl_yearly &lt;- aapl %&gt;% \n  group_by(year = lubridate::year(date)) %&gt;% \n  summarise(avg_adj = mean(adjusted)) \n\nhead(aapl_yearly)\n#&gt; # A tibble: 6 x 2\n#&gt;    year avg_adj\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1  2010    24.9\n#&gt; 2  2011    34.8\n#&gt; 3  2012    56.1\n#&gt; 4  2013    53.1\n#&gt; 5  2014    84.3\n#&gt; 6  2015   113.\nFinally we utilize ggplot2 to create the simple visualization.\nggplot(aapl_yearly, aes(year, avg_adj)) +\n  geom_line() + \n  labs(title = \"AAPL stock growth\", x = \"\", y = \"Average Adjusted\") +\n  scale_y_continuous(labels = scales::dollar) + \n  theme_minimal()\n\n\n\nThat was simple, right? In the above code chunks we utilized both R and python while only interacting and writing R code. That’s the brilliance of this approach."
  },
  {
    "objectID": "posts/2021-03-28-python-r.html#calling-r-from-python",
    "href": "posts/2021-03-28-python-r.html#calling-r-from-python",
    "title": "Python & R in production — the API way",
    "section": "Calling R from Python",
    "text": "Calling R from Python\nThe less often discussed part of this love story—hence unrequited love story—is how can Python users utilize R within their own workflows. Often machine learning engineers will use Python in combination with Scikit Learn to create their models. To illustrate how we can let both R and Python users shine I wanted to adapt the wonderful Bike Prediction example from the Solutions Engineering team at RStudio.\n\n\n\nThe Bike Prediction project is an example of orchestrating a number of data science artifacts into a holistic system on RStudio Connect that all work in unity. This example could just as well have been written entirely with Python. It could even be written as a combination of both R and Python. And that is what I’d like to illustrate.\nThe bike prediction app utilizes a custom R package and the power of dbplyr to perform scheduled ETL jobs. It is effective, efficient, and already deployed. Say one has a colleague who would like to create a new machine learning model using the same data how can we enable them to do so? The example works within the context of its own R Markdown that retrains the model. Rather than making a one time export of the data from the ETL process, we can make the data available consistently through a RESTful API hosted here.\n\n\n\nThe training and testing data have been made available through a plumber API that is hosted on RStudio Connect. With the data being available through an API, all that is needed to interact with it is the requests library. Everything else is as one would anticipate!\nimport requests\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nIn the below code chunk we call the Plumber API using an HTTP request which kicks off an R process. That R process utilizes dbplyr and lubridate to extract and partition data for training and testing.\n# Fetch data from Plumber API \ntest_dat_raw = requests.get(\"https://colorado.rstudio.com/rsc/bike-data-api/testing-data\")\ntest_dat = pd.read_json(test_dat_raw.text)\n\ntrain_dat_raw = requests.get(\"https://colorado.rstudio.com/rsc/bike-data-api/training-data\")\ntrain_dat = pd.read_json(train_dat_raw.text)\nNow that the data have been processed by R and loaded as a pandas dataframe the model training can continue as standard.\n# partition data and one hot encode day of week\ntrain_x = pd.concat([train_dat[[\"hour\", \"month\", \"lat\", \"lon\"]], pd.get_dummies(train_dat.dow)], axis = 1)\ntrain_y = train_dat[\"n_bikes\"]\n\n# instantiate xgb model object\nmodel = XGBRegressor()\n \n# fit the model with the training data\nmodel.fit(train_x,train_y)\n\n# predict the target on the test dataset\ntest_dow = pd.get_dummies(pd.Categorical(test_dat.dow, categories = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday']))\ntest_x = pd.concat([test_dat[[\"hour\", \"month\", \"lat\", \"lon\"]], test_dow], axis = 1)\ntest_y = test_dat.n_bikes\n# predict the target on the test dataset\npredict_test = model.predict(test_x)\n \n# MSE on test dataset\nmean_squared_error(test_y,predict_test, squared = False)\n#&gt; 4.502217132673415\nThrough the API both R and Python were able to flourish all the while building extensible infrastructure that can be utilized beyond their own team. The API approach enables the R and Python user to extend their tools beyond their direct team without having to adopt a new toolkit."
  },
  {
    "objectID": "posts/2021-03-28-python-r.html#adopting-apis-for-cross-language-collaboration",
    "href": "posts/2021-03-28-python-r.html#adopting-apis-for-cross-language-collaboration",
    "title": "Python & R in production — the API way",
    "section": "Adopting APIs for cross language collaboration",
    "text": "Adopting APIs for cross language collaboration\nWhile data scientists may usually think of APIs as something that they use to interact with SaaS products or extract data, they are also a tool that can be utilized to build out the data science infrastructure of a team. Through Flask, Plumber, and other libraries that turn code into RESTful APIs, data scientists can bridge language divides with exceptional ease. I think we ought to begin to transition the ways in which we think about language divides. We ought to utilize the universal language of HTTP more thoroughly. By creating these APIs we not only can aid other data scientists, but entirely other teams. A React JS web development can then tap into your API to either serve up predictions, extract data, send files, or whatever else you can dream up. Let’s not limit ourselves to one language. Let’s build out APIs to enable all languages to thrive.\nDisclaimer: This is a personal opinion and not endorsed or published by RStudio. My statements represent no one but myself—sometimes not even that."
  },
  {
    "objectID": "posts/2018-11-28-function-methods.html",
    "href": "posts/2018-11-28-function-methods.html",
    "title": "[Not so] generic functions",
    "section": "",
    "text": "Lately I have been doing more of my spatial analysis work in R with the help of the sf package. One shapefile I was working with had some horrendously named columns, and naturally, I tried to clean them using the clean_names() function from the janitor package. But lo, an egregious error occurred. To this end, I officially filed my complaint as an issue. The solution presented was to simply create a method for sf objects.\nYeah, methods, how tough can those be? Apparently the process isn’t at all difficult. But figuring out the process? That was difficult. This post will explain how I went about the process for converting the clean_names() function into a generic (I’ll explain this in a second), and creating a method for sf and tbl_graph objects.\n\nThe Jargon\nOkay, I want to address the jargon. What the hell is a generic function, and what is a method? But first, I want to give a quick tl;dr on what a function is. I define as function as bit of code that takes an input, changes it in some way, and produces an output. Even simpler, a function takes an input and creates an output.\n\nGeneric Functions\nNow, what is a generic function? My favorite definition that I’ve seen so far comes from LispWorks Ltd (their website is a historic landmark, I recommend you give it a look for a reminder of what the internet used to be). They define a generic function as\n\na function whose behavior depends on the classes or identities of the arguments supplied to it.\n\nThis means that we have to create a function that looks at the class of an object and perform an operation based on the object class. That means if there is \"numeric\" or \"list\" object, they will be treated differently. These are called methods. Note: you can find the class of an object by using the class() function on any object.\n\n\nMethods\nTo steal from LispWorks Ltd again, a method is\n\npart of a generic function which provides information about how that generic function should behave [for] certain classes.\n\nThis means that a method is part of a generic function and has to be defined separately. Imagine we have a generic function called f with methods for list and numeric objects. The way that we would denote these methods is by putting a period after the function name and indicating the type of object the function is to be used on. These would look like f.list and f.numeric respectively.\nBut to save time you can always create a default method which will be dispatched (used) on any object that it hasn’t been explicitly told how to operate on (by a specific method).\nNow that the intuition of what generic functions and methods R, we can begin the work of actually creating them. This tutorial will walk through the steps I took in changing the clean_names() from a standard function into a generic function with methods for sf objects and tbl_graph objects from the sf and tidygraph packages respectively.\nA brief overview of the process:\n\nDefine the generic function\nCreate a default method\nCreate additional methods\n\nA quick note: The code that follows is not identical to that of the package. I will be changing it up to make it simpler to read and understand what is happening.\n\n\n\nThe Generic Method\nThe first step, as described above, is to create a generic function. Generic functions are made by creating a new function with the body containing only a call to the UseMethod() function. The only argument to this is the name of your generic function—this should be the same as the name of the function you are making. This tells R that you are creating a generic function. Additionally, you should add any arguments that will be necessary for your function. Here, there are two arguments: dat and case. These indicate the data to be cleaned and the preferred style for them to be cleaned according to.\nI am not setting any default values for dat to make it required, whereas I am setting case to \"snake\".\n\nclean_names &lt;- function(dat, case = \"snake\") {\n  UseMethod(\"clean_names\")\n}\n\nNow we have created a generic function. But this function doesn’t know how to run on any given object types. In other words, there are no methods associated with it. To illustrate this try using the clean_names() function we just defined on objects of different types.\nclean_names(1) # numeric \nclean_names(\"test\") # character \nclean_names(TRUE) # logical \n\n#&gt; [1] \"no applicable method for 'clean_names' applied to an object of class \\\"c('double', 'numeric')\\\"\"\n\n\n#&gt; [1] \"no applicable method for 'clean_names' applied to an object of class \\\"character\\\"\"\n\n\n#&gt; [1] \"no applicable method for 'clean_names' applied to an object of class \\\"logical\\\"\"\n\nThe output of these calls say no applicable method for 'x' applied to an object of [class]. In order to prevent this from happening, we can create a default method. A default method will always be used if the function doesn’t have a method for the provided object type.\n\n\nThe Default Method\nRemember that methods are indicated by writing function.method. It is also important to note that the method should indicate an object class. To figure out what class an object is you can use the class() function. For example class(1) tells you that the number 1 is “numeric”.\nIn this next step I want to create a default method that will be used on every object that there isn’t a method explicitly for. To do this I will create a function called clean_names.default.\nAs background, the clean_names() function takes a data frame and changes column headers to fit a given style. clean_names() in the development version is based on the function make_clean_names() which takes a character vector and makes each value match a given style (the default is snake, and you should only use snake case because everything else is wrong * sarcasm * ).\n\nlibrary(janitor)\n\nNow let’s see how this function works. For this we will use the ugliest character vector I have ever seen from the tests for clean_names() (h/t @sfirke for making this).\n\nugly_names &lt;- c(\n  \"sp ace\", \"repeated\", \"a**^@\", \"%\", \"*\", \"!\",\n  \"d(!)9\", \"REPEATED\", \"can\\\"'t\", \"hi_`there`\", \"  leading spaces\",\n  \"€\", \"ação\", \"Farœ\", \"a b c d e f\", \"testCamelCase\", \"!leadingpunct\",\n  \"average # of days\", \"jan2009sales\", \"jan 2009 sales\"\n)\n\nugly_names\n#&gt;  [1] \"sp ace\"            \"repeated\"          \"a**^@\"             \"%\"                \n#&gt;  [5] \"*\"                 \"!\"                 \"d(!)9\"             \"REPEATED\"         \n#&gt;  [9] \"can\\\"'t\"           \"hi_`there`\"        \"  leading spaces\"  \"€\"                \n#&gt; [13] \"ação\"              \"Farœ\"              \"a b c d e f\"       \"testCamelCase\"    \n#&gt; [17] \"!leadingpunct\"     \"average # of days\" \"jan2009sales\"      \"jan 2009 sales\"\n\nNow to see how this function works:\n\nmake_clean_names(ugly_names)\n#&gt;  [1] \"sp_ace\"                 \"repeated\"               \"a\"                     \n#&gt;  [4] \"percent\"                \"x\"                      \"x_2\"                   \n#&gt;  [7] \"d_9\"                    \"repeated_2\"             \"cant\"                  \n#&gt; [10] \"hi_there\"               \"leading_spaces\"         \"x_3\"                   \n#&gt; [13] \"acao\"                   \"faroe\"                  \"a_b_c_d_e_f\"           \n#&gt; [16] \"test_camel_case\"        \"leadingpunct\"           \"average_number_of_days\"\n#&gt; [19] \"jan2009sales\"           \"jan_2009_sales\"\n\nTrès magnifique!\nThe body of the default method will take column names from a dataframe, clean them, and reassign them. Before we can do this, a dataframe is needed!\n\n# create a data frame with 20 columns\ntest_df &lt;- as_tibble(matrix(sample(100, 20), ncol = 20))\n\n# makes the column names the `ugly_names` vector\nnames(test_df) &lt;- ugly_names\n\n# print the data frame.\ntest_df\n#&gt; # A tibble: 1 × 20\n#&gt;   `sp ace` repeated `a**^@`   `%`   `*`   `!` `d(!)9` REPEATED `can\"'t` hi_\\th…¹   lea…²\n#&gt;      &lt;int&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt; 1       78       18      99    28    26    38      43       73       51       33      23\n#&gt; # … with 9 more variables: `€` &lt;int&gt;, ação &lt;int&gt;, Farœ &lt;int&gt;, `a b c d e f` &lt;int&gt;,\n#&gt; #   testCamelCase &lt;int&gt;, `!leadingpunct` &lt;int&gt;, `average # of days` &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, `jan 2009 sales` &lt;int&gt;, and abbreviated variable names\n#&gt; #   ¹​`hi_\\`there\\``, ²​`  leading spaces`\n#&gt; # ℹ Use `colnames()` to see all variable names\n\nThe process for writing this function is:\n\ntake a dataframe\ntake the old column names and clean them\nreassign the column names as the new clean names\nreturn the object\n\n\nclean_names.default &lt;- function(dat, case = \"snake\") { \n  # retrieve the old names\n  old_names &lt;- names(dat)\n  # clean the old names\n  new_names &lt;- make_clean_names(old_names, case = case)\n  # assign the column names as the clean names vector\n  names(dat) &lt;- new_names\n  # return the data\n  return(dat)\n  }\n\nNow that the default method has been defined. Try running the function on our test dataframe!\n\nclean_names(test_df)\n#&gt; # A tibble: 1 × 20\n#&gt;   sp_ace repeated     a percent     x   x_2   d_9 repeated_2  cant hi_th…¹ leadi…²   x_3\n#&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     78       18    99      28    26    38    43         73    51      33      23    27\n#&gt; # … with 8 more variables: acao &lt;int&gt;, faroe &lt;int&gt;, a_b_c_d_e_f &lt;int&gt;,\n#&gt; #   test_camel_case &lt;int&gt;, leadingpunct &lt;int&gt;, average_number_of_days &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, jan_2009_sales &lt;int&gt;, and abbreviated variable names ¹​hi_there,\n#&gt; #   ²​leading_spaces\n#&gt; # ℹ Use `colnames()` to see all variable names\n\nOh, my gorsh. Look at that! We can try replicating this with a named vector to see how the default method dispatched on unknown objects!\n\n# create a vector with 20 elements\ntest_vect &lt;- c(1:20)\n\n# name each element with the ugly_names vector \nnames(test_vect) &lt;- ugly_names\n\n# try cleaning!\nclean_names(test_vect)\n#&gt;                 sp_ace               repeated                      a \n#&gt;                      1                      2                      3 \n#&gt;                percent                      x                    x_2 \n#&gt;                      4                      5                      6 \n#&gt;                    d_9             repeated_2                   cant \n#&gt;                      7                      8                      9 \n#&gt;               hi_there         leading_spaces                    x_3 \n#&gt;                     10                     11                     12 \n#&gt;                   acao                  faroe            a_b_c_d_e_f \n#&gt;                     13                     14                     15 \n#&gt;        test_camel_case           leadingpunct average_number_of_days \n#&gt;                     16                     17                     18 \n#&gt;           jan2009sales         jan_2009_sales \n#&gt;                     19                     20\n\nIt looks like this default function works super well with named objects! Now, we will broach the problem I started with, sf objects.\n\n\nsf method\nThis section will go over the process for creating the sf method. If you have not ever used the sf package, I suggest you give it a try! It makes dataframe objects with spatial data associated with it. This allows you to perform many of the functions from the tidyverse to spatial data.\nBefore getting into it, I want to create a test object to work with. I will take the test_df column, create longitude and latitude columns, and then convert it into an sf object. The details of sf objects is out of the scope of this post.\n\nlibrary(sf)\n\ntest_sf &lt;- test_df %&gt;%\n  # create xy columns\n  mutate(long = -80, \n         lat = 40) %&gt;% \n  # convert to sf object \n  st_as_sf(coords = c(\"long\", \"lat\"))\n\n# converting geometry column name to poor style\nnames(test_sf)[21] &lt;- \"Geometry\"\n\n# telling sf which column is now the geometry\nst_geometry(test_sf) &lt;- \"Geometry\"\n\ntest_sf\n#&gt; Simple feature collection with 1 feature and 20 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -80 ymin: 40 xmax: -80 ymax: 40\n#&gt; CRS:           NA\n#&gt; # A tibble: 1 × 21\n#&gt;   `sp ace` repeated `a**^@`   `%`   `*`   `!` `d(!)9` REPEATED `can\"'t` hi_\\th…¹   lea…²\n#&gt;      &lt;int&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt; 1       78       18      99    28    26    38      43       73       51       33      23\n#&gt; # … with 10 more variables: `€` &lt;int&gt;, ação &lt;int&gt;, Farœ &lt;int&gt;, `a b c d e f` &lt;int&gt;,\n#&gt; #   testCamelCase &lt;int&gt;, `!leadingpunct` &lt;int&gt;, `average # of days` &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, `jan 2009 sales` &lt;int&gt;, Geometry &lt;POINT&gt;, and abbreviated\n#&gt; #   variable names ¹​`hi_\\`there\\``, ²​`  leading spaces`\n#&gt; # ℹ Use `colnames()` to see all variable names\n\nThe sf object has been created. But now how does our default method of the clean_names() function work on this object? There is only one way to know, try it.\nclean_names(test_sf)\n\nError in st_geometry.sf(x) : attr(obj, \"sf_column\") does not point to a geometry column. Did you rename it, without setting st_geometry(obj) &lt;- \"newname\"?\nNotice how it fails. sf noticed that I changed the name of the geometry column without explicitly telling it I did so. Since the geometry column is almost always the last column of an sf object, we can use the make_clean_names() function on every column but the last one! To do this we will use the rename_at() function from dplyr. This function allows you rename columns based on their name or position, and a function that renames it (in this case, make_clean_names()).\nFor this example dataset, say I wanted to clean the first column. How would I do that? Note that the first column is called sp ace. How this works can be seen in a simple example. In the below function call we are using the rename_at() function (for more, go here), selecting the first column name, and renaming it using the make_clean_names() function.\n\nrename_at(test_df, .vars = vars(1), .funs = make_clean_names)\n#&gt; # A tibble: 1 × 20\n#&gt;   sp_ace repea…¹ `a**^@`   `%`   `*`   `!` `d(!)9` REPEA…² can\"'…³ hi_\\t…⁴   lea…⁵   `€`\n#&gt;    &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     78      18      99    28    26    38      43      73      51      33      23    27\n#&gt; # … with 8 more variables: ação &lt;int&gt;, Farœ &lt;int&gt;, `a b c d e f` &lt;int&gt;,\n#&gt; #   testCamelCase &lt;int&gt;, `!leadingpunct` &lt;int&gt;, `average # of days` &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, `jan 2009 sales` &lt;int&gt;, and abbreviated variable names\n#&gt; #   ¹​repeated, ²​REPEATED, ³​`can\"'t`, ⁴​`hi_\\`there\\``, ⁵​`  leading spaces`\n#&gt; # ℹ Use `colnames()` to see all variable names\n\nNotice how only the first column has been cleaned. It went from sp ace to sp_ace. The goal is to replicate this for all columns except the last one.\nTo write the sf method, the above line of code can be adapted to select columns 1 through the number of columns minus 1 (so geometry isn’t selected). In order to make this work, we need to identify the second to last column—this will be supplied as the ending value of our selected variables.\n\nclean_names.sf &lt;- function(dat, case = \"snake\") {\n  # identify last column that is not geometry\n  last_col_to_clean &lt;- ncol(dat) - 1\n  # create a new dat object\n  dat &lt;- rename_at(dat, \n                   # rename the first up until the second to last\n                   .vars = vars(1:last_col_to_clean), \n                   # clean using the make_clean_names\n                   .funs = make_clean_names)\n  return(dat)\n}\n\nVoilà! Our first non-default method has been created. This means that when an sf object is supplied to our generic function clean_names() it looks at the class of the object—class(sf_object)—notices it’s an sf object, then dispatches (uses) the clean_names.sf() method instead of the default.\n\nclean_names(test_sf)\n#&gt; Simple feature collection with 1 feature and 20 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -80 ymin: 40 xmax: -80 ymax: 40\n#&gt; CRS:           NA\n#&gt; # A tibble: 1 × 21\n#&gt;   sp_ace repeated     a percent     x   x_2   d_9 repeated_2  cant hi_th…¹ leadi…²   x_3\n#&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     78       18    99      28    26    38    43         73    51      33      23    27\n#&gt; # … with 9 more variables: acao &lt;int&gt;, faroe &lt;int&gt;, a_b_c_d_e_f &lt;int&gt;,\n#&gt; #   test_camel_case &lt;int&gt;, leadingpunct &lt;int&gt;, average_number_of_days &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, jan_2009_sales &lt;int&gt;, Geometry &lt;POINT&gt;, and abbreviated\n#&gt; #   variable names ¹​hi_there, ²​leading_spaces\n#&gt; # ℹ Use `colnames()` to see all variable names\n\nHere we see that it worked exactly as we hoped. Every column but the last has been altered. This allows sf to name it’s geometry columns whatever it would like without disrupting it.\nShortly after this addition was added to the package I became aware of another type of object that had problems using clean_names(). This is the tbl_graph object from the tidygraph package from Thomas Lin Pederson.\n\n\ntbl_graph method\nIn issue #252 @gvdr noted that calling clean_names() on a tbl_graph doesn’t execute. Thankfully @Tazinho noted that you could easily clean the column headers by using the rename_all() function from dplyr.\nHere the solution was even easier than above. As a reminder, in order to make the tbl_graph method, we need to specify the name of the generic followed by the object class.\n\nclean_names.tbl_graph &lt;- function(dat, case = \"snake\") { \n  # rename all columns\n  dat &lt;- rename_all(dat, make_clean_names)\n  return(dat)\n  }\n\nIn order to test the function, we will need a graph to test it on. This example draws on the example used in the issue.\n\nlibrary(tidygraph)\n# create test graph to test clean_names\ntest_graph &lt;- play_erdos_renyi(0, 0.5) %&gt;% \n  # attach test_df as columns \n  bind_nodes(test_df)\n\ntest_graph\n#&gt; # A tbl_graph: 1 nodes and 0 edges\n#&gt; #\n#&gt; # A rooted tree\n#&gt; #\n#&gt; # Node Data: 1 × 20 (active)\n#&gt;   sp ace… repeat… `a**^@`   `%`   `*`   `!` `d(!)9` REPEAT… can\"'t… hi_\\th…   lead…\n#&gt;     &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n#&gt; 1      78      18      99    28    26    38      43      73      51      33      23\n#&gt; # … with 9 more variables: `€` &lt;int&gt;, ação &lt;int&gt;, Farœ &lt;int&gt;, `a b c d e f` &lt;int&gt;,\n#&gt; #   testCamelCase &lt;int&gt;, `!leadingpunct` &lt;int&gt;, `average # of days` &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, `jan 2009 sales` &lt;int&gt;\n#&gt; #\n#&gt; # Edge Data: 0 × 2\n#&gt; # … with 2 variables: from &lt;int&gt;, to &lt;int&gt;\n\nHere we see that there is a graph with only 1 node and 0 edges (relations) with bad column headers (for more, visit the GitHub page). Now we can test this as well.\n\nclean_names(test_graph)\n#&gt; # A tbl_graph: 1 nodes and 0 edges\n#&gt; #\n#&gt; # A rooted tree\n#&gt; #\n#&gt; # Node Data: 1 × 20 (active)\n#&gt;   sp_ace repeat…     a percent     x   x_2   d_9 repeat…  cant hi_the… leadin…   x_3\n#&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     78      18    99      28    26    38    43      73    51      33      23    27\n#&gt; # … with 8 more variables: acao &lt;int&gt;, faroe &lt;int&gt;, a_b_c_d_e_f &lt;int&gt;,\n#&gt; #   test_camel_case &lt;int&gt;, leadingpunct &lt;int&gt;, average_number_of_days &lt;int&gt;,\n#&gt; #   jan2009sales &lt;int&gt;, jan_2009_sales &lt;int&gt;\n#&gt; #\n#&gt; # Edge Data: 0 × 2\n#&gt; # … with 2 variables: from &lt;int&gt;, to &lt;int&gt;\n\nIt worked as anticipated!\n\n\nReview (tl;dr)\nIn the preceding sections we learned what generic functions and methods are. How to create a generic function, a default method, and methods for objects of different classes.\n\ngeneric function: “A generic function is a function whose behavior depends on the classes or identities of the arguments supplied to it”\ngeneric function method: “part of a generic function and which provides information about how that generic function should behave [for] certain classes”\n\nThe process to create a function with a method is to:\n\nCreate a generic function with:\n\nf_x &lt;- function() { UseMethod(\"f_x\") }\n\nDefine the default method with:\n\nf_x.default &lt;- function() { do something }\n\nDefine object class specific methods with:\n\nf_x.class &lt;- function() { do something else}\n\n\n\nNotes\nIf you have not yet encountered the janitor package it will help you tremendously with various data cleaning processes. Clearly, clean_names() is my favorite function as it helps me enforce my preferred style (and the only). If you are not aware of “proper” R style, I suggest you read the style guide in Advanced R.\nWhile on the subject of Advanced R, I suggest you read the “Creating new methods and generics” section of it. I struggled comprehending it at first because I didn’t even know what a method was. However, if after reading this you feel like you want more, that’s the place to go.\nI’d like to thank @sfirke for being exceptionally helpful in guiding my contributions to the janitor package."
  },
  {
    "objectID": "posts/2023-03-01-learning-rust.html",
    "href": "posts/2023-03-01-learning-rust.html",
    "title": "learning rust",
    "section": "",
    "text": "I have been wanting to learn a low level language for quite some time. Due to the power and prominence of Rcpp I had thought I wanted to learn C++. Every fast package in R uses C++, right? But Rust kept popping up. Rust, is fast. Rust is safe. Linux is going to be rewritten in Rust. Rust is the most loved language for 7 years in a row. Rust is easily multithreaded. Rust. Rust. Rust. I then heard a bit about rextendr a way to incorporate Rust into R packages. With that Rust became a real candidate language to learn.\nThe @hrbrmstr provided some sweet words of encouragement over twitter, an example repo to reference, and provided ideas on how to start learning Rust.\nI spent some time with “The Book” to wrap my head around the basics. I then spent some time writing a chess FEN parser. I wrote a FEN parser. I wrote a whole program in Rust. That was crazy. The reason why I was able to continue learning Rust is because it is so easy to use (in comparison to C++). Once you wrap your head around the basics it rocks.\nRust is a compiled language. Working with a compiled language is a huge paradigm shift. R is an interpreted language. Interpreted languages are super cool because we can run one line of code, do some stuff, and then run another line. Compiled languages have to take everything in at once. So there is no running a line, printing an object, then running another line. But that’s actually okay because of the Rust compiler is the best teacher I’ve ever had.\nWhen you make a mistake in Rust, the compiler will tell you exactly where that mistake is coming from—literally the line and column position. It will also often tell you exactly what code you need to change and how to change it to make your code run. So rather than running line by line, you can compile line by line.\nLearning Rust has made me a better R programmer for many reasons. Here are a few:\n\nI am conscious of type conversions and consistency\nI am conscientious of memory consumption\nI am a glutton for speed now\nI have a better understanding / framework for thinking about inheritance\n\nProgramming in Rust has made me think of ways that R can be improved. Mostly in that scalar classes are missing from R (and from vctrs). We also lack the ability to use 64 bit integers which is a bit of a problem. I also think R packages should be designed to be extended. This would be done by exposing generic s3 functions that can be extended for your class. If the method exists for your class you inherit the functionality. I employed a prototype of this idea in the sdf package."
  },
  {
    "objectID": "posts/csr.html",
    "href": "posts/csr.html",
    "title": "Complete spatial randomness",
    "section": "",
    "text": "&lt; this is a cliche about Tobler’s fist law and things being related in space&gt;. Because of Tobler’s first law, spatial data tend to not follow any specific distribution. So, p-values are sort of…not all that accurate most of the time. P-values in spatial statistics often take a “non-parametric” approach instead of an “analytical” one.\nConsider the t-test. T-tests make the assumption that data are coming from a normal distribution. Then p-values are derived from the cumulative distribution function. The alternative hypothesis, then, is that the true difference in means is not 0.\nIn the spatial case, our alternative hypothesis is generally “the observed statistic different than what we would expect under complete spatial randomness?” But what really does that mean? To know, we have to simulate spatial randomness.\nThere are two approaches to simulating spatial randomness that I’ll go over. One is better than the other. First, I’m going to describe the less good one: bootstrap sampling.\nLoad the super duper cool packages. We create queen contiguity neighbors and row-standardized weights.\nlibrary(sf)\nlibrary(sfdep)\nlibrary(tidyverse)\n\ngrid &lt;- st_make_grid(cellsize = c(1, 1), n = 12, offset = c(0, 0)) |&gt; \n  as_tibble() |&gt; \n  st_as_sf() |&gt; \n  mutate(\n    id = row_number(),\n    nb = st_contiguity(geometry),\n    wt = st_weights(nb)\n    )\nLet’s generate some spatially autocorrelated data. This function is a little slow, but it works.\nnb &lt;- grid[[\"nb\"]]\nwt &lt;- grid[[\"wt\"]]\n\nx &lt;-  geostan::sim_sar(w = wt_as_matrix(nb, wt), rho = 0.78)"
  },
  {
    "objectID": "posts/csr.html#bootstrap-sampling",
    "href": "posts/csr.html#bootstrap-sampling",
    "title": "Complete spatial randomness",
    "section": "Bootstrap sampling",
    "text": "Bootstrap sampling\nUnder the bootstrap approach we are sampling from existing spatial configurations. In our case there are 144 existing neighborhoods. For our simulations, we will randomly sample from existing neighborhoods and then recalculate our statistic. It helps us by imposing randomness into our statistic. We can then repeat the process nsim times. There is a limitation, however. It is that there are only n - 1 possible neighborhood configurations per location.\nHere we visualize a random point and it’s neighbors.\n\n# for a given location create vector indicating position of\n# neighbors and self\ncolor_block &lt;- function(i, nb) {\n  res &lt;- ifelse(1:length(nb) %in% nb[[i]], \"neighbors\", NA)\n  res[i] &lt;- \"self\"\n  res\n}\n\nPlot a point and its neighbors\n\ngrid |&gt; \n  mutate(block = color_block(sample(1:n(), 1), nb)) |&gt; \n  ggplot(aes(fill = block)) +\n  geom_sf() +\n  labs(title = \"Point and it's neighbors\")\n\n\n\n\nFor bootstrap we grab a point and then the neighbors from another point. This function will randomize a nb list object.\n\ncolor_sample_block &lt;- function(i, nb) {\n  index &lt;- 1:length(nb)\n  not_i &lt;- index[-i]\n  \n  sample_block_focal &lt;- sample(not_i, 1)\n  \n  res &lt;- rep(NA, length(index))\n  \n  res[nb[[sample_block_focal]]] &lt;- \"neighbors\"\n  res[i] &lt;- \"self\"\n  res\n}\n\n# visualize it\ngrid |&gt; \n  mutate(block = color_sample_block(sample(1:n(), 1), nb)) |&gt; \n  ggplot(aes(fill = block)) +\n  geom_sf() +\n  labs(title = \"Point and random point's neighbors\")\n\n\n\n\nOften, we will want to create a reference distribution by creating a large number of simulations—typically 999. As the simulations increase in size, we are limited in the amount of samples we can draw. The number of neighborhoods becomes limiting!\nSay we want to look at income distribution in Boston and the only data we have is at the census tract level. I happen to know that Boston has 207 tracts. If we want to do 999 simulations, after the 206th simulation, we will likely have gone through all over the neighborhood configurations!\nHow can we do this sampling? For each observation, we can sample another location, grab their neighbors, and assign them as the observed location’s neighbors."
  },
  {
    "objectID": "posts/csr.html#bootstrap-simulations",
    "href": "posts/csr.html#bootstrap-simulations",
    "title": "Complete spatial randomness",
    "section": "Bootstrap simulations",
    "text": "Bootstrap simulations\nIn sfdep, we use spdep’s nb object. These are lists that store the row position of the neighbors as integer vectors at each element.\n\nIf you want to learn more about neighbors I gave a talk at NY Hackr MeetUp a few months ago that might help.\n\nHere I define a function that samples from the positions (index), then uses that sample to shuffle up the existing neighborhoods and return a shuffled nb object. Note that I add the nb class back to the list.\n\nbootstrap_nbs &lt;- function(nb) {\n  # create index\n  index &lt;- 1:length(nb)\n  # create a resampled index\n  resampled_index &lt;- sample(index, replace = TRUE)\n  # shuffle the neighbors and reassign class\n  structure(nb[resampled_index], class = c(\"nb\", \"list\"))\n}\n\nLet’s compare some observations\n\nnb[1:3]\n\n[[1]]\n[1]  2 13 14\n\n[[2]]\n[1]  1  3 13 14 15\n\n[[3]]\n[1]  2  4 14 15 16\n\nbootstrap_nbs(nb)[1:3]\n\n[[1]]\n[1]  90  91  92 102 104 114 115 116\n\n[[2]]\n[1] 29 30 31 41 43 53 54 55\n\n[[3]]\n[1] 15 16 17 27 29 39 40 41\n\n\nHere we can see the random pattern. Look’s like there is fair amount of clustering of like values.\n\ngrid |&gt; \n  mutate(x = classInt::classify_intervals(x, 7)) |&gt; \n  ggplot(aes(fill = x)) +\n  geom_sf(color = NA, lwd = 0) +\n  scale_fill_brewer(type = \"div\", palette = 5, direction = -1) +\n  theme_void() \n\n\n\n\nWith the weights and the neighbors we can calculate the global Moran. I’ll refer to this as the “observed.” Store it into an object called obs. We’ll need this to calculate a simulated p-value later.\n\nobs &lt;- global_moran(x, nb, wt)\nobs[[\"I\"]]\n\n[1] 0.465776\n\n\n0.47 is a fair amount of positive spatial autocorrelation indicating that like values tend to cluster. But is this due to random chance, or does it depend on where these locations are? Now that we have the observed value of Moran’s I, we can simulate the value under spatial randomness using the bootstrapped sampling. To do so, we bootstrap sample our neighbors, recalculate the weights and then the global Moran. Now, if you’ve read my vignette on conditional permutation, you know what is coming next. We need to create a reference distribution of the global Moran under spatial randomness. To do that, we apply our boot strap nsim times and recalculate the global Moran with each new neighbor list. I love the function replicate() for these purposes.\n\nnsim = 499 \n\n\nAlso, a thing I’ve started doing is assigning scalars / constants with an equals sign because they typically end up becoming function arguments.\n\n\nreps &lt;- replicate(\n  nsim, {\n    nb_sim &lt;- bootstrap_nbs(nb)\n    wt_sim &lt;- st_weights(nb_sim)\n    global_moran(x, nb_sim, wt_sim)[[\"I\"]]\n  }\n)\n\nhist(reps, xlim = c(min(reps), obs[[\"I\"]]))\nabline(v = obs[[\"I\"]], lty = 2)\n\n\n\n\nBootstrap limitations\nThat’s all well and good, but let’s look at this a bit more. Since we’re using the bootstrap approach, we’re limited in the number of unique combinations that are possible. Let’s try something. Let’s calculate the spatial lag nsim times and find the number of unique values that we get.\n\nlags &lt;- replicate(\n  nsim, {\n    # resample the neighbors list\n    nb_sim &lt;- bootstrap_nbs(nb)\n    # recalculate the weights\n    wt_sim &lt;- st_weights(nb_sim)\n    # calculate the lag\n    st_lag(x, nb_sim, wt_sim)\n  }\n)\n\n# cast from matrix to vector\nlags_vec &lt;- as.numeric(lags)\n\n# how many are there?\nlength(lags_vec)\n\n[1] 71856\n\n# how many unique?\nlength(unique(lags_vec))\n\n[1] 144\n\n\nSee this? There are only 144 unique value! That isn’t much! Don’t believe me? Run table(lags_vec). For each location there are only a limited number of combinations that can occur."
  },
  {
    "objectID": "posts/csr.html#conditional-permutation",
    "href": "posts/csr.html#conditional-permutation",
    "title": "Complete spatial randomness",
    "section": "Conditional Permutation",
    "text": "Conditional Permutation\nNow, here is where I want to introduce what I view to be the superior alternative: conditional permutation. Conditional permutation was described by Luc Anselin in his seminal 1995 paper. The idea is that we hold an observation constant, then we randomly assign neighbors. This is like the bootstrap approach but instead of grabbing a random observation’s neighborhood we create a totally new one. We do this be assigning the neighbors randomly from all possible locations.\nLet’s look at how we can program this. For each location we need to sample from an index that excludes the observation’s position. Further we need to ensure that there are the same number of neighbors in each location (cardinality).\n\npermute_nb &lt;- function(nb) {\n  # first get the cardinality\n  cards &lt;- st_cardinalties(nb)\n  # instantiate empty list to fill\n  nb_perm &lt;- vector(mode = \"list\", length = length(nb))\n  \n  # instantiate an index\n  index &lt;- seq_along(nb)\n  \n  # iterate through and full nb_perm\n  for (i in index) {\n    # remove i from the index, then sample and assign\n    nb_perm[[i]] &lt;- sample(index[-i], cards[i])\n  }\n  \n  structure(nb_perm, class = \"nb\")\n}\n\n\nnb[1:3]\n\n[[1]]\n[1]  2 13 14\n\n[[2]]\n[1]  1  3 13 14 15\n\n[[3]]\n[1]  2  4 14 15 16\n\npermute_nb(nb)[1:3]\n\n[[1]]\n[1]  83  98 120\n\n[[2]]\n[1] 124  54 111  69  93\n\n[[3]]\n[1]  49 108  28   4  18\n\n\nNow, let’s repeat the same exercise using conditional permutation.\n\nlags2 &lt;- replicate(\n  nsim, {\n    nb_perm &lt;- permute_nb(nb)\n    st_lag(x, nb_perm, st_weights(nb_perm))\n  }\n)\n\nlags2_vec &lt;- as.numeric(lags2)\n  \nlength(unique(lags2_vec))\n\n[1] 71855\n\n\nThere are farrrrr more unique values. In fact, there is a unique value for each simulation - location pair. If we look at the histograms, the difference is even more stark. The conditional permutation approach actually begins to represent a real distribution.\n\npar(mfrow = c(1, 2))\nhist(lags_vec, breaks = 20) \nhist(lags2_vec, breaks = 20)\n\n\n\n\nSo, this is all for me to say that bootstrapping isn’t it for creating simulated distributions for which to calculate your p-values."
  },
  {
    "objectID": "posts/2023-01-19-raw-strings-in-r.html",
    "href": "posts/2023-01-19-raw-strings-in-r.html",
    "title": "Raw strings in R",
    "section": "",
    "text": "The one thing about Python I actually really like is the ability to use raw strings. Raw strings are super helpful for me because at work I use a windows machine. And windows machines use a silly file path convention. The \\ back slack character is used as the file separator as opposed to the linux / unix / forward slash.\nUsing the backslash is so annoying because it’s also an escape character. In python I can write the following to hard code a file path.\nWhereas in R typically you would have to write:\nSince \\ is an escape character you have to escape it first using itself. So, its annoying. And file.path(\"nav\", \"to\", \"file\", \"path.ext\", fsep = \"\\\\\") is a wee bit cumbersome sometimes."
  },
  {
    "objectID": "posts/2023-01-19-raw-strings-in-r.html#aight.",
    "href": "posts/2023-01-19-raw-strings-in-r.html#aight.",
    "title": "Raw strings in R",
    "section": "Aight.",
    "text": "Aight.\nSo like, you can use raw strings today.\nHow can I get the R-devel news? I’m on the mailing list and get it once a week and it’s like “Re: memory leak in png() ` not this stuff. Tips?\nIt was announced in the news for version 4.0.0.\nThey write:\n\nThere is a new syntax for specifying raw character constants similar to the one used in C++: r”(…)” with … any character sequence not containing the sequence ‘⁠)“⁠’. This makes it easier to write strings that contain backslashes or both single and double quotes. For more details see ?Quotes.\n\nYou can write raw strings using the following formats:\n\nr\"( ... )\"\nr\"{ ... }\"\nr\"[ ... ]\"\nR\"( ... )\"\nR\"{ ... }\"\nR\"[ ... ]\"\n\nYou can even trickier by adding dashes between the quote and the delimter. The dashes need to be symmetrical though. So the following is also valid.\n\nr\"-{ ... }\"-\nr\"--{ ... }--\"\nr\"--{ * _ * }--\"\n\nIt kinda looks like a crab\nAlright so back to the example\n\n r\"{nav\\to\\file\\path.ext}\"\n\n[1] \"nav\\\\to\\\\file\\\\path.ext\"\n\n\nHot damn. Thats nice.\nI freaked out at first though because R prints two backslashes. But if you cat the result they go away. So do not worry.\n\n r\"{nav\\to\\file\\path.ext}\" |&gt; \n  cat()\n\nnav\\to\\file\\path.ext"
  },
  {
    "objectID": "posts/2022-11-23-youtube-videos.html",
    "href": "posts/2022-11-23-youtube-videos.html",
    "title": "YouTube Videos & what not",
    "section": "",
    "text": "Please vote or post a comment in this discuss on what would be helpful for you.\n\nI first made R programming videos when I had the opportunity to teach a remote and asynchronous course called Big Data for Cities. I used the videos as alternative learning material besides a text-book and other required readings.\nI just recently noticed that my video Making your R Markdown Pretty has 16 thousand views. I never thought it would reach that many people! So, if I’ve helped even 0.1% of those people it will have been worth it.\nI’ve started to record some more videos on spatial analaysis in R. I think the spatial anlaysis / statistics videos on youtube are lacking in diversity—with the noteable exception of @GeostatsGuyLectures but he’s more of a environmental scientist :) I will note, though, that the lectures of Luc Anselin are one of the only reason why I am where I am in my knowledge and abilities.\n\n\nThe thing about Anselin’s videos, though, is that they are not about the how of doing it. But they focus more on the theory and the math that sits behind the statistics themselves."
  },
  {
    "objectID": "posts/2022-11-23-youtube-videos.html#i-ask-you",
    "href": "posts/2022-11-23-youtube-videos.html#i-ask-you",
    "title": "YouTube Videos & what not",
    "section": "I ask you!",
    "text": "I ask you!\nWhat do you want to see? What is actually helpful? I’m sure me stumbling and mumbling for 18 minutes on spatial lags can’t be too helpful.\nPlease vote or leave a comment in this discussion."
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html",
    "href": "posts/2020-01-13-gs4-auth.html",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "",
    "text": "This repository contains an example of an R Markdown document that uses googlesheets4 to read from a private Google Sheet and is deployed to RStudio Connect.\nThe path of least resistance for Google auth is to sit back and respond to some interactive prompts, but this won’t work for something that is deployed to a headless machine. You have to do some advance planning to provide your deployed product with a token.\nThe gargle vignette Non-interactive auth is the definitive document for how to do this. The gargle package handles auth for several packages, such as bigrquery, googledrive, gmailr, and googlesheets4.\nThis repo provides a detailed example for the scenario where you are using an OAuth2 user token for a product deployed on RStudio Connect (see vignette section Project-level OAuth cache from which this was adapted). Note that service account tokens are the preferred strategy for a deployed product, but sometimes there are reasons to use a user token."
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html#authenticating",
    "href": "posts/2020-01-13-gs4-auth.html#authenticating",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "Authenticating",
    "text": "Authenticating\nLoad the googlesheets4 package.\n\nlibrary(googlesheets4)\n\nBy default, gargle uses a central token store, outside of the project, which isn’t going to work for us. Instead we specify a project level directory .secrets which will contain our Google token. We will set the gargle_oauth_cache option to refer to this .secrets directory. We can check where the token will be cached with gargle::gargle_oauth_cache().\n\n# designate project-specific cache\noptions(gargle_oauth_cache = \".secrets\")\n\n# check the value of the option, if you like\ngargle::gargle_oauth_cache()\n\nNext we will have to perform the interactive authentication just once. Doing this will generate the token and store it for us. You will be required to select an email account to authenticate with.\n\n# trigger auth on purpose --&gt; store a token in the specified cache\n# a broswer will be opened\ngooglesheets4::sheets_auth()\n\nNow that you have completed the authentication and returned to R, we can double check that the token was cached in .secrets.\n\n# see your token file in the cache, if you like\nlist.files(\".secrets/\")\n\nVoila! Let’s deauthorize in our session so we can try authenticating once more, but this time without interactivity.\n\n# deauth\nsheets_deauth()\n\nIn sheets_auth() we can specify where the token is cached and which email we used to authenticate.\n\n# sheets reauth with specified token and email address\nsheets_auth(\n  cache = \".secrets\",\n  email = \"josiah@email.com\"\n  )\n\nAlternatively, we can specify these in the options() and run the authentication without an arguments supplied. Let’s first deauth in our session to try authenticating again.\n\n# deauth again\nsheets_deauth()\n\n# set values in options\noptions(\n  gargle_oauth_cache = \".secrets\",\n  gargle_oauth_email = \"josiah@email.com\"\n)\n\n# run sheets auth\nsheets_auth()\n\nNow that we are sure that authorization works without an interactive browser session, we should migrate the options into an .Rprofile file. This way, when an R session is spun up the options will be set from session start. Meaning, if you use sheets_auth() within your R Markdown document it will knit without having to open the browser."
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html#deploying-to-connect",
    "href": "posts/2020-01-13-gs4-auth.html#deploying-to-connect",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "Deploying to Connect",
    "text": "Deploying to Connect\nIn order for the deployment to RStudio Connect to work, the .secrets directory and .Rprofile files need to be in the bundle. Be sure to do this from the Add Files button. If you cannot see the files because they are hidden from Finder you cran press cmnd + shift + .. Then publish!"
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html#other-google-platforms",
    "href": "posts/2020-01-13-gs4-auth.html#other-google-platforms",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "Other Google Platforms",
    "text": "Other Google Platforms\nThis same process can be replicated for other packages that use gargle authentication. By virtue of having gargle as the central auth package for Google APIs, the workflow outlined here, and the others in the non-interactive auth vignette, can can be utilized for other google API packages (i.e. googledrive).\n\n# authenticate with googledrive and create a token\ngoogledrive::drive_auth()\n\nThank you to Jenny Bryan for her help editing this!"
  },
  {
    "objectID": "posts/2020-06-12-red-queen.html",
    "href": "posts/2020-06-12-red-queen.html",
    "title": "The Red Queen Effect",
    "section": "",
    "text": "The Red Queen and maintenance of state and society\nIt’s Monday morning. You’re back at work after a few days off. Your inbox is a lot more full than you hoped with 70 emails. Time to get reading and sending. It’s been an hour and you’ve read and sent at least 20 emails but your inbox is still at 70. You’ve been working hard and yet it feels like you’ve gone nowhere. This idea of working really hard but feeling like you’ve gone nowhere is at the center of the Red Queen Effect.\nThere are a number of “Red Queen Effect”s in the scientific literature all of which are inspired from the Red Queen’s race from Lewis Carrols’s Through the Looking Glass.\n\n“Well, in our country,” said Alice, still panting a little, “you’d generally get to somewhere else—if you run very fast for a long time, as we’ve been doing.”\n“A slow sort of country!” said the Queen. “Now, here, you see, it takes all the running you can do, to keep in the same place. If you want to get somewhere else, you must run at least twice as fast as that!” \n\nAlice is running and running and getting nowhere. Much like how as you read emails you get new ones. Daron Acemoglu and James A. Robinson adapt this concept to the evolution of states and connect it to their idea of the Narrow Corridor. The path to a “successful” society is a race between the power of the people and the power of the state. States in which the pace of growth in both the peoples’ power and the power / ability of the state are similar often produce more liberal (in the sense of liberty) nations.\n\nThis graphic is meant to illustrate this race. Getting into the corridor is a game of chase between the power of the state and society. Keeping the balance is delicate act—one that no nation has perfected—which requires society to check the power of the state and the state to provide checks to society. They define the Red Queen as\n\n“The process of competition, struggle and cooperation between state and society”"
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html",
    "href": "posts/2019-03-30-plumber-genius-api.html",
    "title": "genius Plumber API",
    "section": "",
    "text": "get started here\nSince I created genius, I’ve wanted to make a version for python. But frankly, that’s a daunting task for me seeing as my python skills are intermediate at best. But recently I’ve been made aware of the package plumber. To put it plainly, plumber takes your R code and makes it accessible via an API.\nI thought this would be difficult. I was so wrong."
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#using-plumber",
    "href": "posts/2019-03-30-plumber-genius-api.html#using-plumber",
    "title": "genius Plumber API",
    "section": "Using plumber",
    "text": "Using plumber\nPlumber works by using roxygen like comments (#*). Using a single comment, you can define the request type and the end point. Following that you define a function. The arguments to the funciton become the query parameters.\nThe main genius functions only require two main arguments artist and album or song. Making these accessible by API is as simple as:\n#* @get /track\nfunction(artist, song) {\n  genius::genius_lyrics(artist, song)\n}\nWith this line of code I created an endpoint called track to retrieve song lyrics. The two parameters as defined by the anonymous function are artist and song. This means that song lyrics are accessible with a query looking like http://hostname/track?artist=artist_name&song=song_name.\nBut as it stands, this isn’t enough to host the API locally. Save your functions with plumber documentation into a file (I named mine plumber.R).\n\nCreating the API\nCreating the API is probably the easiest part. It takes quite literally, two lines of code. The function plumb() takes two arguments, the file which contains your plumber commented code, and the directory that houses it.\nplumb() creates a router which is “responsible for taking an incoming request, submitting it through the appropriate filters.”\nI created a plumber router which would be used to route income queries.\npr &lt;- plumb(\"plumber.R\")\nThe next step is to actually run the router. Again, this is quite simple by calling the run() method of the pr object. All I need to do is specify the port that the API will listen on, and optionally the host address.\npr$run(port = 80, host = \"0.0.0.0\")\nNow I can construct queries in my browser. An example query is http://localhost/track?artist=andrew%20bird&song=proxy%20war. Sending this request produces a very friendly json output.\n[\n{\"track_title\":\"Proxy War\",\"line\":1,\"lyric\":\"He don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":2,\"lyric\":\"She don't to have get over him\"},\n{\"track_title\":\"Proxy War\",\"line\":3,\"lyric\":\"With all their words preserved forevermore\"},\n{\"track_title\":\"Proxy War\",\"line\":4,\"lyric\":\"You don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":5,\"lyric\":\"She don't have to get over you\"},\n{\"track_title\":\"Proxy War\",\"line\":6,\"lyric\":\"It's true these two have never met before\"},\n{\"track_title\":\"Proxy War\",\"line\":7,\"lyric\":\"At least not in real life\"},\n{\"track_title\":\"Proxy War\",\"line\":8,\"lyric\":\"Where your words cut like a knife\"},\n{\"track_title\":\"Proxy War\",\"line\":9,\"lyric\":\"Conjuring blood, biblical floods\"},\n{\"track_title\":\"Proxy War\",\"line\":10,\"lyric\":\"Looks that stop time\"},\n{\"track_title\":\"Proxy War\",\"line\":11,\"lyric\":\"You don't have to remember\"},\n{\"track_title\":\"Proxy War\",\"line\":12,\"lyric\":\"We forget what memories are for\"},\n{\"track_title\":\"Proxy War\",\"line\":13,\"lyric\":\"Now we store them in the atmosphere\"},\n{\"track_title\":\"Proxy War\",\"line\":14,\"lyric\":\"If you don't want to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":15,\"lyric\":\"You don't have to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":16,\"lyric\":\"It's just what we're calling peer-to-peer\"},\n{\"track_title\":\"Proxy War\",\"line\":17,\"lyric\":\"You don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":18,\"lyric\":\"You don't have to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":19,\"lyric\":\"We store them in the atmosphere\"},\n{\"track_title\":\"Proxy War\",\"line\":20,\"lyric\":\"If you don't want to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":21,\"lyric\":\"She don't have to get over you\"},\n{\"track_title\":\"Proxy War\",\"line\":22,\"lyric\":\"It's true these two have never met before\"},\n{\"track_title\":\"Proxy War\",\"line\":23,\"lyric\":\"At least not in real life\"},\n{\"track_title\":\"Proxy War\",\"line\":24,\"lyric\":\"Where your words cut like a knife\"},\n{\"track_title\":\"Proxy War\",\"line\":25,\"lyric\":\"Conjuring blood, biblical floods\"},\n{\"track_title\":\"Proxy War\",\"line\":26,\"lyric\":\"Looks that stop time\"},\n{\"track_title\":\"Proxy War\",\"line\":27,\"lyric\":\"If you don't want to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":28,\"lyric\":\"You don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":29,\"lyric\":\"If you don't want to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":30,\"lyric\":\"You don't have to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":31,\"lyric\":\"If you want to remember\"},\n{\"track_title\":\"Proxy War\",\"line\":32,\"lyric\":\"If you don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":33,\"lyric\":\"If you don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":34,\"lyric\":\"If you don't want to get over\"}\n]"
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#writing-a-python-wrapper",
    "href": "posts/2019-03-30-plumber-genius-api.html#writing-a-python-wrapper",
    "title": "genius Plumber API",
    "section": "Writing a Python Wrapper",
    "text": "Writing a Python Wrapper\nOne of the appeals of writing an API is that it can be accessed from any language. This was the inspiriation of creating this API. I want to be able to call R using Python. Creating an API is a great intermediary as writing an API wrapper is much easier for me than recreating all of the code that I wrote in R.\nI want to be able to recreate the three main functions of genius. These are genius_lyrics(), genius_album(), and genius_tracklist(). In doing this there are two steps I have to consider. The first is creating query urls, and the second is parsing json.\nTo create the urls, the requests library is used. Next, I created a template for the urls.\nimport requests\nurl_template = \"http://localhost:80/track?artist={}&song={}\"\nThe idea here is that the {} characters will be filled with provided parameters by using the .format() method.\nFor example, if I wanted to get lyrics for Proxy War by Andrew Bird, I would supply \"Andrew Bird\" and \"Proxy War\" as the arguments to format(). It’s important to note that these arguments are taken positionally. The url is created using this method.\nurl = url_template.format(\"andrew bird\", \"proxy war\")\nNow I am at the point where I can ping the server to receive the json. This is accomplished by using the .get() method from requests.\nresponse = requests.get(url)\nThis returns an object that contains the json response. Next, in order to get this into a format that can be analysed, it needs to be parsed. I prefer a Pandas DataFrame, and fortunately Pandas has a lovely read_json function. I will call the .content attribute of the response objectm and feed that into the read_json() function.\nimport pandas as pd\n\nproxy_war = pd.read_json(response.content)\n\nproxy_war.head()\n\n    line    lyric                                   track_title\n0   1   He don't have to get over her               Proxy War\n1   2   She don't to have get over him              Proxy War\n2   3   With all their words preserved forevermore  Proxy War\n3   4   You don't have to get over her              Proxy War\n4   5   She don't have to get over you              Proxy War\nBeautiful. Song lyrics are available through this API and can easily be accessed via python. The next step is to generalize this and the other two functions. The below is the code to create the genius_lyrics() function in python. It works almost identically as in R. However, at this moment it does not have the ability to set the info argument. But this can be changed easily in the original plumber.R file.\n# Define genius_lyrics()\ndef genius_lyrics(artist, song):\n\n    url_template = \"http://localhost:80/track?artist={}&song={}\"\n    \n    url = url_template.format(artist, song)\n\n    response = requests.get(url)\n    \n    song = pd.read_json(response.content)\n    \n    return(song)\nAt this point I’m feeling extremely stoked on the fact that I can use genius with python. Who says R and python practitioners can’t work together?"
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#containerize-with-docker",
    "href": "posts/2019-03-30-plumber-genius-api.html#containerize-with-docker",
    "title": "genius Plumber API",
    "section": "Containerize with Docker",
    "text": "Containerize with Docker\n\nTo make the process of setting up this genius API up easier for those who don’t necessarily interact with R, I created a lightweight-ish Docker container. The idea for this was to be able to pull a Docker image, run a command, and then the API will be available on a local port without having to interact with R at all.\nI’m not the most experience person with creating Docker containers but I can borrow code quite well. Fortunately I came across some wonderful slides from rstudio::conf 2019. Heather Nollis and Jacqueline Nolis presented on “API development with R and TensorFlow at T-Mobile”.\nThis container needs two things: a linux environment and an installation of R with plumber, genius, and its dependencies. An organization called The Rocker Project has created a number of Docker images that are stable and easy to install.\nSince genius relies on many packages from the tidyverse, the rocker/tidyverse image was used. To use their wonderful image, only one line is needed in my Dockerfile.\n# Import existing Docker image\nFROM rocker/tidyverse:3.5.2\nNow, not knowing exactly what I was doing, I copied code from Jacqueline and Heather’s sample Dockerfile in their slides. Their comment says that this is necessary to have the “needed linux libraries for plumber”, I went with it.\n# install needed linux libraries for plumber\nRUN apt-get update -qq && apt-get install -y \\\n  libssl-dev \\\n  libcurl4-gnutls-dev\ngenius and plumber are not part of the tidyverse image and have to be installed manually. The following lines tell Docker to run the listed R commands. For some unknown reason there was an issue with installing genius from CRAN so the repos argument was stated explicitly.\n# Install R packages\nRUN R -e \"install.packages('genius', repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('plumber')\"\nIn addition to the Dockerfile there are two files in my directory which are used to launch the API. The plumber.R and launch_api.R files. These need to be copied into the container. The line COPY / / copies from the location / in my directory to the location / in the container.\nThe Docker image has the libraries and files needed, but it needs to be able to actually launch the API. Since the plumber.R file specifies that the API will be listening on port 80, I need to expose that port in my Docker image using EXPOSE 80.\nThe last part of this is to run the launch_api.R so the API is available. The ENTRYPOINT command tells Docker what to run when the container is launched. In this case ENTRYPOINT [\"Rscript\", \"launch_api.R\"] tells Docker to run the Rscript command with the argument launch_api.R. And with that, the Dockerfile is complete and read to run.\nThe image needs to be built and ran. The simplest way to do this for me was to work from Dockerhub. Thus to run this container only three lines of code are needed!\ndocker pull josiahparry/genius-api:working\n\ndocker build -t josiahparry/genius-api .\n\ndocker run --rm -p 80:80 josiahparry/genius-api\n\nBoom, now you have an API that will be able to use the functionality of genius. If you wish to use Python with the API, I wrote a simple script which creates a nice tidy wrapper around it.\n\nIf anyone is interested in writing a more stable Python library that can call the functionality described above I’d love your help to make genius more readily available to the python community."
  },
  {
    "objectID": "posts/2020-12-09-secure-package-environment.html",
    "href": "posts/2020-12-09-secure-package-environment.html",
    "title": "Secure R Package Environments",
    "section": "",
    "text": "One of the biggest challenges faced by public sector organizations and other security conscious groups is package management. These groups are typically characterized air gapped network environments—i.e. no internet connectivity to the outside world. The purpose of an air gapped network is to get rid of any possibility of an intrusion in your network from an unwanted visitor. Air gapped installations come with some challenges particularly with package management.\nTypically, when you want to install a new package it comes from The Comprehensive R Archive Network (CRAN). While CRAN has a comprehensive testing system as part of their software development life cycle, security teams are still hesitant to trust any domains outside of their network.\nAt RStudio, our solution to this is our RStudio Package Manager or RSPM for short. “RStudio Package Manager is a repository management server to organize and centralize R packages across your team, department, or entire organization.” With RSPM there are a few different ways of addressing this concern. Here I’ll walk through some different approaches. Each subsequent approach is stricter than the last.\n\n\nThe first approach to do this is to install and configure RSPM in your air gapped network. However, RSPM will need special permission to reach out to our sync service (https://rspm-sync.rstudio.com). In many cases security teams are willing to open up an outbound internet connection to just RStudio’s sync service—we hope you trust us!\nThis solution is the easiest as it is a quick configuration. Moreover, packages will only be installed as they are requested. Giving access to the sync service also enables your team to be able to download the latest versions of packages from CRAN.\n\n\n\nThe limitation of the first approach is that is permits a constant outbound internet connection. For some groups, this is a no go. The next best approach then is to have a completely air-gapped CRAN mirror. To do this you will need an internet connection for a brief amount of time—there’s no way to have data magically appear on your server! During the brief period in which your proxy is open you will have to copy all of CRAN, binaries and source, into your server. RSPM provides a utility tool to do this. Once complete, you can close your network again and be confident that there is no possibility of having any connection with the outside world.\nOnce you’ve completed moving data into your server everything behaves as expected—just ensure your options('repos') is set properly. The one downside to this approach is that you will not be able to have access to the latest versions of packages. To rectify this, you can sync on periodic basis.\n\n\n\nOften there are even further restrictions placed on data scientists which limit what packages can be used for their work. We refer to this as a validated set of packages or a curated CRAN. Packages are often “validated” and through that validation process are promoted to the CRAN repository. The upside to this approach is that teams can be confident in the packages their team are using.\nSome approaches to validating the package environment include selecting the top n packages from CRAN (post on identifying those packages here), having a subject matter expert provide a list of preferred packages, or a ticketing system. The ticketing system is the least scalable, most restrictive, and will likely hinder your work. I don’t recommend it.\nThe limitations with this are rather straight forward: your data scientists do not have too much leeway in utilizing packages that may expedite or even enable their work.\n\n\n\nWith approach 3 there are usually two repositories: 1) a mirror of CRAN and 2) a subset of CRAN. While the subset of CRAN is preferred there is nothing stopping users from using the CRAN repository if they know the URL. To prevent this you can implement strict rules with your proxy to prevent users installing from the CRAN mirror thus forcing users to use the subset. In essence, approach 4 is approach 3 but with an enforcement mechanism."
  },
  {
    "objectID": "posts/2020-12-09-secure-package-environment.html#securing-your-r-package-environment",
    "href": "posts/2020-12-09-secure-package-environment.html#securing-your-r-package-environment",
    "title": "Secure R Package Environments",
    "section": "",
    "text": "One of the biggest challenges faced by public sector organizations and other security conscious groups is package management. These groups are typically characterized air gapped network environments—i.e. no internet connectivity to the outside world. The purpose of an air gapped network is to get rid of any possibility of an intrusion in your network from an unwanted visitor. Air gapped installations come with some challenges particularly with package management.\nTypically, when you want to install a new package it comes from The Comprehensive R Archive Network (CRAN). While CRAN has a comprehensive testing system as part of their software development life cycle, security teams are still hesitant to trust any domains outside of their network.\nAt RStudio, our solution to this is our RStudio Package Manager or RSPM for short. “RStudio Package Manager is a repository management server to organize and centralize R packages across your team, department, or entire organization.” With RSPM there are a few different ways of addressing this concern. Here I’ll walk through some different approaches. Each subsequent approach is stricter than the last.\n\n\nThe first approach to do this is to install and configure RSPM in your air gapped network. However, RSPM will need special permission to reach out to our sync service (https://rspm-sync.rstudio.com). In many cases security teams are willing to open up an outbound internet connection to just RStudio’s sync service—we hope you trust us!\nThis solution is the easiest as it is a quick configuration. Moreover, packages will only be installed as they are requested. Giving access to the sync service also enables your team to be able to download the latest versions of packages from CRAN.\n\n\n\nThe limitation of the first approach is that is permits a constant outbound internet connection. For some groups, this is a no go. The next best approach then is to have a completely air-gapped CRAN mirror. To do this you will need an internet connection for a brief amount of time—there’s no way to have data magically appear on your server! During the brief period in which your proxy is open you will have to copy all of CRAN, binaries and source, into your server. RSPM provides a utility tool to do this. Once complete, you can close your network again and be confident that there is no possibility of having any connection with the outside world.\nOnce you’ve completed moving data into your server everything behaves as expected—just ensure your options('repos') is set properly. The one downside to this approach is that you will not be able to have access to the latest versions of packages. To rectify this, you can sync on periodic basis.\n\n\n\nOften there are even further restrictions placed on data scientists which limit what packages can be used for their work. We refer to this as a validated set of packages or a curated CRAN. Packages are often “validated” and through that validation process are promoted to the CRAN repository. The upside to this approach is that teams can be confident in the packages their team are using.\nSome approaches to validating the package environment include selecting the top n packages from CRAN (post on identifying those packages here), having a subject matter expert provide a list of preferred packages, or a ticketing system. The ticketing system is the least scalable, most restrictive, and will likely hinder your work. I don’t recommend it.\nThe limitations with this are rather straight forward: your data scientists do not have too much leeway in utilizing packages that may expedite or even enable their work.\n\n\n\nWith approach 3 there are usually two repositories: 1) a mirror of CRAN and 2) a subset of CRAN. While the subset of CRAN is preferred there is nothing stopping users from using the CRAN repository if they know the URL. To prevent this you can implement strict rules with your proxy to prevent users installing from the CRAN mirror thus forcing users to use the subset. In essence, approach 4 is approach 3 but with an enforcement mechanism."
  },
  {
    "objectID": "posts/2020-12-09-secure-package-environment.html#review",
    "href": "posts/2020-12-09-secure-package-environment.html#review",
    "title": "Secure R Package Environments",
    "section": "Review",
    "text": "Review\nPackage management isn’t easy. It’s even tougher in an offline environment. You’re not going to be able to know exactly what every package does. You’re going to have to make tradeoffs. You can secure your package environment by migrating packages into your own network. You can implement progressively stricter rules to reduce your exposure to potential packages. RStudio Package Manager is a wonderful tool that will make accomplishing all of this a whole lot easier.\nFeel free to reach out to me via twitter or email and we can talk this through."
  },
  {
    "objectID": "posts/2022-05-11-make-your-r-scripts-databricks-notebooks.html",
    "href": "posts/2022-05-11-make-your-r-scripts-databricks-notebooks.html",
    "title": "Make your R scripts Databricks notebooks",
    "section": "",
    "text": "I’ve never had a good reason to deviate from the canonical .R file extension until today.\nAs you may have seen over the past few month from my numerous rage-tweets and Databricks related threads, I’ve been doing a lot of work getting figuring out Databricks as an R user so we can get onboard with adoption here at NPD.\nOne of my biggest qualms about Databricks is that it’s tailored to their notebooks. The notebooks get magical superpowers that aren’t available anywhere else. Notebooks get root permissions, they have access to dbutils, and are the only thing that can actually be scheduled by Databricks outside of a jar file or SparkSQL code.\nI’ve spent quite a bit of time thinking about how we can schedule R scripts through a notebook. If you’re wondering, have the notebook kickoff the R script with a shell command.\nBut, alas, I’ve learned something today. If you connect your Git repo to Databricks through their \"Repos\", you can have your R scripts be accessible as notebooks with quite literally only two changes.\nFirst, R scripts need to have the less-preferable, though equally functional, file extension .r. Second, the first line of the script should be a comment that says # Databricks notebook source. And that’s it. Then once the git repo has been connected, it will recognize those as notebooks.\nIf you want to create cells in your code write a comment # COMMAND ----------—that’s 10 hyphens at the end.\nIf you create a file main.r which contains the body\n# Databricks notebook source\n\nprint(\"Hello world\")\n\n# COMMAND ---------\n\nprint(\"Databricks! I've figured you out—sorta....\")\n\n# COMMAND ---------\n\nprint(\"I feel powerful.\")\nYou will have an R script that is recognized as a notebook by Databricks that can be scheduled using Databricks’ scheduling wizard."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html",
    "href": "posts/2022-10-03-spacetime-representations.html",
    "title": "spacetime representations aren’t good—yet",
    "section": "",
    "text": "My beliefs can be summarized somewhat succinctly.\nWe should not limit space-time data to dates or timestamps.\nThe R ecosystem should always utilize a normalized approach as described above. Further, a representation should use friendly R objects. The friendliest object is a data frame. A new representation should allow context switching between geometries and temporal data. That new representation should always use time-long formats and the geometries should never be repeated.\nA spacetime representation should give users complete and total freedom to manipulate their data as they see fit (e.g. dplyr or data.table operations).\nThe only time to be strict in the format of spacetime data is when statstics are going to be derived from the data."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#background",
    "href": "posts/2022-10-03-spacetime-representations.html#background",
    "title": "spacetime representations aren’t good—yet",
    "section": "Background",
    "text": "Background\nWhile implementing emerging hotspot analysis in sfdep I encountered the need for a formalized spacetime class in R. As my focus in sfdep has been tidyverse-centric functionality, I desired a “tidy” data frame that could be used as a spacetime representation. Moreover, space (in the spacetime representation) should be represented as an sf or sfc object. In sfdep I introduced the new S3 class spacetime based on Edzer Pebesma’s 2012 article “spacetime: Spatio-Temporal Data in R” and Thomas Lin Pederson’s tidygraph package."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#representations-of-spatial-data",
    "href": "posts/2022-10-03-spacetime-representations.html#representations-of-spatial-data",
    "title": "spacetime representations aren’t good—yet",
    "section": "Representations of Spatial Data",
    "text": "Representations of Spatial Data\nBefore describing my preferences in a spacetime representation in R, I want to review possible representations of spacetime data.\nPebesma (2012) outlines three tabular representations of spatio-temporal data.\n\n“Time-wide: Where different columns reflect different moments in time.\n\nSpace-wide: Where different columns reflect different measurement locations or areas.\n\nLong formats: Where each record reflects a single time and space combination.\n\nThe “long format” is what we may consider “tidy” per Wickham (2014). In this case, both time and space are variables with unique combinations as rows.\nPebesma further qualifies spatial data representation into a “sparse grid” and a “full grid.” Say we have a variable X. In a spatio temporal full grid we will store all combinations of time (t) and locations (i) . If Xi is missing at any of those location and time combinations (Xit is missing), the value of X is recorded as a missing value. Whereas in a sparse grid, if there is any missing data, the observation is omitted. Necessarily, in a full grid there will be i x t number of rows. In a sparse grid there will be fewer than i x t rows.\nVery recently in an r-spatial blog post, “Vector Data Cubes”, Edzer describes another approach to representing spacetime using a database normalization approach. Database normalization is a process that reduces redundancy by creating a number of smaller tables containing IDs and values. These tables can then be joined only when needed. When we consider spacetime data, we have repeating geometries across time. It is inefficient to to keep multiple copies of the geometry. Instead, we can keep track of the unique ID of a geometry and store the geometry in another table."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#sfdep-spacetime-representation",
    "href": "posts/2022-10-03-spacetime-representations.html#sfdep-spacetime-representation",
    "title": "spacetime representations aren’t good—yet",
    "section": "sfdep spacetime representation",
    "text": "sfdep spacetime representation\nThe spacetime class in sfdep is in essence a database normalization approach (see above blog post). It is implemented with the database normalization approach and the ergonomics of tidygraph in mind.\nThe objective of the spacetime class in sfdep is to\n\nallow complete freedom of data manipulation via data.frame objects,\nprevent duplication of geometries,\nand provide leeway in what “time” can be defined as.\n\nSimilar to tidygraph, spacetime provides access to two contexts: data and geometry. The data context is a data frame and the geometry context. These are linked based on a unqie identifie that is present in both contexts.\nR code\n\nlibrary(dplyr)\n\ntimes &lt;- seq(\n  Sys.time(), \n  Sys.time() + lubridate::hours(5),\n  length.out = 5\n)\n\nlocations &lt;- c(\"001\", \"002\")\n\ndata_context &lt;- tidyr::crossing(\n  location = locations,\n  time = times\n) |&gt; \n  mutate(value = rnorm(n())) |&gt; \n  arrange(location)\n\n\nlibrary(sf)\n\nLinking to GEOS 3.9.1, GDAL 3.2.3, PROJ 7.2.1; sf_use_s2() is TRUE\n\ngeometry_context &lt;- st_sfc(\n  list(st_point(c(0, 1)), st_point(c(1, 1)))\n  ) |&gt; \n  st_as_sf() |&gt; \n  mutate(location = c(\"001\", \"002\"))\n\nUse the spacetime constructor\n\nlibrary(sfdep)\nspt &lt;- spacetime(\n  .data = data_context,\n  .geometry = geometry_context, \n  .loc_col = \"location\", \n  .time_col = \"time\"\n)\n\nSwap contexts with activate\nactivate(spt, \"geometry\")\nspacetime ────\nContext:`geometry`\n2 locations `location`\n5 time periods `time`\n── geometry context ────────────────────────────────────────────────────────────\nSimple feature collection with 2 features and 1 field Geometry type: POINT Dimension: XY Bounding box: xmin: 0 ymin: 1 xmax: 1 ymax: 1 CRS: NA x location 1 POINT (0 1) 001 2 POINT (1 1) 002\nOne of my very strong beliefs is that temporal data does not, and should not, always be represented as a date or a timestamp. This paradigm is too limiting. What about panel data where you’re measuring cohorts along periods 1 - 10? Should these be represented as dates? No, definitely not. Because of this, sfdep allows you to utilize any numeric column that can be sorted.\n\nPerhaps I’ve just spent too much time listening to ecometricians…\n\nexample of using integers\n\nspacetime(\n  mutate(data_context, period = row_number()),\n  geometry_context, \n  .loc_col = \"location\",\n  .time_col = \"period\"\n)\n\nspacetime ────\n\n\nContext:`data`\n\n\n2 locations `location`\n\n\n10 time periods `period`\n\n\n── data context ────────────────────────────────────────────────────────────────\n\n\n# A tibble: 10 × 4\n   location time                 value period\n * &lt;chr&gt;    &lt;dttm&gt;               &lt;dbl&gt;  &lt;int&gt;\n 1 001      2022-11-15 08:23:08 -0.206      1\n 2 001      2022-11-15 09:38:08 -0.289      2\n 3 001      2022-11-15 10:53:08 -0.275      3\n 4 001      2022-11-15 12:08:08  0.136      4\n 5 001      2022-11-15 13:23:08 -1.48       5\n 6 002      2022-11-15 08:23:08  1.01       6\n 7 002      2022-11-15 09:38:08  2.11       7\n 8 002      2022-11-15 10:53:08 -1.68       8\n 9 002      2022-11-15 12:08:08  0.880      9\n10 002      2022-11-15 13:23:08  0.698     10"
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#qualifiers",
    "href": "posts/2022-10-03-spacetime-representations.html#qualifiers",
    "title": "spacetime representations aren’t good—yet",
    "section": "Qualifiers",
    "text": "Qualifiers\nI don’t think my spacetime class is the panacea. I don’t have the technical chops to make a great data format. I also don’t want to have that burden. Additionally, the class is desgned with lattice data in mind. I don’t think it is sufficient for trajectories or point pattern without repeating locations.\nThere’s a new R package called cubble for spatio-temporal data. I’ve not explored it. It may be better suited to your tidy-centric spatio-temporal data."
  },
  {
    "objectID": "posts/2022-02-12-the-heck-is-a-statistical-moment.html",
    "href": "posts/2022-02-12-the-heck-is-a-statistical-moment.html",
    "title": "The heck is a statistical moment??",
    "section": "",
    "text": "I wrote myself a short story to help me remember what the moments are.\n\n“The first moment I looked at the distribution I thought only of the average. The second, I thought of the variance. Soon after, I thought then of the skewness. Only then did I think about the kurtosis.”\n\nThis all started when reading Luc Anselin’s “Spatial Regression Analysis in R: A Workbook”, I encountered the following:\n\n“Under the normal assumption for the null, the theoretical moments of Moran’s I only depend on the characteristics of the weights matrix.”\n\nThe moments? The what? Under the normal assumption of my study habits I would skip over this word and continue to the next sentence. However, this was critical for understanding the formula for Moran’s I: \\(E[I] = \\frac{-1}{n - 1}\\).\nWikipedia was likely written by the same gatekeepers. I turned to the article on “Method of moments (statistics)” which writes\n\n“Those expressions are then set equal to the sample moments. The number of such equations is the same as the number of parameters to be estimated. Those equations are then solved for the parameters of interest. The solutions are estimates of those parameters.”\n\nNaturally, I turned to twitter to vent.\n\n\nThe use of the word \"moment\" in statistics is cruel.\n\n— jos (@JosiahParry) October 23, 2021\n\n\nThanks to Nicole Radziwill for a very helpful link from the US Naval Academy.\nThe method of moments is no more than simple summary statistics from a distribution. There are four “moments”.\n\nMean\nVariance\nSkew\nKurtosis\n\nWhy would one use these words? To gatekeep, of course. Academia uses needlessly complex language quite often.\nRemember, friends. Use clear and concise language. Let’s remove “moments” from our statistical lexicon."
  },
  {
    "objectID": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html",
    "href": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html",
    "title": "Exploratory Spatial Data Analysis in R",
    "section": "",
    "text": "Geospatial data is becoming increasingly common across domains and industries. Spatial data is no longer only in the hands of soil scientists, meteorologists, and criminologists, but in marketing, retail, finance, etc. It is common for spatial data to be treated as any other tabular data set. However, there is information to be drawn from our data’s relation to space. The standard exploratory data analysis toolkit will not always suffice. In this talk I introduce the basics of exploratory spatial data analysis (ESDA) and the {sfdep} package. {sfdep} builds on the shoulders of {spdep} for spatial dependence, emphasizes the use of simple features and the {sf} package, and integrates within your tidyverse-centric workflow. By the end of this talk users will understand the basics of ESDA and know how to start incorporating these skills in their own work."
  },
  {
    "objectID": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#about-the-talk",
    "href": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#about-the-talk",
    "title": "Exploratory Spatial Data Analysis in R",
    "section": "",
    "text": "Geospatial data is becoming increasingly common across domains and industries. Spatial data is no longer only in the hands of soil scientists, meteorologists, and criminologists, but in marketing, retail, finance, etc. It is common for spatial data to be treated as any other tabular data set. However, there is information to be drawn from our data’s relation to space. The standard exploratory data analysis toolkit will not always suffice. In this talk I introduce the basics of exploratory spatial data analysis (ESDA) and the {sfdep} package. {sfdep} builds on the shoulders of {spdep} for spatial dependence, emphasizes the use of simple features and the {sf} package, and integrates within your tidyverse-centric workflow. By the end of this talk users will understand the basics of ESDA and know how to start incorporating these skills in their own work."
  },
  {
    "objectID": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#recording",
    "href": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#recording",
    "title": "Exploratory Spatial Data Analysis in R",
    "section": "Recording",
    "text": "Recording"
  },
  {
    "objectID": "posts/2019-05-25-introducing-trendyy.html",
    "href": "posts/2019-05-25-introducing-trendyy.html",
    "title": "Introducing trendyy",
    "section": "",
    "text": "trendyy is a package for querying Google Trends. It is build around Philippe Massicotte’s package gtrendsR which accesses this data wonderfully.\nThe inspiration for this package was to provide a tidy interface to the trends data."
  },
  {
    "objectID": "posts/2019-05-25-introducing-trendyy.html#getting-started",
    "href": "posts/2019-05-25-introducing-trendyy.html#getting-started",
    "title": "Introducing trendyy",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstallation\nYou can install trendyy from CRAN using install.packages(\"trendyy\").\n\n\nUsage\nUse trendy() to search Google Trends. The only mandatory argument is search_terms. This is a character vector with the terms of interest. It is important to note that Google Trends is only capable of comparing up to five terms. Thus, if your search_terms vector is longer than 5, it will search each term individually. This will remove the direct comparative advantage that Google Trends gives you.\n\nAdditional arguments\n\n\nfrom: The beginning date of the query in \"YYYY-MM-DD\" format.\nto: The end date of the query in \"YYYY-MM-DD\" format.\n... : any additional arguments that would be passed to gtrendsR::gtrends(). Note that it might be useful to indicate the geography of interest. See gtrendsR::countries for list of possible geographies.\n\n\n\nAccessor Functions\n\nget_interest(): Retrieve interest over time\nget_interest_city(): Retrieve interest by city\nget_interest_country(): Retrieve interest by country\nget_interest_dma(): Retrieve interest by DMA\nget_interest_region(): Retrieve interest by region\nget_related_queries(): Retrieve related queries\nget_related_topics(): Retrieve related topics"
  },
  {
    "objectID": "posts/2019-05-25-introducing-trendyy.html#example",
    "href": "posts/2019-05-25-introducing-trendyy.html#example",
    "title": "Introducing trendyy",
    "section": "Example",
    "text": "Example\nSeeing as I found an interest in this due to the relatively pervasive use of Google Trends in political analysis, I will compare the top five polling candidates in the 2020 Democratic Primary. As of May 22nd, they were Joe Biden, Kamala Harris, Beto O’Rourke, Bernie Sanders, and Elizabeth Warren.\nFirst, I will create a vector of my desired search terms. Second, I will pass that vector to trendy() specifying my query date range from the first of 2019 until today (May 25th, 2019).\n\ncandidates &lt;- c(\"Joe Biden\", \"Kamala Harris\", \"Beto O'Rourke\", \"Bernie Sanders\", \"Elizabeth Warren\")\n\ncandidate_trends &lt;- trendy(candidates, from = \"2019-01-01\", to = Sys.Date())\n\nNow that we have a trendy object, we can print it out to get a summary of the trends.\n\ncandidate_trends\n#&gt; ~Trendy results~\n#&gt; \n#&gt; Search Terms: Joe Biden, Kamala Harris, Beto O'Rourke, Bernie Sanders, Elizabeth Warren\n#&gt; \n#&gt; (&gt;^.^)&gt; ~~~~~~~~~~~~~~~~~~~~ summary ~~~~~~~~~~~~~~~~~~~~ &lt;(^.^&lt;)\n#&gt; # A tibble: 5 × 5\n#&gt;   keyword          max_hits min_hits from       to        \n#&gt;   &lt;chr&gt;               &lt;int&gt;    &lt;int&gt; &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 Bernie Sanders         21        1 2019-01-06 2022-11-06\n#&gt; 2 Beto O'Rourke           1        0 2019-01-06 2022-11-06\n#&gt; 3 Elizabeth Warren        8        1 2019-01-06 2022-11-06\n#&gt; 4 Joe Biden             100        1 2019-01-06 2022-11-06\n#&gt; 5 Kamala Harris          48        1 2019-01-06 2022-11-06\n\nIn order to retrieve the trend data, use get_interest(). Note, that this is dplyr friendly.\n\nget_interest(candidate_trends)\n#&gt; # A tibble: 1,005 × 7\n#&gt;    date                 hits keyword   geo   time                  gprop category      \n#&gt;    &lt;dttm&gt;              &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;                 &lt;chr&gt; &lt;chr&gt;         \n#&gt;  1 2019-01-06 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  2 2019-01-13 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  3 2019-01-20 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  4 2019-01-27 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  5 2019-02-03 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  6 2019-02-10 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  7 2019-02-17 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  8 2019-02-24 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  9 2019-03-03 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt; 10 2019-03-10 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt; # … with 995 more rows\n#&gt; # ℹ Use `print(n = ...)` to see more rows\n\n\nPlotting Interest\n\ncandidate_trends %&gt;% \n  get_interest() %&gt;% \n  ggplot(aes(date, hits, color = keyword)) +\n  geom_line() +\n  geom_point(alpha = .2) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"\", \n       y = \"Relative Search Popularity\",\n       title = \"Google Search Popularity\")\n\n\n\nIt is also possible to view the related search queries for a given set of keywords using get_related_queries().\n\ncandidate_trends %&gt;% \n  get_related_queries() %&gt;% \n  group_by(keyword) %&gt;% \n  sample_n(2)\n#&gt; # A tibble: 10 × 5\n#&gt; # Groups:   keyword [5]\n#&gt;    subject  related_queries value                        keyword          category      \n#&gt;    &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;                        &lt;chr&gt;            &lt;chr&gt;         \n#&gt;  1 +3,450%  rising          klobuchar                    Bernie Sanders   All categories\n#&gt;  2 81       top             joe biden                    Bernie Sanders   All categories\n#&gt;  3 32       top             kamala harris                Beto ORourke     All categories\n#&gt;  4 Breakout rising          beto orourke announcement    Beto ORourke     All categories\n#&gt;  5 Breakout rising          elizabeth warren beer video  Elizabeth Warren All categories\n#&gt;  6 40       top             elizabeth warren net worth   Elizabeth Warren All categories\n#&gt;  7 Breakout rising          joe biden stimulus           Joe Biden        All categories\n#&gt;  8 Breakout rising          joe biden senile             Joe Biden        All categories\n#&gt;  9 Breakout rising          kamala harris husbands       Kamala Harris    All categories\n#&gt; 10 30       top             vice president kamala harris Kamala Harris    All categories\n\n\n\nUseful Resources\n\nHow Trends Data Is Adjusted\nPost by Google News Lab"
  },
  {
    "objectID": "posts/2020-09-07-demographic-change-white-fear-and-social-construction-of-race.html",
    "href": "posts/2020-09-07-demographic-change-white-fear-and-social-construction-of-race.html",
    "title": "Demographic Change, White Fear, and Social Construction of Race",
    "section": "",
    "text": "Two or three weeks ago, somewhere between Carter Dome and Mount Hight in the White Mountains of New Hampshire my friend posed a thought experiment to me. It’s one that I have heard dozens of times whether at a bar top, a fire pit, or an inflatable tube on the Pemigewasset River. It goes something like this.\n\nNote that this is rather extreme example and may not be comfortable for some readers. But thought experiments are supposed to be uncomfortable.\n\n“Take the country Iceland, it has a small population of about 350,000. Say, 100,000 Chinese immigrants move to the country within the period of a year. Is it still Iceland?”\n“Yes, of course.”\n“Okay, say this new population brings a massive baby boom. We know the fertility rate in China is much greater than that of Iceland. This new population has parity with the original 350,000 Icelanders. Making 700,000 total. A massive election is held and there is complete overturn of elected officials and each new official is either from the massive Chinese influx or immediate descendants of the Chinese immigrants. This new government enacts laws that greatly resemble China. Is this country no longer Iceland? What about the Icelandic culture? How can it be preserved? Are you okay with the destruction of a culture?”\nAt this point, for some reason, I’ve always found it tough to provide an argument that can persuade him. Upon reflection, it’s likely because the conversation shifts abruptly from one of pure demographic consideration to one of cultural preservation. The thought experiment feels challenging mainly because the idea of an ethnic and cultural Iceland is portrayed as some static, unshifting, unyielding, monolith. And that is what is at the crux of this.\nThere is an extant fear of racial elimination as a product of demographic growth. Research shows that when white individuals learn about a projected demographic shift from being a majority to minority of the population they show racial preferences for their identified race (source). This has consequences for political party preference as well. White Americans who express concern become more “conservative”—a term I increasingly struggle to use or condone the use of—political views and lead to a great partisan divide (source). Rather prescient, right?\nIceland, while they do not maintain official statistics on race, we do know that approximately 94% of the population are ethnically Icelandic. If we take the complement as entirely people of color (POC) that makes Iceland at most 6% POC. It is likely much less. But what does it mean to be ethnically Icelandic?\nIceland is a discovered land. At the time of its settlement by Norwegians in the 9th century, the land was uninhabited. Icelandic settlers, confirmed by genomic study, are largely from the Scandinavian countries, Ireland, and Scotland. Thus, in the one thousand and change years since its inception, ethnic Icelanders were derived from a melange of Northern Europeans. It would be unreasonable to think that sex would only occur between people of the same homeland indefinitely—that small genetic pool would lead to things like the Hapsburg Jaw. This is illustrative of two points pertinent to the thought experiment.\n\nEthnic Icelanders are descendants of other ethnic groups. Or, put another way, ethnicity is a social construction.\nThe movement of people is a constant in human history.\n\nSay, for the sake of the mental experiment, we give way to the idea that there is an Icelandic culture which can be nailed down and is not in flux. When was it in its purest state? Surely, if we take a snapshot of Icelandic culture of today, it would be unrecognizable to people a century ago, or maybe considerably different than even a few decades ago.\nIf, however, we define culture as an artifact of the history of Iceland—as we rightly should—where the past is important in informing the present, then we must be willing to concede that was is happening presently will become context for understanding Icelandic culture in the future. And that what will happen is soon to be the present and, following, the past (time is a construct I still don’t fully grasp). This is all to say is that culture is a constantly changing (in a state of flux) and that the concept of indefinite cultural preservation is unattainable—and, I’d argue, undesirable. We must accept that populations grow and change; that movement of peoples is a constant in human history; that culture is not a monolith and is constantly shifting; and that ethnicity and race is are myths.\nAt the end of the day, his thought experiment isn’t so much a thought experiment but rather an argument against in-migration. People will move across borders—another social construction, but perhaps with more contemporary utility than that of ethnicity—and the directionality is not only in, but it is also out. For every immigration problem there is an emigration problem. Call me a globalist, but I believe international borders should be more open.\nThere is a scene from Parks and Recreation where Leslie Knope refers to Ann Perkins as racially ambiguous. Today, this might be a problematic statement, but it is a reality of the future.\n\nAll of our descendants within a few generation will be ethnically ambiguous and new ethnic and racial identities will emerge. Fearing change solely based on the color of others’ skin and your preconceptions of them is not a good reason for fearing change. In this thought experiment, the concern ought not be about culture and race. But rather the focus of my argument should have been that of infrastructure. How can we ensure that there is enough housing? Nourishment? Education? Opportunity? You know, the things that truly matter to humanity."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "recent posts",
    "section": "",
    "text": "Building local packages for WebR\n\n\n\n\n\n\nwebr\n\n\nr\n\n\nwasm\n\n\n\n\n\n\n\n\n\nApr 30, 2025\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nCreate and edit TOML in R with {tomledit}\n\n\n\n\n\n\nr\n\n\ntoml\n\n\nrust\n\n\nextendr\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n🎄dvent Day 1: Rust and R solutions\n\n\n\n\n\n\nr\n\n\nrust\n\n\naoc\n\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing OpenID Connect (OIDC) in R\n\n\n\n\n\n\nr\n\n\nhttr2\n\n\nauth\n\n\noidc\n\n\n\n\n\n\n\n\n\nNov 28, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nS7 & Options objects\n\n\nreimagining readr::read_csv()\n\n\n\npkg-dev\n\n\nr\n\n\ns7\n\n\n\n\n\n\n\n\n\nNov 21, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nAdd syntax highlighting to leptos\n\n\n\n\n\n\nrust\n\n\nleptos\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nGet notified of failing CRAN checks\n\n\nA GitHub Action for package developers\n\n\n\ncran\n\n\nr\n\n\npkg-dev\n\n\n\n\n\n\n\n\n\nSep 1, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nType safe(r) R code\n\n\nRobust type checking with r-lib\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nRead a CSV in a production API\n\n\n{plumber} and multipart request #RinProd\n\n\n\nplumber\n\n\nr\n\n\nprod\n\n\n\n\n\n\n\n\n\nJun 25, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nStatic file server in R\n\n\n{plumber} is (always) the answer\n\n\n\nr\n\n\nplumber\n\n\napi\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nCaching WebR from CDN\n\n\nNotes from developing webr-js-rs\n\n\n\nwebr\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluate strings as code\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nDocker: keep your secrets secret\n\n\n\n\n\n\nproduction\n\n\ndocker\n\n\n\n\n\n\n\n\n\nJun 11, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a Ridiculously Fast™ API Client\n\n\nDesign choices for a highly performant R package\n\n\n\n\n\n\n\n\nJun 6, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIndexMap instead of BTreeMap\n\n\n\n\n\n\nrust\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n{duckdb} or {duckplyr}?\n\n\nDuckDB and the R ecosystem\n\n\n\nr\n\n\nprod\n\n\nduckdb\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial ML: Predicting on out-of-sample data\n\n\n3 approaches to using spatially derived features\n\n\n\nspatial\n\n\nml\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nUnivariate Spatial Dimensionality Reduction\n\n\nExtening PCoA and Moran Eigenvector Maps to include attributes\n\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nEncoding spatial patterns as variables\n\n\nPrincipal Coordinate Analysis & Moran Eigenvectors\n\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nDatabases for Data Scientist\n\n\nAnd why you probably dont need one\n\n\n\nproduction\n\n\narrow\n\n\nduckdb\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we sleep on factors?\n\n\nAnd how I wish things may behave?\n\n\n\nr\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nShared segment of parallel lines\n\n\n\n\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nLeptos & wasm-bindgen\n\n\nnote to self: its tricksy\n\n\n\nrust\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWriting S3 head() methods\n\n\na note to self for later\n\n\n\npkg-dev\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a DataFusion CSV reader with arrow-extendr\n\n\nextending R with Arrow and Rust\n\n\n\nrust\n\n\npkg-dev\n\n\nextendr\n\n\narrow\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nEnums in R: towards type safe R\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhere am I in the sky?\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhere am I in the sky?\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExport Python functions in R packages\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExport Python functions in R packages\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Science Across Languages\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nValve: putting R in production\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nR is still fast: a salty reaction to a salty blog post\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s so special about arrays?\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFeeling rusty: counting characters\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFeeling rusty: counting characters\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nRust traits for R users\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nlearning rust\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nJHU talk (slides)\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nRaw strings in R\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically Create Formulas in R\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nYouTube Videos & what not\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFishnets and overlapping polygons\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nComplete spatial randomness\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nspacetime representations aren’t good—yet\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMake your R scripts Databricks notebooks\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Spatial Data Analysis in R\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMy new IDE theme: xxEmoCandyLandxx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nActually identifying R package System Requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe heck is a statistical moment??\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Python on my M1 in under 10 minutes\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSLICED! a brief reflection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n{cpcinema} & associated journey\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nOSINT in 7 minutes\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nAPIs: the language agnostic love story\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nPython & R in production — the API way\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nColor Palette Cinema\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSecure R Package Environments\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is critical race theory, anyways?\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nDemographic Change, White Fear, and Social Construction of Race\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMedium Data and Production API Pipeline\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Red Queen Effect\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExcel in pRod\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nDesign Paradigms in R\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nR Security Concerns and 2019 CRAN downloads\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nNon-interactive user tokens with googlesheets4\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFinding an SPSS {haven}\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Tidy Modeling\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWater Quality Analysis\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n∑ { my parts }\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Trends for Campaigns\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWeb-scraping for Campaigns\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing trendyy\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\ngenius tutorial\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\ngenius Plumber API\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Fallacy of one person, one vote\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cost of Gridlock\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nxgboost feature importance\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n[Not so] generic functions\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nUS Representation: Part I\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing: Letters to a layperson\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nChunking your csv\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nReading Multiple csvs as 1 data frame\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nCoursera R-Programming: Week 2 Problems\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing geniusR\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "posts"
    ]
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html",
    "title": "What is critical race theory, anyways?",
    "section": "",
    "text": "TL;DR critical race theory is a mental framework used for understanding racial inequality that focuses on power imbalances.\nTrump recently suggested that all educational institutions stop teaching critical race theory and, if they fail to do so, lose funding. Like most things Trump does I was appalled. But this came from a different place. This came from a fear of educational censorship and suppression of science.\nBut what the hell is critical race theory, anyways? What does it matter?\nTo understand critical race theory we need to understand where it came from. Critical race theory came from the sociological critical theory. And sociological critical theory came from what is called conflict theory. And conflict theory came from—now don’t lose it—Karl Marx. Let’s try and grasp each theory in chronological order from when they were created to understand how each new theory came to be.\nThe very rough timeline looks like the below."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#historical-materialism",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#historical-materialism",
    "title": "What is critical race theory, anyways?",
    "section": "Historical Materialism",
    "text": "Historical Materialism\nOkay, Karl Marx. Sure, sure, he’s considered the father of communism. But he is really so much more than that. His theories have been absolutely critical to the social sciences for decades.\nMarx believed that there were two broad categories of people in a society, the proletariat and the bourgeoisie. Think of them as the workers and the business owners / managers, respectively. These two groups are always in tension with each other. Workers want better pay and better working conditions. Business owners want to save costs by paying workers less to increase their earnings at the margin (margin is jargon for each additional unit of goods and services). In order to get along they each need to concede to achieve something in the middle ground. This meeting in the middle is how progress is supposedly made.\n\n“Meeting in the middle” is a simplification of the German philosopher Hegel’s idea of a dialectic. A dialectic is described as “thesis, antithesis, synthesis.” In normal people words a dialectic is two opposites (thesis and antithesis) creating something (synthesis)."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#conflict-theory",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#conflict-theory",
    "title": "What is critical race theory, anyways?",
    "section": "Conflict Theory",
    "text": "Conflict Theory\nConflict theory is a bit more general than Marx’s Historical Materialism. Conflict theory suggests that social structures are created from power struggles between different groups of people—not just proletariat and bourgeoisie. One group may have more authority and resources at hand and are using it to the detriment of the other group. We can think of students and teacher, homeless and housed, secular and religious, so on and so forth.\nIn sociology, conflict theory is considered the antithesis (the direct opposite) of functionalism. Functionalism states that each social institution exists to serve some purpose. For example, policing serves the social function of reducing crime. Functionalists tend to think that’s a good thing. Conflict theorists are likely to disagree because they see the power imbalance between the oppressed and the oppressor causing crime. These social institutions that we create tend to reinforce the status quo."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-theory",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-theory",
    "title": "What is critical race theory, anyways?",
    "section": "Critical Theory",
    "text": "Critical Theory\nOkay so we’re getting closer to critical race theory. To recap:\n\nWe can trace critical race theory’s origin to Marx’s Historical Materialism.\nHistorical Materialism says that history is a product of economic struggle between workers and business owners.\nConflict theory generalizes Marx’s theory to say that social structures are created by tension between groups based on interests, resources, and power.\n\nCritical theory is famously defined as\n\n“an essential element in historical effort to create a world which satisfied the needs and powers of men…[and] its goal is man’s emancipation from slavery” - Horkheimer\n\nCritical theory is essentially conflict theory but with an embedded social critique. It’s goal is to improve the human condition by illustrating power imbalance and to improve the conditions of the oppressed. Because of this desire to improve society, critical theory is often used by activists.\n\n\n\n\n\n\nThe above diagram attempts to illustrate the hierarchical nature of these theories."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-race-theory",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-race-theory",
    "title": "What is critical race theory, anyways?",
    "section": "Critical Race Theory",
    "text": "Critical Race Theory\nNow making the leap from conflict theory and critical theory to critical race theory isn’t all that difficult. We understand that there is historical conflict between groups of people. This conflict creates social structures and reinforces the relative power of one group to another. The created social structures perpetuate and often exacerbate inequalities. A lot of times the inequalities between groups, from a philosophical standpoint, are incongruent with our beliefs and need to be rectified or improved.\nCritical race theory, then, is the application of critical theory to the concept of race. In the American context we can understand critical race theory going all the way back to the 17th century.\n\nCritical Race Theory in the United States: the really, really, and I cannot stress this enough, really, short version\n(White) Europeans and then Americans enslaved Africans to be their source of labor. Europeans built institutions to maintain slave trading. The economy of the new world was built almost entirely on “free” labor. Social structures were modified to reinforce ownership rights between people and of people.\nThen one day on July 4th, 1776 this really important document was written which said\n\n“We hold these truths to be self-evident, that all men [people] are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.” – Declaration of Independence\n\nBut that wasn’t true at all. Those words were aspirational at best. In the south, slavery was so deeply embedded that a life without it was deemed problematic enough to start a war over. When Mississippi seceded from the United States prior to the Civil War they enumerated their grievances (much like the Declaration of Independence did) to justify their departure from the US and wrote:\n\n“Our position is thoroughly identified with the institution of slavery—the greatest material interest of the world. Its labor supplies the product which constitutes by far the largest and most important portions of commerce of the earth.” – A Declaration of the Immediate Causes which Induce and Justify the Secession of the State of Mississippi from the Federal Union, 1861\n\nIt wasn’t until 1865 when the 13th Amendment was ratified, 1868 for the 14th Amendment and still that wasn’t enough. In 1896 we had the landmark Plessy v. Ferguson court case which paved the way for Jim Crow laws—a.k.a. American apartheid. It wasn’t until 1964 until the Civil Rights Act was passed. But the passing of laws doesn’t doesn’t change our thinking or our behaviors right away. We still have social structures and institutions which condition and alter our thinking. They don’t disappear with the stroke of a pen.\nIn short, sh!t has been f*cked up in the United States for a very long time and things still aren’t perfect. They’re better. But they’re not good. Critical race theory helps us understand how we got here, why we’re still having problems, and suggests some ways that we can improve it.\nIf we suppress critical race theory we’re also tossing aside critical theory, conflict theory, and historical materialism. These are theories that have helped us make sense of the world for over one hundred years. If we allow this censorship we’re giving way to an unjust power imbalance—the very thing these theories help us understand. If we throw away textbooks we’re walking down the same path that was taken in 1930s Germany and in 1960s China. If we allow suppression of free thought we’re giving way to authoritarians who don’t want you to challenge injustices."
  },
  {
    "objectID": "posts/2021-04-04-OSINT-youtube.html",
    "href": "posts/2021-04-04-OSINT-youtube.html",
    "title": "OSINT in 7 minutes",
    "section": "",
    "text": "As I was perusing the bowels of YouTube, as one does, I stumbled across a video titled “Using My Python Skills To Punish Credit Card Scammers”. The video is both whimsical, informative, and largely educational. It teaches us about:\n\nweb scraping / resource discovery\nsending API requests\nscaling requests using process threading.\n\n\n\nEngineer Man received a phishing text message. He opens up the url in an incognito browser and begins to walk us through the steps. He fills out the fake Amazon form and begins toying with the submit form with fake data. Upon reading the error messages, he gets the sense that the form might be utilizing a real service. And doing so is never free. He takes this as an opportunity to exploit the hacker’s use of the service.\nHe does this by taking advantage of my latest favorite thing—API requests. Engineer Man walks us through the process of opening up the web developer tools in a browser and using them to our advantage to understand the network traffic. He identifies the POST endpoint and shows us how to make a request from the information that is provided.\nBut this isn’t where it ends. Engineer Man’s face lights up as he begins to talk about threading. He utilizes the threading library to spin up 50 infinite loops.\nThis video sparked my excitement as it is a wonderful expample of open source intelligence (OSINT). OSINT is the process of collecting publicly all available data to be used by attackers. While attacking someone is not OSINT, the information gathering process is. This video is a wonderful resource for illustrating how to utilize the browser tools and public, though, hidden API endpoints."
  },
  {
    "objectID": "posts/2018-11-15-introducing-letters-to-a-layperson.html",
    "href": "posts/2018-11-15-introducing-letters-to-a-layperson.html",
    "title": "Introducing: Letters to a layperson",
    "section": "",
    "text": "I have been in the world of academia for nearly five years now. During this time I’ve read countless scholarly journal articles that I’ve struggled to wrap my head around. The academic language is riddled with obfuscating words like “milieux” and “nexus” which are often used to explain relatively simple concepts in a not so simple language. I’ve had to train myself to understand the academic language and translate it to regular people (layperson) speak.\nThe academic dialect is often associated with the “elitist media” (see Chomsky) which has recently been blamed for creating a strong divide in American politics—as we’ve seen since the beginning of the 2016 presidential primaries. Many words, phrases, and ideas have been shrouded by this language barrier. I have been trying to break down this barrier for myself for years now. I feel like I’ve only made a small dent. I have been trying to educate myself, a layperson, on these phrases and concepts.\nAs an undergraduate student I studied sociology and anthropology, but I found that I was enamored with economics, political science, urban theory, data science, psychology, and other disciplines. Across these fields there are identical concepts represented by different words or phrases—an ever frustrating thing. This is a barrier to understanding these fields. You must know certain ideas, words, and histories to understand the content.\nI have been collecting notes on these ideas and often revisit them to remind myself of what they are, what they mean, and why they exist. These notes were created for a myself, a layperson.\nIn this series of forthcoming posts, I will write about concepts that I wish I knew better in a language that I can understand. I call this collection of posts Letters To a Layperson, inspired by the phenomenal book Letters to a Young Contrarian by Christopher Hitchens."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html",
    "title": "The Fallacy of one person, one vote",
    "section": "",
    "text": "On October 6, 2018, the US Senate voted 50–48 in favor of the appointment of Associate Justice Brett Kavanaugh. This led many pundits to point out a “disconnect” between the Senate and the body politic. The 50 senators who voted “yea” represent only 44% of the nation’s population. The year prior, Supreme Court Justice Neil Gorsuch was confirmed by 54 senators representing approximately 45% of the population. This trend of increasing control by a decreasing portion of the constituency has been attributed to a rise in partisanship.\nSince the mid 90’s Dr. Frances E. Lee has been developing a body of literature on Senate apportionment, and her book Sizing Up the Senate has become part of the current political milieu (Vox, CNN, New York Times). The book discusses, among many things, the relevant historical context surrounding the creation and organization of the Senate at the constitutional Convention. In her 1998 paper “The Consequences of Senate Apportionment for the Geographic Distribution of Federal Funds” (Lee, 1998), Dr. Lee describes the “representation index”, a measure to quantify the over- or underrepresentation of a state in the US senate. In the formulation described in the paper, “the index is simply the ratio of the state’s actual population to 1/50th of the nation’s population” (Lee, 1998). In the formulation described in the paper, “the index is simply the ratio of the state’s actual population to 1/50th of the nation’s population” (Lee, 1998). It is written mathematically as \\(\\frac{State \\ Population}{1/50 \\ * \\ US \\ Population}\\). This creates a number between \\((0, \\infty)\\). As it is put in Sizing up the Senate,\nMany of Lee’s analyses utilize this index, and it has proved useful in temporal comparisons and modeling. However, it does not seem immediately capable of effectively evaluating other legislative bodies such as the House of Representatives. Here I will put forth an adaptation of this measure. That measure will then be adjusted to evaluate the House of Representatives. The House model will then be generalized to fit any representative body."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#interpretibility",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#interpretibility",
    "title": "The Fallacy of one person, one vote",
    "section": "Interpretibility",
    "text": "Interpretibility\nThe representation index has three main shortcomings, each of which are simple to address. First, the index produces a counter-intuitive number. An index of greater than 1 indicates an underrepresented state, and vice versa. Second, the interpretation of a middle value of 1 is useful for the “one person, one vote” standard. But the index is a divergent measure where 1 is the middle with the bounds (0,∞). Often, when one thinks of divergence, it is from an origin, or 0. Third, the index has a lower limit of 0 and no upper bound. This inhibits comparisons in both directions.\nTo illustrate the point let’s take the populations of California and Wyoming based on 2010 Census figures.\n\n\n\n\nRegion\nPopulation\n\n\n\nWY\n563626\n\n\nCA\n37253956\n\n\nUS\n308745538\n\n\n\n\n\nIf we calculate the representation index for these places, we get:\n\n\n\n\nRegion\nPopulation\nRepresentation Index\n\n\n\nWY\n563626\n0.09\n\n\nCA\n37253956\n6.03\n\n\n\n\n\nIn this example, California has a representation index of 6. This means that it is vastly underrepresented, whereas Wyoming has an index value of nearly 0 meaning it is vastly overrepresented. To interpret this, we must remember that a larger value actually means less representation.\nBut if we invert our formula, we obtain a more informative number.\n\n\n\n\nRegion\nPopulation\nNew Rep. Index\n\n\n\nWY\n563626\n10.96\n\n\nCA\n37253956\n0.17\n\n\n\n\n\nIn this table it is clear that Wyoming is overrepresented and California is underrepresented. But still, in evaluating these numbers we are required to do the mental math to contextualize the divergence from a middle value. California has a value of 0.83 less than the one person, one vote standard. To handle this, we can center the score around 0 by simply subtracting 1.\n\n\n\n\nRegion\nPopulation\nRepresentation Index\nNew Rep. Index\n\n\n\nWY\n563626\n0.09\n9.96\n\n\nCA\n37253956\n6.03\n-0.83\n\n\n\n\n\nThus the formula for the new representation index is \\(\\dfrac{\\dfrac{1}{50} \\ * \\ US \\ Population}{State \\ Population} - 1\\). When the measures are compared, we see that the initial measure used by Dr. Lee emphasizes underrepresentation of California, whereas the measure I have suggested emphasises the overrepresentation of Wyoming. From here on I will refer to these as the underrepresentation index (URI) and the overrepresentation index (ORI), respectively.\nThe URI and ORI are informative, but both are biased in scale. The bounds of the URI are \\((0,\\infty)\\) and the ORI are \\((-1, \\infty)\\). A value is needed that can simultaneously demonstrate the over- and underrepresentation of a state.\nThe ORI can be altered slightly to create this balanced measure. By taking the natural logarithm of the ratio 1/50th of the US population to a state’s population, a divergent scale naturally occurs. When the ratio is equal to 1 (or adhering to the one person, one vote standard), the value becomes 0. When the denominator is less than the numerator (or when the state has a smaller share of population than its share of votes), the value is positive and vice versa. Thus we arrive at the formula \\(\\ln\\Bigg({\\dfrac{\\dfrac{1}{50} \\ * \\ US \\ Population}{State \\ Population}}\\Bigg)\\). The following table compares these three measures.\n\n\n\n\nRegion\nPopulation\nURI\nORI\nNew Rep. Index\n\n\n\nWY\n563626\n0.09\n9.96\n2.39\n\n\nCA\n37253956\n6.03\n-0.83\n-1.80\n\n\n\n\n\nThe new representation index can be generalized to the House of Representatives or any other representative body. In the following sections, the representation index is adapted to the House of Representatives, the California Assembly and Senate, and the New Hampshire House and Senate."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#representation-index-and-the-house-of-representatives",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#representation-index-and-the-house-of-representatives",
    "title": "The Fallacy of one person, one vote",
    "section": "Representation Index and the House of Representatives",
    "text": "Representation Index and the House of Representatives\nRepresentation in the House of Representative is proportional meaning that a state has a number of legislative representatives proportional to its population. For example, if a state were to have 50% of the nation’s population it should represent 50% of the legislative body. This is the principle that the representation index evaluates.\nIn the above adaptation of the representation index, the nation’s population is divided by 50. This would be the population of a single state if every state had the same number of citizens. Then, that number is scaled (divided) by the state’s actual population, and the logarithm of the result is the representation index. Thus if a state’s population is exactly equal to 1/50th of the nation’s population, its representation in the Senate is proportional.\nTo adapt this measure to the House, we must think about how the relationship between proportional representation and population can be expressed numerically. As mentioned above, proportional representation would mean that a state comprising 50% of the national population would likewise comprise 50% of the House’s representatives. The ratio of these two proportions is 1, which creates a similar comparison to Lee’s ratio of 1/50th of national population to state population. This is the motivation for a formula of a representation index for the House of Representatives. The new formula, then, is \\(\\ln\\Big(\\frac{State \\ share \\ of \\ reps}{State \\ share \\ of \\ pop.}\\Big)\\).\n\n\n\n\n\n\nIn this case, if the share of the population is smaller than the share of representatives, the index is inflated, meaning the state is overrepresented. If the share of population is greater than the share of representatives, the index is deflated, meaning the state is underrepresented. This index ranges from \\((-\\infty, \\infty)\\)."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#generalizing-the-representation-index",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#generalizing-the-representation-index",
    "title": "The Fallacy of one person, one vote",
    "section": "Generalizing the Representation Index",
    "text": "Generalizing the Representation Index\nThe representation index for the House of Representatives is written in such a way that it can be adapted for any representative body. The formula evaluates equality of the share of representatives and the share of the total constituency. In general, the formula can be written as \\(\\ln\\Big(\\frac{\\%\\ share \\ of \\ reps}{\\% \\ share \\ of \\ constituency}\\Big)\\).\nTo illustrate, let’s use this formula to calculate the representation index of the Assembly and Senate of California. In 2011, after the most recent census, California redrew its districts. The data used in this demonstration are from the LA Times. The California Assembly and Senate have 80 and 40 members respectively each representing one district.\nFor this example, I consider a difference of 5% in either direction as adhering to the one person, one vote principle. To illustrate this, if the ratio is \\(\\frac{1%}{0.95}\\) the index score is 0.05. Alternatively, if the ratio is \\(\\frac{1}{1.05}\\), the index score is -0.05.\n\n\n\n\n\n\n\n\n\n\n\n\nThe above example demonstrates the use of the representation index for both houses of the California legislature. This is good news as it demonstrates that the state upholds the Equal Protection Clause of the Fourteenth Amendment and adheres to the Supreme Court decision Reynolds v. Sims, in which the court held that state districts must be proportionally drawn (unlike US Senate districts).\nAs Wikipedia states, “[given California]’s large population and relatively small legislature, the Assembly has the largest population per representative ratio of any lower house legislature in the United States; only the federal U.S. House of Representatives has a larger ratio.” California’s representative body differs greatly from that of, for example, New Hampshire.\nNew Hampshire has arguably the most unique lower house legislature of any state: there are 400 representatives from 204 districts. House districts also include what are called floterial districts, areas that represent multiple municipalities. The legality of such districts has been disputed in the state Supreme Court, but nonetheless they persist, and as a result, New Hampshire has one of the smallest constituent-to-representative ratios in the nation. This results in overrepresentation for almost every municipality.\n\n\n\n\n\n\nThe above chart illustrates this phenomenon. Interestingly, the most populous cities and towns in the region are represented according to the one person, one vote paradigm. When applied to the state senate, the results are much different.\n\n\n\n\n\n\nThe representation index for the New Hampshire Senate trends toward underrepresentation. The median value is shown with a dotted red line. It is apparent that the representation of the Senate of New Hampshire is not as equally representative as that of California. The population distribution across the state is highly unequal with a vast majority residents living close to the Maine and Massachusetts borders plausibly contributing to this inequality."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#further-directions",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#further-directions",
    "title": "The Fallacy of one person, one vote",
    "section": "Further Directions",
    "text": "Further Directions\nThe ability to compare representation across governing bodies has large implications for comparative political analysis. Further development of the representation index allows scholars and researchers to compare constituency representation among similar bodies—as demonstrated with the case of California and New Hampshire.\nThe new formulation of the representation index is conducive to inter-governmental body analysis. This is possible by the index’s ability to place bodies of different size on the same scale. A result of this is the ability to perform hypothesis testing among groups. As a motivating example, the representation indexes of states are compared along partisanship lines.\n\n\n\n\n\n\nState Senate representation indexes were calculated using the general representation index formula for all 50 states. A two-sample t-test was performed comparing states with two Republican senators to those with two Democratic senators. In doing so, we fail to reject the null hypothesis \\(( \\ t(39) = 1.117, \\ p = 0.27 \\ )\\) that there is a difference of representation index based solely on partisanship.\n\n\n\n\n\n\n\n\n\nParty\nn\nMean\nSD\nSE\n\n\n\nDemocrat\n21\n0.19\n0.99\n0.22\n\n\nRepublican\n23\n0.67\n0.99\n0.21\n\n\nSplit / Other\n6\n0.79\n1.16\n0.47\n\n\nTotal\n50\n0.48\n1.02\n0.14\n\n\n\n\n\nIt has been demonstrated that the representation index is an informative measure that can be utilized to examine over and underrepresentation of a governing body. This new formulation of the representation index is useful in its ability to evaluate both over and under-representation and to compare different political entities. One could imagine, for example, a comparison of constituency representation between the United States and France’s upper and lower legislative houses."
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html",
    "href": "posts/2018-01-27-introducing-geniusr.html",
    "title": "Introducing geniusR",
    "section": "",
    "text": "knitr::opts_chunk$set(eval=FALSE)"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#install-and-load-the-package",
    "href": "posts/2018-01-27-introducing-geniusr.html#install-and-load-the-package",
    "title": "Introducing geniusR",
    "section": "Install and load the package",
    "text": "Install and load the package\n\ndevtools::install_github(\"josiahparry/geniusR\")\n\nLoad the package:\n\nlibrary(geniusR)\nlibrary(tidyverse) # For manipulation"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#whole-albums",
    "href": "posts/2018-01-27-introducing-geniusr.html#whole-albums",
    "title": "Introducing geniusR",
    "section": "Whole Albums",
    "text": "Whole Albums\ngenius_album() allows you to download the lyrics for an entire album in a tidy format. There are two arguments artists and album. Supply the quoted name of artist and the album (if it gives you issues check that you have the album name and artists as specified on Genius).\nThis returns a tidy data frame with three columns:\n\ntitle: track name\ntrack_n: track number\ntext: lyrics\n\n\nemotions_math &lt;- genius_album(artist = \"Margaret Glaspy\", album = \"Emotions and Math\")\nemotions_math"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#multiple-albums",
    "href": "posts/2018-01-27-introducing-geniusr.html#multiple-albums",
    "title": "Introducing geniusR",
    "section": "Multiple Albums",
    "text": "Multiple Albums\nIf you wish to download multiple albums from multiple artists, try and keep it tidy and avoid binding rows if you can. We can achieve this in a tidy workflow by creating a tibble with two columns: artist and album where each row is an artist and their album. We can then iterate over those columns with purrr:map2().\nIn this example I will extract 3 albums from Kendrick Lamar and Sara Bareilles (two of my favotire musicians). The first step is to create the tibble with artists and album titles.\n\nalbums &lt;-  tibble(\n  artist = c(\n    rep(\"Kendrick Lamar\", 3), \n    rep(\"Sara Bareilles\", 3)\n    ),\n  album = c(\n    \"Section 80\", \"Good Kid, M.A.A.D City\", \"DAMN.\",\n    \"The Blessed Unrest\", \"Kaleidoscope Heart\", \"Little Voice\"\n    )\n)\n\nalbums\n\nNo we can iterate over each row using the map2 function. This allows us to feed each value from the artist and album columns to the genius_album() function. Utilizing a map call within a dplyr::mutate() function creates a list column where each value is a tibble with the data frame from genius_album(). We will later unnest this.\n\n## We will have an additional artist column that will have to be dropped\nalbum_lyrics &lt;- albums %&gt;% \n  mutate(tracks = map2(artist, album, genius_album))\n\nalbum_lyrics\n\nNow when you view this you will see that each value within the tracks column is &lt;tibble&gt;. This means that that value is infact another tibble. We expand this using tidyr::unnest().\n\n# Unnest the lyrics to expand \nlyrics &lt;- album_lyrics %&gt;% \n  unnest(tracks) %&gt;%    # Expanding the lyrics \n  arrange(desc(artist)) # Arranging by artist name \n\nhead(lyrics)"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#song-lyrics",
    "href": "posts/2018-01-27-introducing-geniusr.html#song-lyrics",
    "title": "Introducing geniusR",
    "section": "Song Lyrics",
    "text": "Song Lyrics\n\ngenius_lyrics()\nGetting lyrics to a single song is pretty easy. Let’s get in our ELEMENT. and checkout DNA. by Kendrick Lamar. But first, note that the genius_lyrics() function takes two main arguments, artist and song. Be sure to spell the name of the artist and the song correctly.\n\nDNA &lt;- genius_lyrics(artist = \"Kendrick Lamar\", song = \"DNA.\")\n\nDNA\n\nThis returns a tibble with three columns title, text, and line. However, you can specifiy additional arguments to control the amount of information to be returned using the info argument.\n\ninfo = \"title\" (default): Return the lyrics, line number, and song title.\ninfo = \"simple\": Return just the lyrics and line number.\ninfo = \"artist\": Return the lyrics, line number, and artist.\ninfo = \"all\": Return lyrics, line number, song title, artist."
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#tracklists",
    "href": "posts/2018-01-27-introducing-geniusr.html#tracklists",
    "title": "Introducing geniusR",
    "section": "Tracklists",
    "text": "Tracklists\ngenius_tracklist(), given an artist and an album will return a barebones tibble with the track title, track number, and the url to the lyrics.\n\ngenius_tracklist(artist = \"Basement\", album = \"Colourmeinkindness\")"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#nitty-gritty",
    "href": "posts/2018-01-27-introducing-geniusr.html#nitty-gritty",
    "title": "Introducing geniusR",
    "section": "Nitty Gritty",
    "text": "Nitty Gritty\ngenius_lyrics() generates a url to Genius which is fed to genius_url(), the function that does the heavy lifting of actually fetching lyrics.\nI have not figured out all of the patterns that are used for generating the Genius.com urls, so errors are bound to happen. If genius_lyrics() returns an error. Try utilizing genius_tracklist() and genius_url() together to get the song lyrics.\nFor example, say “(No One Knows Me) Like the Piano” by Sampha wasn’t working in a standard genius_lyrics() call.\n\npiano &lt;- genius_lyrics(\"Sampha\", \"(No One Knows Me) Like the Piano\")\n\nWe could grab the tracklist for the album Process which the song is from. We could then isolate the url for (No One Knows Me) Like the Piano and feed that into `genius_url().\n\n# Get the tracklist for \nprocess &lt;- genius_tracklist(\"Sampha\", \"Process\")\n\n# Filter down to find the individual song\npiano_info &lt;- process %&gt;% \n  filter(title == \"(No One Knows Me) Like the Piano\")\n\n# Filter song using string detection\n# process %&gt;% \n#  filter(stringr::str_detect(title, coll(\"Like the piano\", ignore_case = TRUE)))\n\npiano_url &lt;- piano_info$track_url\n\nNow that we have the url, feed it into genius_url().\n\ngenius_url(piano_url, info = \"simple\")"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#generative-functions",
    "href": "posts/2018-01-27-introducing-geniusr.html#generative-functions",
    "title": "Introducing geniusR",
    "section": "Generative functions",
    "text": "Generative functions\nThis package works almost entirely on pattern detection. The urls from Genius are (mostly) easily reproducible (shout out to Angela Li for pointing this out).\nThe two functions that generate urls are gen_song_url() and gen_album_url(). To see how the functions work, try feeding an artist and song title to gen_song_url() and an artist and album title to gen_album_url().\n\ngen_song_url(\"Laura Marling\", \"Soothing\")\n\n\ngen_album_url(\"Daniel Caesar\", \"Freudian\")\n\ngenius_lyrics() calls gen_song_url() and feeds the output to genius_url() which preforms the scraping.\nGetting lyrics for albums is slightly more involved. It first calls genius_tracklist() which first calls gen_album_url() then using the handy package rvest scrapes the song titles, track numbers, and song lyric urls. Next, the song urls from the output are iterated over and fed to genius_url().\nTo make this more clear, take a look inside of genius_album()\n\ngenius_album &lt;- function(artist = NULL, album = NULL, info = \"simple\") {\n\n  # Obtain tracklist from genius_tracklist\n  album &lt;- genius_tracklist(artist, album) %&gt;%\n\n    # Iterate over the url to the song title\n    mutate(lyrics = map(track_url, genius_url, info)) %&gt;%\n\n    # Unnest the tibble with lyrics\n    unnest(lyrics) %&gt;%\n    \n    # Deselect the track url\n    select(-track_url)\n\n\n  return(album)\n}\n\n\nNotes:\nAs this is my first “package” there will be many issues. Please submit an issue and I will do my best to attend to it.\nThere are already issues of which I am present (the lack of error handling). If you would like to take those on, please go ahead and make a pull request. Please contact me on Twitter."
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html",
    "title": "genius tutorial",
    "section": "",
    "text": "knitr::opts_chunk$set(eval = FALSE)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#introducing-genius",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#introducing-genius",
    "title": "genius tutorial",
    "section": "Introducing genius",
    "text": "Introducing genius\nYou want to start analysing song lyrics, where do you go? There have been music information retrieval papers written on the topic of programmatically extracting lyrics from the web. Dozens of people have gone through the laborious task of scraping song lyrics from websites. Even a recent winner of the Shiny competition scraped lyrics from Genius.com.\nI too have been there. Scraping websites is not always the best use of your time. genius is an R package that will enable you to programatically download song lyrics in a tidy format ready for analysis. To begin using the package, it first must be installed, and loaded. In addition to genius, we will need our standard data manipulation tools from the tidyverse.\n\ninstall.packages(\"genius\")\n\n\nlibrary(genius)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#single-song-lyrics",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#single-song-lyrics",
    "title": "genius tutorial",
    "section": "Single song lyrics",
    "text": "Single song lyrics\nThe simplest method of extracting song lyrics is to get just a single song at a time. This is done with the genius_lyrics() function. It takes two main arguments: artist and song. These are the quoted name of the artist and song. Additionally there is a third argument info which determines what extra metadata you can get. The possible values are title, simple, artist, features, and all. I recommend trying them all to see how they work.\nIn this example we will work to retrieve the song lyrics for the upcoming musician Renny Conti.\n\nfloating &lt;- genius_lyrics(\"renny conti\", \"people floating\")\nfloating"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#album-lyrics",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#album-lyrics",
    "title": "genius tutorial",
    "section": "Album Lyrics",
    "text": "Album Lyrics\nNow that you have the intuition for obtaining lyrics for a single song, we can now create a larger dataset for the lyrics of an entire album using genius_album(). Similar to genius_lyrics(), the arguments are artist, album, and info.\nIn the exercise below the lyrics for Snail Mail’s album Lush. Try retrieving the lyrics for an album of your own choosing.\n\nlush &lt;- genius_album(\"Snail Mail\", \"Lush\")\nlush"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#adding-lyrics-to-a-data-frame",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#adding-lyrics-to-a-data-frame",
    "title": "genius tutorial",
    "section": "Adding Lyrics to a data frame",
    "text": "Adding Lyrics to a data frame\n\nMultiple songs\nA common use for lyric analysis is to compare the lyrics of one artist to another. In order to do that, you could potentially retrieve the lyrics for multiple songs and albums and then join them together. This has one major issue in my mind, it makes you create multiple object taking up precious memory. For this reason, the function add_genius() was developed. This enables you to create a tibble with a column for an artists name and their album or song title. add_genius() will then go through the entire tibble and add song lyrics for the tracks and albums that are available.\nLet’s try this with a tibble of three songs.\n\nthree_songs &lt;- tribble(\n  ~ artist, ~ title,\n  \"Big Thief\", \"UFOF\",\n  \"Andrew Bird\", \"Imitosis\",\n  \"Sylvan Esso\", \"Slack Jaw\"\n)\n\nsong_lyrics &lt;- three_songs %&gt;% \n  add_genius(artist, title, type = \"lyrics\")\n\nsong_lyrics %&gt;% \n  count(artist)\n\n\n\n\nMultiple albums\nadd_genius() also extends this functionality to albums.\n\nalbums &lt;- tribble(\n  ~ artist, ~ title,\n  \"Andrew Bird\", \"Armchair Apocrypha\",\n  \"Andrew Bird\", \"Things are really great here sort of\"\n)\n\nalbum_lyrics &lt;- albums %&gt;% \n  add_genius(artist, title, type = \"album\")\n\nalbum_lyrics\n\nWhat is important to note here is that the warnings for this function are somewhat informative. When a 404 error occurs, this may be because that the song does not exist in Genius. Or, that the song is actually an instrumental which is the case here with Andrew Bird.\n\n\nAlbums and Songs\nIn the scenario that you want to mix single songs and lyrics, you can supply a column with the type value of each row. The example below illustrates this. First a tibble with artist, track or album title, and type columns are created. Next, the tibble is piped to add_genius() with the unquote column names for the artist, title, and type columns. This will then iterate over each row and fetch the appropriate song lyrics.\n\nsong_album &lt;- tribble(\n  ~ artist, ~ title, ~ type,\n  \"Big Thief\", \"UFOF\", \"lyrics\",\n  \"Andrew Bird\", \"Imitosis\", \"lyrics\",\n  \"Sylvan Esso\", \"Slack Jaw\", \"lyrics\",\n  \"Movements\", \"Feel Something\", \"album\"\n)\n\nmixed_lyrics &lt;- song_album %&gt;% \n  add_genius(artist, title, type)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#self-similarity",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#self-similarity",
    "title": "genius tutorial",
    "section": "Self-similarity",
    "text": "Self-similarity\nAnother feature of genius is the ability to create self-similarity matrices to visualize lyrical patterns within a song. This idea was taken from Colin Morris’ wonderful javascript based Song Sim project. Colin explains the interpretation of a self-similarity matrix in their TEDx talk. An even better description of the interpretation is available in this post.\nTo use Colin’s example we will look at the structure of Ke$ha’s Tik Tok.\nThe function calc_self_sim() will create a self-similarity matrix of a given song. The main arguments for this function are the tibble (df), and the column containing the lyrics (lyric_col). Ideally this is one line per observation as is default from the output of genius_*(). The tidy output compares every ith word with every word in the song. This measures repetition of words and will show us the structure of the lyrics.\n\ntik_tok &lt;- genius_lyrics(\"Ke$ha\", \"Tik Tok\")\n\ntt_self_sim &lt;- calc_self_sim(tik_tok, lyric, output = \"tidy\")\n\ntt_self_sim\n\ntt_self_sim %&gt;% \n  ggplot(aes(x = x_id, y = y_id, fill = identical)) +\n  geom_tile() +\n  scale_fill_manual(values = c(\"white\", \"black\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        axis.text = element_blank()) +\n  scale_y_continuous(trans = \"reverse\") +\n  labs(title = \"Tik Tok\", subtitle = \"Self-similarity matrix\", x = \"\", y = \"\", \n       caption = \"The matrix displays that there are three choruses with a bridge between the last two. The bridge displays internal repetition.\")"
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "",
    "text": "Installing python has never been an easy task for me. I remember back in 2016 I wanted to learn how to use pyspark and thus python, I couldn’t figure out how to install python so I gave up. In graduate school I couldn’t install python so I used a docker container my professor created and never changed a thing. When working at RStudio I used the Jupyter Lab instance in RStudio Workbench when I couldn’t install it locally.\nNow, I want to compare pysal results to some functionality I’ve written in R. To do that, I need a python installation. I’ve heard extra horror stories about installing Python on the new Mac M1 chip—which I have.\nPrior to installing, I took to twitter for suggestions. I received the phenomenal tweet below encouraging me to install with {reticulate}1 which was absolutely phenomenal advice."
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#installing-python",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#installing-python",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Installing Python",
    "text": "Installing Python\nThe steps to install python, at least for me, was very simple.\n\nInstall reticulate\nInstall miniconda\n\ninstall.packages(\"reticulate\")\nreticulate::install_miniconda()\nThat’s it. That’s all it took."
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#creating-my-first-conda-environment",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#creating-my-first-conda-environment",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Creating my first conda environment",
    "text": "Creating my first conda environment\nAfter installing python, I restarted R, and began building my first conda environment. I created a conda environment called geo for my geospatial work. I installed libpysal, geopandas, and esda. These installed every other dependency I needed–e.g. pandas, and numpy.\nreticulate::conda_create(\"geo\")\nreticulate::use_condaenv(\"geo\")\nreticulate::conda_install(\"geo\", c(\"libpysal\", \"geopandas\", \"esda\"))"
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#using-my-conda-environment",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#using-my-conda-environment",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Using my conda environment",
    "text": "Using my conda environment\nTo begin using my new conda environment, I opened up a fresh R session and a fresh R Markdown document. In my first code chunk I told reticulate which conda environment to use. Then my following code chunks were python which opened up the python repl. Make sure that you start your code chunk with ```{python}\n\nreticulate::use_condaenv(\"geo\")\n\nIn the following example I utilize esda to calculate a local join count.\n\nimport libpysal\nimport geopandas as gpd\nfrom esda.join_counts_local import Join_Counts_Local\n\nfp = libpysal.examples.root + \"/guerry/\" + \"Guerry.shp\" \n\nguerry_ds = gpd.read_file(fp)\nguerry_ds['SELECTED'] = 0\nguerry_ds.loc[(guerry_ds['Donatns'] &gt; 10997), 'SELECTED'] = 1\n\nw = libpysal.weights.Queen.from_dataframe(guerry_ds)\n\nLJC_uni = Join_Counts_Local(connectivity=w).fit(guerry_ds['SELECTED'])\n\nLJC_uni.p_sim\n\n## array([  nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan, 0.435,   nan, 0.025, 0.025,   nan, 0.328,\n##          nan,   nan,   nan,   nan,   nan,   nan, 0.342,   nan, 0.334,\n##          nan,   nan,   nan,   nan,   nan,   nan, 0.329,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan, 0.481,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan, 0.02 ,   nan,   nan,   nan,   nan,   nan, 0.125,\n##          nan, 0.043,   nan,   nan])"
  },
  {
    "objectID": "projects/pkgs/sysreqs.html",
    "href": "projects/pkgs/sysreqs.html",
    "title": "R package system requirements",
    "section": "",
    "text": "GitHub repo\nRelated blog post\n\nThe goal of sysreqs is to make it easy to identify R package system dependencies. There are two components to this package: an “API” and a wrapper package.\nThis API and package is based on rstudio/r-system-requirements and the API client for RStudio Package Manager. The functionality is inspired by pak::pkg_system_requirements()."
  },
  {
    "objectID": "projects/writing/uitk.html",
    "href": "projects/writing/uitk.html",
    "title": "Urban Informatics Toolkit",
    "section": "",
    "text": "The Urban Informatics Toolkit (uitk) is an open text book I wrote with the intention of teaching first year graduate students in Urban Informatics the R programming language.\nWithin it are two years of study in the Urban Informatics Program at Northeastern University, five years of self-directed education in R, two years of teaching R, and innumerable hours learning R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "I make R go fast 🏎️💨\nMy name is Josiah (pronounced Joe-sigh-uh) and I believe R belongs in production.\nI have a penchant for writing R packages that are really fast and efficient. Typically, this involves writing Rust and glueing R them together using extendr.\nI also, quite specifically, like solving geospatial problems. I work at Esri doing spatial statistics and—you guessed it—writing R packages."
  },
  {
    "objectID": "index.html#letters-to-a-layperson-myself",
    "href": "index.html#letters-to-a-layperson-myself",
    "title": "Josiah Parry",
    "section": "",
    "text": "Welcome to my personal website. I always aspire to keep pumping out content, but I don’t.\nHere you will find my “recent” blog posts, possibly outdated biographical information, my socials, and links to projects I have made or worked on.\n\n\n\n\n\n\n\n\n\nDesign choices for a highly performant R package\n\n\n\n\n\n\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\n\n\n\n\n\n\nDuckDB and the R ecosystem\n\n\n\nr\n\n\nprod\n\n\nduckdb\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\n\n\n\n\n\n\n3 approaches to using spatially derived features\n\n\n\nspatial\n\n\nml\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\n\n\n\n\n\n\nExtening PCoA and Moran Eigenvector Maps to include attributes\n\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\n\n\n\n\n\n\nPrincipal Coordinate Analysis & Moran Eigenvectors\n\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\n\n\n\n\n\n\nAnd why you probably dont need one\n\n\n\nproduction\n\n\narrow\n\n\nduckdb\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\nAnd how I wish things may behave?\n\n\n\nr\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\n\n\n\n\n\n\nnote to self: its tricksy\n\n\n\nrust\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\n\n\n\n\n\n\na note to self for later\n\n\n\npkg-dev\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n\n\n\n\n\n\nextending R with Arrow and Rust\n\n\n\nrust\n\n\npkg-dev\n\n\nextendr\n\n\narrow\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\n\n\n\n\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\n\n\n\n\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\n\n\n\n\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\n\n\n\n\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\n\n\n\n\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\n\n\n\n\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\n\n\n\n\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\n\n\n\n\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\n\n\n\n\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "home "
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "employment: Senior Product Engineer @ Esri\neducation:\n\nMS Urban Informatic, Northeastern University (2020)\nBA Sociology, Plymouth State University\n\nMinor, General Mathematics\nProfessional Certificate GIS\nI am a Senior Product Engineer on the Spatial Analysis team at Esri. Previously, I was at The NPD Group as a Research Analyst where I worked to modernize our data science infrastructure to use Databricks, Docker, and Spark. Before that, I was at RStudio, PBC on the customer success team enabling public sector adoption of data science tools. In 2020 I received my master’s degree in Urban Informatics from Northeastern University following my bachelor’s degree in sociology with focuses in geographic information systems and general mathematics from Plymouth State University in 2018.",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "about",
    "section": "Contact me",
    "text": "Contact me\nIf you want to get in contact with me please send me an email at josiah.parry at gmail dot com.\n\n\ntalks i’ve given\n\n\nExploratory Spatal Data Analysis in the tidyverse\n\nJuly 28th, 2022 rstudio::conf(2022L)\n\nExploratory Spatial Data Analysis in R\n\nRecording\nApril 28th, 2022\n\nAPIs: you’re probably not using them and why you probably should\n\nGovernment Advances in Statistical Programming\nNovember 6th, 2020\n\n“Old Town Road” Rap or Country?: Putting R in Production with Tidymodels, Plumber, and Shiny\n\nBoston useR group\nDecember 10th, 2019\n\nTidy Lyrical Analysis\n\nBoston useR group\nJuly 17th, 2018\n\nNewfound Lake Landscape Value Analysis: Exploring the efficacy of PPGIS, NESTVAL 2016\n\nNew England St. Lawrence River Valley regional American Associations of Geographers Conference\n2016",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html",
    "href": "posts/2023-09-20-sdsl/index.html",
    "title": "Spatial Data Science Across Languages",
    "section": "",
    "text": "I feel very fortunate to have been invited to the first Spatial Data Science Across Languages (SDSL) workshop at the University of Münster. I am even more fortunate that I have an employer who sees the value in an event such as this and be my patron for it.\nThe event brought together package maintainers from Julia, Python, and R languages to just discuss. The event was loosely framed around a few broad discussion topics that were varied and drifted.\nIn general, the theme of the workshop was “standards.” We need standards be able to ensure cohesion not only within languages, but across them. Users should be able to move between languages and be able to expect similar behavior, have similar terminology, and expect the same analysis results."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#arrow",
    "href": "posts/2023-09-20-sdsl/index.html#arrow",
    "title": "Spatial Data Science Across Languages",
    "section": "Arrow",
    "text": "Arrow\nWe started everything off by discussing Arrow which set the theme of “standards.” Arrow gets conflated at many thing all at once—I do that. At the core Arrow is a memory format specification. It describes how data should be held in memory.\nR holds objects in memory one way, Python another, and Julia another as well. Arrow describes just one way that specific types of object can be held in memory. GeoArrow is an extension of Arrow that specifies the memory layout for geometry arrays.\n\nGeoArrow\nSpecifications like well-known text (WKT) and well-known binary (WKB) are encodings of a single geometry. GeoArrow recognizes that we almost never work with scalar objects alone. GeoArrow is a memory layout for an array of geometries.\n\nIf each language can hold Arrow arrays in memory, they can be passed from one tool to another with 0 cost. Python can create an arrow array and R can pick it up if it knows where it exists.\nThe current approach looks something like this. Each tool serializes its data in one way. In order for another tool to use it, the data needs to be copied (memory inefficient) and converted (computationally expensive) into the appropriate format.\n\nThe Arrow specification would allow data handoff between tools to be much more seamless and look like so:\n\n\n\nMaybe we ought to start framing adoption of Arrow as an effort to be more “green.” If we spend less time computing we use less energy which is overall a net positive for the world.\nThis is a massive productivity improvement. There’s no computation cost in converting between one format to another saving time, energy, and money.\n\n\nThere’s a good chance that in order to adopt Arrow in {sf} there will be breaking changes. I am an advocate for breaking changes when they are for a good reason. Being on the leading edge is how to make a language succeed.\nI also think if we can move towards a “trait-driven” approach to spatial data frames, we can support both GeoArrow geometries as well as current sfc objects.\nRead my spatial data frame manifesto.\nThe key thing though, is that in order for Arrow to be useful, it has to be adopted widely. If GeoPandas uses Arrow and {sf} does not, we have to go through the copy and convert process anyways.\n\n\nWhy GeoArrow excites me\nThe promise of Arrow and GeoArrow is that memory can be handed off between tools without any additional cost. This (in theory) lowers the bar for what is needed to hand off between tools and languages. Hopefully ending the language wars\nKyle Barron demonstrated really cool example use-case where he created GeoArrow arrays using GeoPolars. That array was then written to a buffer and picked up by javascript. Since there was no serialization or deserialization it was unbelievably fast!\n\nAdditionally, we are seeing WebAssembly proliferate in the data science community. WebR provides R users with the ability to execute R in the browser. This is also possible in Python, Rust, Go, and I’m sure many others. Each language can be compiled to be used in the browser and hand off components between them.\nClient side computation will reduce the need for server side operations. If we can reduce the amount of hours that servers are constantly running by offloading lighter operations into the browser, we may be able to save money, energy, be more green, and create tools that do not necessarily require an active internet connection."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#spatial-support",
    "href": "posts/2023-09-20-sdsl/index.html#spatial-support",
    "title": "Spatial Data Science Across Languages",
    "section": "Spatial Support",
    "text": "Spatial Support\nWe also discussed the more esoteric topic of spatial support. This was completely new to me. Support defines the relationship between an attribute to the geometry. There are two kinds:\n\npoint support - a constant values associated with every location in a geometry\n\nexample: temperature measurement at a weather station\n\nblock support - a value derived from aggregating measures over space\n\nexample: population count in a census tract\n\n\n\n\n\n\n\n\nTip\n\n\n\nRead chapter 1.6 of Spatial Data Science (SDS) for more on this topic.\n\n\nWhen geometries are manipulated and the associated attributes come along for the ride, support assumptions are often violated resulting in inaccurate calculations or maps.\n\nAttribute-Geometry Relationships\nSDS formalizes the relationship between attributes and geometry a bit further in something they call the Attribute-Geometry Relationship (AGR). Attributes of spatial features can have one of 3 types of AGR:\n\nconstant value (i.e. point support)\naggregate value (i.e. block support)\nidentity (i.e. attribute unique to a geometry)\n\nKnowing the relationships between geometries can be useful in tracking the assumptions of analyses. For example, taking the mean of an aggregate attribute such as median age, creates as assumptions of homogeneity in the aggregated areas and can contribute to the modifiable areal unit problem (MAUP).\n\n\nIntensive vs Extensive\nSpatial intensive vs extensive variables were also discussed in the context of spatial interpolation. I’m still quite unclear on how to conceptualize intensive and extensive variables. Tobias G pointed out that these terms come from physics and provided a useful non-geometry motivating example.\n\n“The price of an ingot of gold is an extensive property and its temperature would be intensive.”\n\n\n\nThe common example is that population is extensive and population density is intensive. This requires the assumption that population is constant across space. So the examples are more confusing than helpful. I have yet to come up with an example of a spatially intensive variable that makes sense.\nIf you can think of one, please comment on below!\nExtensive variables are one that are associated with the physical geometry itself. Intensive ones do not change when a geometry is modified.\nIf an ingot of gold is split into half the price changes, each piece is now worth less than the whole. But, assuming the room temperature didn’t change, the temperature of each piece remained the same.\n\n\nDomains\nThese properties of attributes are quite important but are forgotten about. One of the ideas raised in discussions was adding attribute-geometry relationship and a flag like is_intensive to a field domain.\nA Domain is a concept that I think originated at Esri. It allows you to specify the field type, range of valid values, as well as policies that determine how fields behave when they are split or merged. Field domains were added to GDAL in version 3.3.\nIs there utility in adding AGR and (ex/in)tensive flags to a field domain?\n\n\nArrow allows for embedded metadata at an array and table level. Perhaps there should be a GeoArrow table (data frame) format spec too? I’d like that. It would fit with my generic spatial data frame manifesto as well."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#geodesic-first",
    "href": "posts/2023-09-20-sdsl/index.html#geodesic-first",
    "title": "Spatial Data Science Across Languages",
    "section": "Geodesic first",
    "text": "Geodesic first\nA good amount of attention was paid to geodesic coordinate operations. The conversation was kicked off by this “motivating example.”\n\n\n\nReally, I think this was just an excuse for Edzer to poke fun at the GeoPandas devs! 😵‍💫🤣\nThe example shows an area calculation on a dataset that uses a geographic coordinate system (GCS). Area, though, is typically calculated under the assumption that coordinates are on a plane (rectangle). With GCS, the data is on a circle. So if we calculate the area of angles the result is definitely wrong.\n\n\nI think about it like calculating the area but cutting through the Earth like so: \nThe open source ecosystem is behind on support of geodetic calculations. Data more often than not is captured using GCS and users often fail to project their data. It would be nice if tools did this.\nR supports spherical geometries by using Google’s S2 library. Python is presently building out support for S2. Some operations like buffering still aren’t made available by S2.\n\nThe “full polygon”\nOne interesting point that was brought up is that in a GCS there is a concept of a full polygon. This is the polygon that covers the entire ellipsoid. There is no way to capture this using typical coordinates."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#spatial-weights-matrix-serialization",
    "href": "posts/2023-09-20-sdsl/index.html#spatial-weights-matrix-serialization",
    "title": "Spatial Data Science Across Languages",
    "section": "Spatial Weights Matrix Serialization",
    "text": "Spatial Weights Matrix Serialization\nProbably the second most interesting topic to me was around how to store spatial weights matrixes. We didn’t really discuss the “how” of holding it memory. Though I think they can be held in memory as a ragged Arrow array of indices or IDs. What was quite interesting was the concept of serializing the spatial weights matrix.\nMartin mentioned that in Pysal they had moved to serializing spatial weights as a parquet file which greatly improved their speed. In essence, spatial weights are stored in a 3 column table.\n\n\n\nSpatial Weights Matrix\n\n\ni\nj\nwij\n\n\n\n\n1\n3\n0.5\n\n\n1\n2\n0.75\n\n\n1\n9\n0.3\n\n\n2\n7\n0.2\n\n\n\nIt was noted that additional metadata can be associated at the table level or column level. This can be very handy to keep track of things like the method for identifying neighbors, the weighting mechanism used, storing flags to know if the focal feature is included and maybe even remove weights if there is a constant value.\nAdditionally, since this is parquet, it can be handled and stored by a number of databases.\nOne benefit of using arrow here, is that we can conceivably have a future where spatial weights matrices are interchangeable between spdep, pysal, geoda, and ArcGIS."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#closing",
    "href": "posts/2023-09-20-sdsl/index.html#closing",
    "title": "Spatial Data Science Across Languages",
    "section": "Closing",
    "text": "Closing\nI’m about to hop on a flight back to the US now—10 hours without internet is going to be a test of monk-like fortitude. I have left my strongest feels for another time. Chatting with devs from other languages makes is clear how great CRAN is as a package storage and testing mechanism but yet how utterly abysmal it is as a developer. I will write another post soon on the topics of retirement and how I think we can make CRAN a better place for developers."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#links",
    "href": "posts/2023-09-20-sdsl/index.html#links",
    "title": "Spatial Data Science Across Languages",
    "section": "Additional links",
    "text": "Additional links\nI’ll add more here as I can (hopefully).\n\nMartin’s SDSL blog post"
  },
  {
    "objectID": "posts/2024-01-15.html",
    "href": "posts/2024-01-15.html",
    "title": "Leptos & wasm-bindgen",
    "section": "",
    "text": "As a side project, I’m trying to build a full stack web app with auth, session management etc. I do data science and no web-dev. I am very out of my element. But no better way to learn than trial by fire.\nI’m doing all of this through the Rust leptos framework. But the challenge is that javascript is always present and I can’t really get away from it for some things and making the two interact is honestly super tricky and is probably where most of my frustration with leptos has come from.\nTo be able to call javascript from your leptos app. You need to use something called wasm-bindgen. This lets you call javascript functions from Rust.\nPart of what I’m playing with involves webR. And I have a javascript file like so:\n\n\nwebr.js\n\nimport { WebR, Console } from 'https://webr.r-wasm.org/latest/webr.mjs';\n\nconst webR = new WebR();\nawait webR.init();\n\n/* Create a webR console using the Console helper class */\nconst webRConsole = new Console();\nconsole.log(\"webR console started\");\nwebRConsole.run();\nwebRConsole.stdin(\"options('max.print' = 1000)\");\n\n// a bunch of other stuff that fetches things from the DOM\n\nexport async function resize_plot(w, h) {\n    var call = \"options(device = webr::canvas(\" + w + \", \" + h + \"))\";\n    console.log(call);\n    var res = webRConsole.stdin(call);\n}\n\nwhich will load webR at the start and exports a function to resize the plot window.\nNow in a rust file I have:\n\n\nbindings.rs\n\nuse wasm_bindgen::prelude::*;\n\n// create rust functions from the javascript functions\n#[wasm_bindgen(module = \"/webr.js\")]\nextern \"C\" {\n    pub fn resize_plot(w: f64, h: f64) -&gt; JsValue;\n}\n\nThis lets me call resize_plot() directly from my rust code which is super cool! However, because of the way that wasm-bindgen works, whatever is not contained inside of a function is execute on every single page even where it is not needed.\nBecause I have resize_plot() called in one of my leptos components it gets imported in the site-wide javascript via wasm-bindgen at /pkg/myapp.js.\nimport { resize_plot } from './snippets/myapp-aa4c1e3078dc6708/webr.js';\nOn pages where this code isn’t needed or uses errors can abound which will break client side reactivity in leptos.\nNow it’s a matter of having to figure out how to appropriately import and expose javascript functions so that no errors arise with wasm-bindgen.\nIn one case, I was able to move everything over to pure javascript which is fine. I’m unsure how I will handle others."
  },
  {
    "objectID": "posts/2024-01-27-shared-segments.html",
    "href": "posts/2024-01-27-shared-segments.html",
    "title": "Shared segment of parallel lines",
    "section": "",
    "text": "I am working on a problem where I identify approximately parallel lines. From the two lines that I have deemed parallel, I want to calculate the length of the segment that has a shared domain or range, or both domain and range.\nIn these examples I am using truly parallel lines for sake of simplicity.\nThere are four scenarios that we have to solve for: positive slope, negative slope, no slope, and undefined slopes.\nHelper functions: \n\nCode# calculate range of x values \nx_range &lt;- function(x) {\n  bbox &lt;- sf::st_bbox(x)\n  return(c(bbox$xmin, bbox$xmax))\n}\n\n# calculate range of y values \ny_range &lt;- function(x) {\n  bbox &lt;- sf::st_bbox(x)\n  return(c(bbox$ymin, bbox$ymax))\n}\n\n# calculate overlapping range between two ranges\noverlap_range &lt;- function(r1, r2) {\n  if (r1[2] &lt; r2[1] || r2[2] &lt; r1[1]) {\n    return(NA)\n  } else {\n    return(c(max(r1[1], r2[1]), min(r1[2], r2[2])))\n  }\n}\n\nfind_overlaps &lt;- function(a, b) {\n  list(\n    x_overlap = overlap_range(x_range(a), x_range(b)),\n    y_overlap = overlap_range(y_range(a), y_range(b))\n  )\n}\n\noverlap_rect &lt;- function(x) {\n  bbox &lt;- sf::st_bbox(\n    c(\n      xmin = x$x_overlap[1], \n      xmax = x$x_overlap[2],\n      ymin = x$y_overlap[1],\n      ymax = x$y_overlap[2]\n    )\n  )\n  sf::st_as_sfc(bbox)\n}"
  },
  {
    "objectID": "posts/2024-01-27-shared-segments.html#positive-slope",
    "href": "posts/2024-01-27-shared-segments.html#positive-slope",
    "title": "Shared segment of parallel lines",
    "section": "Positive Slope",
    "text": "Positive Slope\nThe first scenario is the shared positive slope.\nQuestion:\nHow do I find the coordinates of the contained line segment to calculate the length? The solution should be able to handle the scenario where x and y are flipped as well.\n\n# Positive Slope Scenario\nx &lt;- wk::wkt(\n  c(\n    \"LINESTRING(0.0 0.0, 2.0 2.0)\", # target line\n    \"LINESTRING(0.5 0.75, 2.5 2.75)\" # one we've deemed parallel\n  )\n) \nplot(x)\n\n\n\n\nWe can see that these two lines are parallel. We find their overlapping range:\n\noverlap &lt;- find_overlaps(x[1], x[2])\noverlap\n\n$x_overlap\n[1] 0.5 2.0\n\n$y_overlap\n[1] 0.75 2.00\n\n\nWhat we want to calculate is the length of the red line segment contained by the bounding box.\n\nplot(x, col = c(\"red\", \"black\"))\nplot(overlap_rect(overlap), add = TRUE)"
  },
  {
    "objectID": "posts/2024-01-27-shared-segments.html#i-need-your-help",
    "href": "posts/2024-01-27-shared-segments.html#i-need-your-help",
    "title": "Shared segment of parallel lines",
    "section": "",
    "text": "I am working on a problem where I identify approximately parallel lines. From the two lines that I have deemed parallel, I want to calculate the length of the segment that has a shared domain or range, or both domain and range.\nIn these examples I am using truly parallel lines for sake of simplicity.\nThere are four scenarios that we have to solve for: positive slope, negative slope, no slope, and undefined slopes.\nHelper functions: \n\nCode# calculate range of x values \nx_range &lt;- function(x) {\n  bbox &lt;- sf::st_bbox(x)\n  return(c(bbox$xmin, bbox$xmax))\n}\n\n# calculate range of y values \ny_range &lt;- function(x) {\n  bbox &lt;- sf::st_bbox(x)\n  return(c(bbox$ymin, bbox$ymax))\n}\n\n# calculate overlapping range between two ranges\noverlap_range &lt;- function(r1, r2) {\n  if (r1[2] &lt; r2[1] || r2[2] &lt; r1[1]) {\n    return(NA)\n  } else {\n    return(c(max(r1[1], r2[1]), min(r1[2], r2[2])))\n  }\n}\n\nfind_overlaps &lt;- function(a, b) {\n  list(\n    x_overlap = overlap_range(x_range(a), x_range(b)),\n    y_overlap = overlap_range(y_range(a), y_range(b))\n  )\n}\n\noverlap_rect &lt;- function(x) {\n  bbox &lt;- sf::st_bbox(\n    c(\n      xmin = x$x_overlap[1], \n      xmax = x$x_overlap[2],\n      ymin = x$y_overlap[1],\n      ymax = x$y_overlap[2]\n    )\n  )\n  sf::st_as_sfc(bbox)\n}"
  },
  {
    "objectID": "posts/2024-02-15-factor-finagling.html",
    "href": "posts/2024-02-15-factor-finagling.html",
    "title": "Why do we sleep on factors?",
    "section": "",
    "text": "Factors are R’s version of an enum(eration) (related post). They’re quite handy and I think we can probably rely on them a bit more for enumations like c(\"a\", \"b\", \"c\"). Today I’ve been helping test a new possible feature of extendr involving factors and it has me thinking a bit about some behaviors. Here are my extemporaneous thoughts:\nWhen we have a factor, how can we get new values and associate it with an existing factor?\nFor example, we can create a factor of the alphabet.\nf &lt;- as.factor(letters)\nf\n\n [1] a b c d e f g h i j k l m n o p q r s t u v w x y z\nLevels: a b c d e f g h i j k l m n o p q r s t u v w x y z\nSay we have new values that match the level names and want to extend the vector or create a new one based on the levels.\nIt would be nice if we could subset a factor based on the levels name\nf[\"a\"]\n\n[1] &lt;NA&gt;\nLevels: a b c d e f g h i j k l m n o p q r s t u v w x y z\nbut this gives us an NA because there is no named element \"a\". If we gave them names we could access it accordingly\nsetNames(f, letters)[\"a\"]\n\na \na \nLevels: a b c d e f g h i j k l m n o p q r s t u v w x y z\nbut this would be antithetical to the efficiency of a factor.\nTo create a new factor we have to pass in the levels accordingly:\nfactor(\"d\", levels(f))\n\n[1] d\nLevels: a b c d e f g h i j k l m n o p q r s t u v w x y z\nThis is actually pretty nice! But I feel like there could be an even better experience, though I don’t know what it would be…\nIf we wanted to extend the vector by combining the existing factor with levels names we coerce to a character vector but instead of the levels we get the integer values.\nc(f, \"a\")\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\" \"24\" \"25\" \"26\" \"a\"\nTo combine them we would need to ensure that they are both factors.\nc(f, factor(\"d\", levels(f)))\n\n [1] a b c d e f g h i j k l m n o p q r s t u v w x y z d\nLevels: a b c d e f g h i j k l m n o p q r s t u v w x y z"
  },
  {
    "objectID": "posts/2024-02-15-factor-finagling.html#using-vcrts",
    "href": "posts/2024-02-15-factor-finagling.html#using-vcrts",
    "title": "Why do we sleep on factors?",
    "section": "Using vcrts\n",
    "text": "Using vcrts\n\nUpon further thinking, vctrs tends to have the type-safe behavior that I wish from R (and aspects of it should probably be adapted into base R).\nI think vctrs gets to the behavior that I want actually. If I have a value and I use vctrs::vec_cast() and provide the existing factor vector f to the to argument, it will use the levels.\n\nvctrs::vec_cast(\"z\", f)\n\n[1] z\nLevels: a b c d e f g h i j k l m n o p q r s t u v w x y z\n\n\nBut this will not succeed if we pass it a value that is unknown. The error message is a bit cryptic and frankly feels a little pythonic in the verbosity of the traceback! But this is type safe! And I LIKE IT!\n\nvctrs::vec_cast(\"123\", f)\n\nError:\n! Can't convert from `\"123\"` &lt;character&gt; to &lt;factor&lt;754f0&gt;&gt; due to loss of generality.\n• Locations: 1"
  },
  {
    "objectID": "posts/2024-02-15-factor-finagling.html#using-vctrs",
    "href": "posts/2024-02-15-factor-finagling.html#using-vctrs",
    "title": "Why do we sleep on factors?",
    "section": "Using vctrs\n",
    "text": "Using vctrs\n\nUpon further thinking, vctrs tends to have the type-safe behavior that I wish from R (and aspects of it should probably be adapted into base R).\nI think vctrs gets to the behavior that I want actually. If I have a value and I use vctrs::vec_cast() and provide the existing factor vector f to the to argument, it will use the levels.\n\nvctrs::vec_cast(\"z\", f)\n\n[1] z\nLevels: a b c d e f g h i j k l m n o p q r s t u v w x y z\n\n\nBut this will not succeed if we pass it a value that is unknown. The error message is a bit cryptic and frankly feels a little pythonic in the verbosity of the traceback! But this is type safe! And I LIKE IT!\n\nvctrs::vec_cast(\"123\", f)\n\nError:\n! Can't convert from `\"123\"` &lt;character&gt; to &lt;factor&lt;754f0&gt;&gt; due to loss of generality.\n• Locations: 1"
  },
  {
    "objectID": "posts/2024-05-16.html",
    "href": "posts/2024-05-16.html",
    "title": "Databases for Data Scientist",
    "section": "",
    "text": "It’s been coming up a lot recently, or, maybe, I’ve just been focused on this a lot more. Data scientists are coming to terms with the fact that they have to work with databases if they want their analytics to scale. That is pretty normal. But one of the bigger challenges is that these data scientists don’t really know how to make that leap. What do they need to know to make that transition?\nFor many of use in the “know”, we know that there actually isn’t all that much different between a database and a data.frame. A data frame is in memory but a database table is just over there sitting somewhere else."
  },
  {
    "objectID": "posts/2024-05-16.html#what-do-you-need-to-know",
    "href": "posts/2024-05-16.html#what-do-you-need-to-know",
    "title": "Databases for Data Scientist",
    "section": "What do you need to know?",
    "text": "What do you need to know?\nFor those of you who want to begin to use databases in your work and want to start scaling your analysis, there are a few topics that would be helpful for you to know. I’m not going to teach you them here. But list them out so you can google it. And truthfully, you already know what these are but you don’t know the terminology.\nHere is my list of things to know:\n\nLearn what RDBMS means.\n\n\nrelational database management system or sometimes just DBMS\n\n\nUnderstand primary keys and foreign keys\nFigure out what database noramlization is and when its useful\nSchemas vs. tables for organizational purposes\nViews vs tables (this is handy for making tables to be consumed by BI / other things)\nTable indexes and what they are (that way you can know when you might need them)"
  },
  {
    "objectID": "posts/2024-05-16.html#why-you-might-not-actually-need-a-full-rdbms",
    "href": "posts/2024-05-16.html#why-you-might-not-actually-need-a-full-rdbms",
    "title": "Databases for Data Scientist",
    "section": "Why you might not actually need a full RDBMS",
    "text": "Why you might not actually need a full RDBMS\nWith the ubiquity of parquet and tools like apache arrow and DuckDB, there’s a good chance that for what you want to accomplish in your analytical workflow, you don’t need a fully fledged database. Organized parquet files into a database-like structure will be sufficient. DuckDB and Arrow can allow you to work with these data in a larger than memory capacity. You don’t need to read it all into memory, actually.\nBefore you say you need Postgres for analytics, instead, try parquet (and with hive partitioning if your data are larger) with DuckDB and Apache Arrow. It’s likely all you need."
  },
  {
    "objectID": "posts/2024-05-16-databases-for-ds.html",
    "href": "posts/2024-05-16-databases-for-ds.html",
    "title": "Databases for Data Scientist",
    "section": "",
    "text": "It’s been coming up a lot recently, or, maybe, I’ve just been focused on this a lot more. Data scientists are coming to terms with the fact that they have to work with databases if they want their analytics to scale. That is pretty normal. But one of the bigger challenges is that these data scientists don’t really know how to make that leap. What do they need to know to make that transition?\nFor many of use in the “know”, we know that there actually isn’t all that much different between a database and a data.frame. A data frame is in memory but a database table is just over there sitting somewhere else."
  },
  {
    "objectID": "posts/2024-05-16-databases-for-ds.html#what-do-you-need-to-know",
    "href": "posts/2024-05-16-databases-for-ds.html#what-do-you-need-to-know",
    "title": "Databases for Data Scientist",
    "section": "What do you need to know?",
    "text": "What do you need to know?\nFor those of you who want to begin to use databases in your work and want to start scaling your analysis, there are a few topics that would be helpful for you to know. I’m not going to teach you them here. But list them out so you can google it. And truthfully, you already know what these are but you don’t know the terminology.\nHere is my list of things to know:\n\nLearn what RDBMS means.\n\n\nrelational database management system or sometimes just DBMS\n\n\nUnderstand primary keys and foreign keys\nFigure out what database normalization is and when its useful\nSchemas vs. tables for organizational purposes\nViews vs tables (this is handy for making tables to be consumed by BI / other things)\nTable indexes and what they are (that way you can know when you might need them)"
  },
  {
    "objectID": "posts/2024-05-16-databases-for-ds.html#why-you-might-not-actually-need-a-full-rdbms",
    "href": "posts/2024-05-16-databases-for-ds.html#why-you-might-not-actually-need-a-full-rdbms",
    "title": "Databases for Data Scientist",
    "section": "Why you might not actually need a full RDBMS",
    "text": "Why you might not actually need a full RDBMS\nWith the ubiquity of parquet and tools like apache arrow and DuckDB, there’s a good chance that for what you want to accomplish in your analytical workflow, you don’t need a fully fledged database. Organized parquet files into a database-like structure will be sufficient. DuckDB and Arrow can allow you to work with these data in a larger than memory capacity. You don’t need to read it all into memory, actually.\nBefore you say you need Postgres for analytics, instead, try parquet (and with hive partitioning if your data are larger) with DuckDB and Apache Arrow. It’s likely all you need."
  },
  {
    "objectID": "posts/2024-05-17-moran-eigenvectors.html",
    "href": "posts/2024-05-17-moran-eigenvectors.html",
    "title": "Encoding spatial patterns as variables",
    "section": "",
    "text": "I’ve begun reading “Spatial modelling: a comprehensive framework for principal coordinate analysis of neighbour matrices (PCNM)” which describes the process of making “Moran Eigenvector Maps.”\nIn this case, I haven’t finished reading the paper but am quite thrilled by the prospect of it. One of the biggest problems in ecological and social science modelling is that space is often a confounder in models. By this I mean that a lot of phenomena we see are spatially dependent.\nReductively, spatial dependence means that variables or outcomes are strongly linked to where things are. For example, income tends to be spatially dependent. Meaning that high income areas are typically surounded by other high income areas."
  },
  {
    "objectID": "posts/2024-05-17-moran-eigenvectors.html#the-problem",
    "href": "posts/2024-05-17-moran-eigenvectors.html#the-problem",
    "title": "Encoding spatial patterns as variables",
    "section": "The problem",
    "text": "The problem\nWhen modelling data that exhibit spatial dependence, spatial relationships need to be accounted for. Otherwise, you will often find that model residuals (errors) also exhibit spatial dependence. So? How can you control for this.\nThere are a number of techniques that people use from more statistically sound ones, to tricks used by ML engineers. For example you may introduce the spatial lag (neighborhood average of a variable) to account for some of the spatial association."
  },
  {
    "objectID": "posts/2024-05-17-moran-eigenvectors.html#principal-coordinate-analysis",
    "href": "posts/2024-05-17-moran-eigenvectors.html#principal-coordinate-analysis",
    "title": "Encoding spatial patterns as variables",
    "section": "Principal Coordinate Analysis",
    "text": "Principal Coordinate Analysis\nOne interesting idea is using principle components analysis to encode geography into numeric variables. Conceptually, the idea is actually rather simple!\nWhen we do spatial statistics, we create what are called spatial weights matrices. These define which features are related to eachother.\nFor example we can identify the neighbors from the famous guerry dataset based on the contiguity—that is if they are touching. We create a nb and wt object. The nb are the neighbors and wt is an inverse distance weight (IDW). IDW assigns more weight to t\n\nlibrary(sfdep)\nlibrary(dplyr)\n\ngeoms &lt;- guerry$geometry\ncentroids &lt;- sf::st_centroid(geoms)\n\nnb &lt;- st_contiguity(geoms)\nwt &lt;- st_inverse_distance(nb, centroids, scale = 1e6)\n\n\nCodelibrary(ggplot2)\nsfn &lt;- st_as_graph(geoms, nb, )\n\nautoplot(sfn) +\n  geom_sf(data = geoms, fill = NA, color = \"black\", lwd = 0.2) +\n  theme_void()"
  },
  {
    "objectID": "posts/2024-05-17-moran-eigenvectors.html#principal-coordinate-analysis-pcoa",
    "href": "posts/2024-05-17-moran-eigenvectors.html#principal-coordinate-analysis-pcoa",
    "title": "Encoding spatial patterns as variables",
    "section": "Principal Coordinate Analysis (PCoA)",
    "text": "Principal Coordinate Analysis (PCoA)\nOne interesting idea is using principle components analysis to encode geography into numeric variables. Conceptually, the idea is actually rather simple!\nWhen we do spatial statistics, we create what are called spatial weights matrices. These define which features are related to eachother.\nFor example we can identify the neighbors from the famous guerry dataset based on the contiguity—that is if they are touching. We create a nb and wt object. The nb are the neighbors and wt uses a gaussian kernel. The gaussian kernel assigns more weight to to locations that are closer and less weight to those that are further—essentially following the normal distribution.\n\nlibrary(sfdep)\nlibrary(dplyr)\n\ngeoms &lt;- guerry$geometry\ncentroids &lt;- sf::st_centroid(geoms)\n\nnb &lt;- st_contiguity(geoms)\nwt &lt;- st_kernel_weights(nb, centroids, \"gaussian\")\n\nVisually, this is what the neighborhood relationship looks like:\n\nCodelibrary(ggplot2)\nsfn &lt;- st_as_graph(geoms, nb, )\n\nautoplot(sfn) +\n  geom_sf(data = geoms, fill = NA, color = \"black\", lwd = 0.2) +\n  theme_void()\n\n\n\n\nThe weights object is a ragged array which is used to be a sparse matrix representation of the spatial weights.\n\nhead(wt)\n\n[[1]]\n[1] 1.553402 1.857660 2.062100 1.676694\n\n[[2]]\n[1] 1.801787 1.717777 1.439955 1.721547 1.260566 1.429496\n\n[[3]]\n[1] 1.599532 1.527097 1.376795 1.722723 1.865664 1.350771\n\n[[4]]\n[1] 2.040754 1.356645 1.871658 1.685343\n\n[[5]]\n[1] 2.040754 1.674375 1.689488\n\n[[6]]\n[1] 2.075805 1.679763 1.357435 1.308397 2.009760 1.812262 1.432539\n\n\nThe spatial weights are an n x n square matrix. The idea behind the paper above is that we can encode the spatial relationships in this neighborhood matrix using principle components.\nWe can take the weights matrix and create a dense matrix from it:\n\nm &lt;- wt_as_matrix(nb, wt)\n\nUsing this new matrix, we can perform PCA on it.\n\npca_res &lt;- prcomp(m)\nsummary(pca_res)\n\nImportance of components:\n                           PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     0.95957 0.89585 0.85836 0.81484 0.79239 0.72065 0.66135\nProportion of Variance 0.06928 0.06038 0.05543 0.04996 0.04724 0.03907 0.03291\nCumulative Proportion  0.06928 0.12966 0.18510 0.23505 0.28229 0.32137 0.35428\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.65347 0.60391 0.58993 0.54551 0.51607 0.51048 0.50266\nProportion of Variance 0.03213 0.02744 0.02618 0.02239 0.02004 0.01961 0.01901\nCumulative Proportion  0.38640 0.41385 0.44003 0.46242 0.48246 0.50206 0.52107\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.50008 0.49651 0.48004 0.47334 0.46571 0.46447 0.45886\nProportion of Variance 0.01882 0.01855 0.01734 0.01686 0.01632 0.01623 0.01584\nCumulative Proportion  0.53989 0.55844 0.57578 0.59263 0.60895 0.62518 0.64103\n                          PC22   PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.45371 0.4495 0.43495 0.43208 0.42533 0.42265 0.40912\nProportion of Variance 0.01549 0.0152 0.01423 0.01405 0.01361 0.01344 0.01259\nCumulative Proportion  0.65651 0.6717 0.68595 0.70000 0.71361 0.72705 0.73964\n                          PC29    PC30    PC31    PC32    PC33    PC34    PC35\nStandard deviation     0.40662 0.40248 0.39657 0.38949 0.38172 0.37648 0.36612\nProportion of Variance 0.01244 0.01219 0.01183 0.01141 0.01096 0.01066 0.01009\nCumulative Proportion  0.75208 0.76427 0.77610 0.78752 0.79848 0.80915 0.81923\n                          PC36    PC37    PC38    PC39    PC40    PC41    PC42\nStandard deviation     0.35885 0.35324 0.35042 0.34655 0.33906 0.33458 0.32477\nProportion of Variance 0.00969 0.00939 0.00924 0.00904 0.00865 0.00842 0.00794\nCumulative Proportion  0.82892 0.83831 0.84755 0.85658 0.86523 0.87366 0.88159\n                          PC43    PC44    PC45    PC46    PC47    PC48    PC49\nStandard deviation     0.32182 0.30859 0.30426 0.30100 0.29700 0.28072 0.27493\nProportion of Variance 0.00779 0.00716 0.00697 0.00682 0.00664 0.00593 0.00569\nCumulative Proportion  0.88938 0.89655 0.90351 0.91033 0.91697 0.92290 0.92858\n                          PC50    PC51    PC52    PC53    PC54    PC55    PC56\nStandard deviation     0.26620 0.25927 0.25817 0.25373 0.25203 0.23148 0.22505\nProportion of Variance 0.00533 0.00506 0.00501 0.00484 0.00478 0.00403 0.00381\nCumulative Proportion  0.93391 0.93897 0.94399 0.94883 0.95361 0.95764 0.96145\n                          PC57   PC58    PC59    PC60    PC61    PC62    PC63\nStandard deviation     0.21925 0.2124 0.20738 0.20542 0.20426 0.17969 0.17415\nProportion of Variance 0.00362 0.0034 0.00324 0.00318 0.00314 0.00243 0.00228\nCumulative Proportion  0.96507 0.9685 0.97170 0.97488 0.97801 0.98044 0.98273\n                          PC64    PC65    PC66    PC67    PC68    PC69    PC70\nStandard deviation     0.17330 0.16078 0.15374 0.14641 0.14201 0.13335 0.13088\nProportion of Variance 0.00226 0.00194 0.00178 0.00161 0.00152 0.00134 0.00129\nCumulative Proportion  0.98499 0.98693 0.98871 0.99032 0.99184 0.99318 0.99447\n                          PC71    PC72    PC73    PC74    PC75    PC76    PC77\nStandard deviation     0.12526 0.10810 0.10181 0.08943 0.08425 0.07410 0.07172\nProportion of Variance 0.00118 0.00088 0.00078 0.00060 0.00053 0.00041 0.00039\nCumulative Proportion  0.99565 0.99653 0.99731 0.99791 0.99844 0.99885 0.99924\n                          PC78    PC79    PC80    PC81    PC82    PC83     PC84\nStandard deviation     0.06809 0.04608 0.03760 0.03481 0.02250 0.01105 0.008143\nProportion of Variance 0.00035 0.00016 0.00011 0.00009 0.00004 0.00001 0.000000\nCumulative Proportion  0.99959 0.99975 0.99986 0.99995 0.99999 1.00000 1.000000\n                            PC85\nStandard deviation     3.921e-16\nProportion of Variance 0.000e+00\nCumulative Proportion  1.000e+00\n\n\nThe spatial relationships that are embedded by the spatial weights matrix, are now encoded as components from a PCA. This means that we can use each of these components as a univariate measure of space. And, they also exhibit quite interesting patterns of spatial dependence."
  },
  {
    "objectID": "posts/2024-05-17-moran-eigenvectors.html#exploring-pcoa",
    "href": "posts/2024-05-17-moran-eigenvectors.html#exploring-pcoa",
    "title": "Encoding spatial patterns as variables",
    "section": "Exploring PCoA",
    "text": "Exploring PCoA\nThese components essentially capture spatial autocorrelation. For example we an look at the first component.\n\n# extract the first component\ncomp1 &lt;- pca_res$rotation[, 1]\n\nggplot(guerry, aes(fill = comp1)) +\n  geom_sf(color = \"black\", lwd = 0.2) +\n  scale_fill_viridis_c() +\n  theme_void() +\n  labs(fill = \"Eigenvector\")\n\n\n\n\nIt displays a pattern of being near Paris (the dark purple, or negative eigenvector values) or being nearer to Aveyron, the positive eigenvector values. Clearly, this displays some interesting global spatial autocorrelation. But how much?\nWe can measure the global spatial autocorrelation of this component using Moran’s I.\n\nglobal_moran_perm(comp1, nb, wt)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 500 \n\nstatistic = 1.0698, observed rank = 500, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe result is 1.0698 which is greater than the theoretical maximum of 1. There is a ridiculous amount of spatial autocorrelation here."
  },
  {
    "objectID": "posts/2024-05-17-moran-eigenvectors.html#using-pcoa-eigenvectors-to-reduce-spatial-confounding",
    "href": "posts/2024-05-17-moran-eigenvectors.html#using-pcoa-eigenvectors-to-reduce-spatial-confounding",
    "title": "Encoding spatial patterns as variables",
    "section": "Using PCoA Eigenvectors to reduce spatial confounding",
    "text": "Using PCoA Eigenvectors to reduce spatial confounding\nPredicting crime based on population and the prostitution levels of 1830s France shows that there is a lot of spatial autocorrelation in the residuals. This means that the results of the model do not appropriately account for spatial dependence.\n\nmod &lt;- lm(crime_pers ~ pop1831 + prostitutes, data = guerry)\nsummary(mod)\n\n\nCall:\nlm(formula = crime_pers ~ pop1831 + prostitutes, data = guerry)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13762.2  -4592.1   -974.6   4892.4  18672.5 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13688.932   2265.393   6.043 4.27e-08 ***\npop1831        17.676      5.881   3.006  0.00352 ** \nprostitutes    -3.197      1.665  -1.920  0.05833 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7001 on 82 degrees of freedom\nMultiple R-squared:  0.1021,    Adjusted R-squared:  0.08016 \nF-statistic:  4.66 on 2 and 82 DF,  p-value: 0.01211\n\nglobal_moran_test(resid(mod), nb, wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 5.0424, p-value = 2.298e-07\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.328493924      -0.011904762       0.004557168 \n\n\nIf you include the first eigenvector component, the spatial autocorrelation of the residuals decrease dramatically.\n\nmod &lt;- lm(crime_pers ~ pop1831 + prostitutes + comp1, data = guerry)\nsummary(mod)\n\n\nCall:\nlm(formula = crime_pers ~ pop1831 + prostitutes + comp1, data = guerry)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14228.8  -3822.7   -893.4   4232.5  19718.8 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  14847.292   2080.269   7.137 3.66e-10 ***\npop1831         15.179      5.386   2.818  0.00607 ** \nprostitutes     -4.597      1.551  -2.964  0.00399 ** \ncomp1       -28422.828   6708.123  -4.237 5.95e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6373 on 81 degrees of freedom\nMultiple R-squared:  0.265, Adjusted R-squared:  0.2378 \nF-statistic: 9.733 on 3 and 81 DF,  p-value: 1.482e-05\n\nglobal_moran_test(resid(mod), nb, wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 2.666, p-value = 0.003838\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.167439656      -0.011904762       0.004525529 \n\n\nInterestingly, this increases the \\(R^2\\) by 16 which is nothing to scoff at. The significance of prostitutes variable increases and the \\(\\beta\\) values shrink. And the first component accounts for pretty much everything else lol!\nWhat about another component?\nWe can plot the relationship that is capture by the second component.\n\n# extract the second component\ncomp2 &lt;- pca_res$rotation[, 2]\n\nggplot(guerry, aes(fill = comp2)) +\n  geom_sf(color = \"black\", lwd = 0.2) +\n  scale_fill_viridis_c() +\n  theme_void() +\n  labs(fill = \"Eigenvector\")\n\n\n\n\nThis component captures a west to east relationship rather than a north to south one. Is the second component spatially autocorrelated?\n\nglobal_moran_perm(comp2, nb, wt)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 500 \n\nstatistic = 0.99864, observed rank = 500, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nOh hell yeah it is.\nIf this component is included in the model instead of the first one we see something interesting.\n\nmod &lt;- lm(crime_pers ~ pop1831 + prostitutes + comp2, data = guerry)\nsummary(mod)\n\n\nCall:\nlm(formula = crime_pers ~ pop1831 + prostitutes + comp2, data = guerry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13617  -4584  -1150   4831  18360 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13730.388   2278.027   6.027 4.71e-08 ***\npop1831        17.518      5.919   2.960  0.00404 ** \nprostitutes    -3.098      1.686  -1.837  0.06989 .  \ncomp2        3303.053   7091.459   0.466  0.64262    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7034 on 81 degrees of freedom\nMultiple R-squared:  0.1045,    Adjusted R-squared:  0.0713 \nF-statistic:  3.15 on 3 and 81 DF,  p-value: 0.02941\n\nglobal_moran_test(resid(mod), nb, wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 5.0189, p-value = 2.598e-07\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.326953780      -0.011904762       0.004558477 \n\n\nThe model is not impacted nor is the spatial autocorrelation. So the pattern encompassed by the second component is not confounding our variables like the first one is."
  },
  {
    "objectID": "posts/2024-05-17-moran-eigenvectors.html#what-does-this-mean",
    "href": "posts/2024-05-17-moran-eigenvectors.html#what-does-this-mean",
    "title": "Encoding spatial patterns as variables",
    "section": "What does this mean?",
    "text": "What does this mean?\nIf you have spatially dependent features that you’re predicting you should consider using these as input features to your models. I have a hunch that they would work insanely well with computer vision tasks and things models like Random Forests and XGBoost."
  },
  {
    "objectID": "posts/2024-05-22-multivariate-mem.html",
    "href": "posts/2024-05-22-multivariate-mem.html",
    "title": "Univariate Spatial Dimensionality Reduction",
    "section": "",
    "text": "In discussing principal coordinate analysis (PCoA), the question naturally arose of “how could we incorporate non-spatial data into this method?” Well, that is what I wanted to explore.\nIf we include an attribute e.g. crime rate, and embed that into the spatial components generated by PCoA we would truly be doing spatial dimension reduction. Not only would we be encoding spatial patterns, but we would be encoding spatial patterns as they relate to some attribute across the spatial surface.\nThis is already explored in the context of gene expression via {spatialPCA} (website here). This is somewhat different to what I explored in my previous blog post. My brief review of the paper and R package tells me that the spatialPCA method applies the spatial weights matrix into the covariance matrix used in PCA.\nWhat I’m going to explore is a bit different than that."
  },
  {
    "objectID": "posts/2024-05-22-multivariate-mem.html#univariate-spatial-encoding",
    "href": "posts/2024-05-22-multivariate-mem.html#univariate-spatial-encoding",
    "title": "Multivariate Spatial Dimensionality Reduction",
    "section": "Univariate spatial encoding",
    "text": "Univariate spatial encoding\nNaturally, we want to look at how we can incorporate attributes into PCoA. Lets first start by creating a spatial weights matrix.\nAs always, we use the Guerry dataset since it is tried and true. Here I create a spatial weights matrix using contiguity to identify neighborhoods and we use a guassian kernel (like spatial PCA). The Gaussian kernel makes it so that locations that are closer have more weight than those further away. Since we are using contiguity, the distance is estimated by using centroids.\n\ngeom &lt;- guerry$geometry\npnts &lt;- st_centroid(geom)\n\n# neighbors via contiguity\nnb &lt;- st_contiguity(geom)\n\n# gaussian kernel weights for neighbors\nwt &lt;- st_kernel_weights(nb, pnts, \"gaussian\")\n\n(listw &lt;- nb2listw(nb, wt, style = \"B\"))\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 85 \nNumber of nonzero links: 420 \nPercentage nonzero weights: 5.813149 \nAverage number of links: 4.941176 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1       S2\nB 85 7225 698.7942 2378.909 24825.15\n\n\nRevising the spatial lag\nThe spatial lag is arguably the most fundamental spatial statistic. It is, in essence, the weighted average of a variable \\(x\\) across a neighborhood.\nTo calculate the spatial lag, referred often to as \\(Wy\\), we multiply our spatial weights matrix by our neighboring values. Let’s walk through how this works really quickly.\nWe have the index positions of our neighbors and the weights that are associated with each of them.\nhead(nb)\nhead(wt)\n\n\n\n[[1]]\n[1] 36 37 67 69\n\n[[2]]\n[1]  7 49 57 58 73 76\n\n[[3]]\n[1] 17 21 40 56 61 69\n\n[[4]]\n[1]  5 24 79 80\n\n[[5]]\n[1]  4 24 36\n\n[[6]]\n[1] 24 28 36 40 41 46 80\n\n\n[[1]]\n[1] 1.553402 1.857660 2.062100 1.676694\n\n[[2]]\n[1] 1.801787 1.717777 1.439955 1.721547 1.260566 1.429496\n\n[[3]]\n[1] 1.599532 1.527097 1.376795 1.722723 1.865664 1.350771\n\n[[4]]\n[1] 2.040754 1.356645 1.871658 1.685343\n\n[[5]]\n[1] 2.040754 1.674375 1.689488\n\n[[6]]\n[1] 2.075805 1.679763 1.357435 1.308397 2.009760 1.812262 1.432539\n\n\n\nTo calculate the spatial lag we first find the neighbors’ values for a vector x, multiply them by the weights and sum them up. In this case the variable is literacy.\n\nx &lt;- guerry$literacy\nxj &lt;- find_xj(x, nb)\nhead(xj)\n\n[[1]]\n[1] 29 73 45 32\n\n[[2]]\n[1] 67 63 45 54 54 44\n\n[[3]]\n[1] 13 23 29 20 19 32\n\n[[4]]\n[1] 69 42 23 37\n\n[[5]]\n[1] 46 42 29\n\n[[6]]\n[1] 42 40 29 29 21 27 37\n\n\nWith the neighboring values, we can multiply them by the spatial weights.\n\nxj_wt &lt;- purrr::map2(xj, wt, \\(.xj, .wt) .xj * .wt)\nhead(xj_wt)\n\n[[1]]\n[1]  45.04865 135.60918  92.79449  53.65420\n\n[[2]]\n[1] 120.71975 108.21994  64.79797  92.96353  68.07056  62.89782\n\n[[3]]\n[1] 20.79392 35.12323 39.92704 34.45446 35.44761 43.22467\n\n[[4]]\n[1] 140.81202  56.97907  43.04814  62.35767\n\n[[5]]\n[1] 93.87468 70.32374 48.99514\n\n[[6]]\n[1] 87.18379 67.19052 39.36560 37.94350 42.20495 48.93109 53.00394\n\n\nThe last step in calculating the spatial lag is to sum that all up:\n\nx_lag &lt;- vapply(xj_wt, sum, numeric(1))\nhead(x_lag)\n\n[1] 327.1065 517.6696 208.9709 303.1969 213.1936 375.8234\n\n\nOr, simplified, that is:\n\nhead(st_lag(x, nb, wt))\n\n[1] 327.1065 517.6696 208.9709 303.1969 213.1936 375.8234\n\n\nStopping short of the spatial lag\nWe use the spatial lag to get a univariate estimate of the spatial neighborhoods value. We have a matrix of xj values that we multiply the spatial weights matrix and then perform a row summation.\nWhat if we didn’t perform that summation, and instead applied PCA onto the weighted values of xj?\nWe would then be encoding space and attributes into the components! In the below code chunk I am converting the spatial weights matrix to a sparse matrix and also creating a sparse matrix of the neighboring values. I am also sacling them.\n\nm &lt;- as(listw, \"CsparseMatrix\")\n\nxj &lt;- as(\n  nb2listw(\n    nb, find_xj(scale(x), nb), style = \"B\"\n  ),\n  \"CsparseMatrix\"\n)\n\nNow, we multiply them and perform PCA on the resultant matrix:\n\nCodeplot_comp &lt;- function(comp) {\n    ggplot(guerry, aes(fill = comp)) +\n    geom_sf(color = \"black\", lwd = 0.2) +\n    scale_fill_viridis_c() +\n    theme_void() +\n    theme(legend.position = \"none\")\n}\n\n\ncomps &lt;- prcomp_irlba(xj * m, 4)\n\nplot_comp(comps$rotation[,1])\nplot_comp(comps$rotation[,2])\nplot_comp(comps$rotation[,3])\nplot_comp(comps$rotation[,4])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that there are some meaningful components in here!\nThe original variable’s distribution:\n\nggplot(guerry, aes(fill = literacy)) +\n    geom_sf(color = \"black\", lwd = 0.2) +\n    scale_fill_viridis_c() +\n    labs(title = \"Literacy\") +\n    theme_void()\n\n\n\n\nWe can see that there is some resemblance to original variable in these components.\nDo they exhibit spatial autocorrelation?\nYes, yes they do!\n\nCodecomps_autocorr &lt;- function(comps) {\n    apply(comps$rotation, 2, function(.x) {\n        broom::tidy(global_moran_test(.x, nb, wt))\n        }, simplify = FALSE) |&gt;\n        dplyr::bind_rows(.id = \"component\") |&gt;\n        dplyr::mutate(\n            estimate1 = round(estimate1, 3), p.value = rstatix::p_format(p.value)\n        ) |&gt;\n        dplyr::select(component, \"Moran's I\" = estimate1, p_val = p.value) \n}\n\n\n\ncomps_autocorr(comps)\n\n# A tibble: 4 × 3\n  component `Moran's I` p_val  \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;  \n1 PC1             0.689 &lt;0.0001\n2 PC2             0.611 &lt;0.0001\n3 PC3             0.409 &lt;0.0001\n4 PC4             0.594 &lt;0.0001\n\n\n\nlibrary(irlba)\nlibrary(sfdep)\nlibrary(spdep)\nlibrary(ggplot2)\nlibrary(spatialreg)\n\n\n\n\n# sparse matrix of the weights\nm &lt;- as(listw, \"CsparseMatrix\")\n\n# Anselin, 2019 paper uses\n# Crime persons Crime property Literacy Donations Infants Suicides for variables\nvars &lt;- c(\"crime_pers\", \"crime_prop\", \"literacy\", \"donations\", \"infants\", \"suicides\")\n\n#' @param wt is a CsparseMatrix of the weights\nxij_dist &lt;- function(x, nb, wt) {\n  # scale\n  x &lt;- scale(x)\n  # get neighbor values\n  xij &lt;- find_xj(x, nb)\n  # convert to sparse matrix\n  xj &lt;- as(\n    nb2listw(nb, xij, style = \"B\"),\n    \"CsparseMatrix\"\n  )\n  # do squared distances\n  ((xj - as.numeric(x))^2) * m\n}\n\nall_dists &lt;- lapply(vars, function(.var) {\n  xij_dist(guerry[[.var]], nb, m)\n})\n\nsquared_dists_sum &lt;- Reduce(`+`, all_dists) / length(all_dists)\n\n\ncomps &lt;- irlba::prcomp_irlba(\n  # first multiply by the spatial weights\n  # then scale & center\n  # then center again for consistency\n  # double centering ensures that we're around 0\n  scale(scale(squared_dists_sum * m, T, T), T, F),\n  10\n)\n\n# preview all of the moran's I of the components\napply(comps$rotation, 2, function(.x) {\n  broom::tidy(global_moran_test(.x, nb, wt))\n}, simplify = FALSE) |&gt;\n  dplyr::bind_rows(.id = \"component\") |&gt;\n  dplyr::mutate(estimate1 = round(estimate1, 3), p.value = rstatix::p_format(p.value)) |&gt;\n  dplyr::select(component, \"Moran's I\" = estimate1, p_val = p.value) |&gt;\n  gt::gt()\n\n\nggs &lt;- lapply(1:10, \\(i) {\n  ggplot(guerry, aes(fill = comps$rotation[, i])) +\n    geom_sf(color = \"black\", lwd = 0.2) +\n    scale_fill_viridis_c() +\n    theme_void() +\n    labs(title = paste0(\"Component \", i)) +\n    theme(legend.position = \"none\")\n})\n\nrlang::inject(patchwork::wrap_plots(!!!ggs))"
  },
  {
    "objectID": "posts/2024-05-22-multivariate-mem.html#revisiting-the-spatial-lag",
    "href": "posts/2024-05-22-multivariate-mem.html#revisiting-the-spatial-lag",
    "title": "Univariate Spatial Dimensionality Reduction",
    "section": "Revisiting the spatial lag",
    "text": "Revisiting the spatial lag\nThe spatial lag is arguably the most fundamental spatial statistic. It is, in essence, the weighted average of a variable \\(x\\) across a neighborhood.\nTo calculate the spatial lag, referred often to as \\(Wy\\), we multiply our spatial weights matrix by our neighboring values. Let’s walk through how this works really quickly.\nWe have the index positions of our neighbors and the weights that are associated with each of them.\nhead(nb)\nhead(wt)\n\n\n\n[[1]]\n[1] 36 37 67 69\n\n[[2]]\n[1]  7 49 57 58 73 76\n\n[[3]]\n[1] 17 21 40 56 61 69\n\n[[4]]\n[1]  5 24 79 80\n\n[[5]]\n[1]  4 24 36\n\n[[6]]\n[1] 24 28 36 40 41 46 80\n\n\n[[1]]\n[1] 1.553402 1.857660 2.062100 1.676694\n\n[[2]]\n[1] 1.801787 1.717777 1.439955 1.721547 1.260566 1.429496\n\n[[3]]\n[1] 1.599532 1.527097 1.376795 1.722723 1.865664 1.350771\n\n[[4]]\n[1] 2.040754 1.356645 1.871658 1.685343\n\n[[5]]\n[1] 2.040754 1.674375 1.689488\n\n[[6]]\n[1] 2.075805 1.679763 1.357435 1.308397 2.009760 1.812262 1.432539\n\n\n\nTo calculate the spatial lag we first find the neighbors’ values for a vector x, multiply them by the weights and sum them up. In this case the variable is literacy.\n\nx &lt;- guerry$literacy\nxj &lt;- find_xj(x, nb)\nhead(xj)\n\n[[1]]\n[1] 29 73 45 32\n\n[[2]]\n[1] 67 63 45 54 54 44\n\n[[3]]\n[1] 13 23 29 20 19 32\n\n[[4]]\n[1] 69 42 23 37\n\n[[5]]\n[1] 46 42 29\n\n[[6]]\n[1] 42 40 29 29 21 27 37\n\n\nWith the neighboring values, we can multiply them by the spatial weights.\n\nxj_wt &lt;- purrr::map2(xj, wt, \\(.xj, .wt) .xj * .wt)\nhead(xj_wt)\n\n[[1]]\n[1]  45.04865 135.60918  92.79449  53.65420\n\n[[2]]\n[1] 120.71975 108.21994  64.79797  92.96353  68.07056  62.89782\n\n[[3]]\n[1] 20.79392 35.12323 39.92704 34.45446 35.44761 43.22467\n\n[[4]]\n[1] 140.81202  56.97907  43.04814  62.35767\n\n[[5]]\n[1] 93.87468 70.32374 48.99514\n\n[[6]]\n[1] 87.18379 67.19052 39.36560 37.94350 42.20495 48.93109 53.00394\n\n\nThe last step in calculating the spatial lag is to sum that all up:\n\nx_lag &lt;- vapply(xj_wt, sum, numeric(1))\nhead(x_lag)\n\n[1] 327.1065 517.6696 208.9709 303.1969 213.1936 375.8234\n\n\nOr, simplified, that is:\n\nhead(st_lag(x, nb, wt))\n\n[1] 327.1065 517.6696 208.9709 303.1969 213.1936 375.8234"
  },
  {
    "objectID": "posts/2024-05-22-multivariate-mem.html#stopping-short-of-the-spatial-lag",
    "href": "posts/2024-05-22-multivariate-mem.html#stopping-short-of-the-spatial-lag",
    "title": "Univariate Spatial Dimensionality Reduction",
    "section": "Stopping short of the spatial lag",
    "text": "Stopping short of the spatial lag\nWe use the spatial lag to get a univariate estimate of the spatial neighborhoods value. We have a matrix of xj values that we multiply the spatial weights matrix and then perform a row summation.\nWhat if we didn’t perform that summation, and instead applied PCA onto the weighted values of xj?\nWe would then be encoding space and attributes into the components! In the below code chunk I am converting the spatial weights matrix to a sparse matrix and also creating a sparse matrix of the neighboring values. I am also scaling them.\n\nm &lt;- as(listw, \"CsparseMatrix\")\n\nxj &lt;- as(\n  nb2listw(\n    nb, find_xj(scale(x), nb), style = \"B\"\n  ),\n  \"CsparseMatrix\"\n)\n\nNow, we multiply them and perform PCA on the resultant matrix:\n\nCodeplot_comp &lt;- function(comp) {\n    ggplot(guerry, aes(fill = comp)) +\n    geom_sf(color = \"black\", lwd = 0.2) +\n    scale_fill_viridis_c() +\n    theme_void() +\n    theme(legend.position = \"none\")\n}\n\n\ncomps &lt;- prcomp_irlba(xj * m, 4)\n\nplot_comp(comps$rotation[,1])\nplot_comp(comps$rotation[,2])\nplot_comp(comps$rotation[,3])\nplot_comp(comps$rotation[,4])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that there are some meaningful components in here!\nThe original variable’s distribution:\n\nggplot(guerry, aes(fill = literacy)) +\n    geom_sf(color = \"black\", lwd = 0.2) +\n    scale_fill_viridis_c() +\n    labs(title = \"Literacy\") +\n    theme_void()\n\n\n\n\nWe can see that there is some resemblance to original variable in these components."
  },
  {
    "objectID": "posts/2024-05-22-multivariate-mem.html#do-they-exhibit-spatial-autocorrelation",
    "href": "posts/2024-05-22-multivariate-mem.html#do-they-exhibit-spatial-autocorrelation",
    "title": "Univariate Spatial Dimensionality Reduction",
    "section": "Do they exhibit spatial autocorrelation?",
    "text": "Do they exhibit spatial autocorrelation?\nYes, yes they do!\n\nCodecomps_autocorr &lt;- function(comps) {\n    apply(comps$rotation, 2, function(.x) {\n        broom::tidy(global_moran_test(.x, nb, wt))\n        }, simplify = FALSE) |&gt;\n        dplyr::bind_rows(.id = \"component\") |&gt;\n        dplyr::mutate(\n            estimate1 = round(estimate1, 3), p.value = rstatix::p_format(p.value)\n        ) |&gt;\n        dplyr::select(component, \"Moran's I\" = estimate1, p_val = p.value) \n}\n\n\n\ncomps_autocorr(comps)\n\n# A tibble: 4 × 3\n  component `Moran's I` p_val  \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;  \n1 PC1             0.689 &lt;0.0001\n2 PC2             0.611 &lt;0.0001\n3 PC3             0.409 &lt;0.0001\n4 PC4             0.594 &lt;0.0001\n\n\nHow do we use this, though? That is the part I am not so clear on. We have multiple components and they all exhibit significant autocorrelation. The applicability of each of these components may depend a lot upon what they are used to predict. The interpretation of them is tougher than identifying the applicability.\n\nCoderegress_component &lt;- function(z) {\n  vars &lt;- setdiff(\n    colnames(guerry), \n    c(\"code_dept\", \"count\", \"region\", \"geometry\", \"area\", \"distance\", \"ave_id_geo\", \"main_city\", \"dept\", \"department\")\n  )\n  \n  lapply(vars, \\(var) {\n    broom::glance(summary(lm(guerry[[var]] ~ z)))\n  }) |&gt; \n    setNames(vars) |&gt; \n    dplyr::bind_rows(.id = \"variable\") |&gt; \n    dplyr::arrange(-r.squared) |&gt; \n    dplyr::select(variable, r.squared) |&gt;\n    dplyr::filter(r.squared &gt; 0.1)\n}\n\n\nIf we regress our components upon the variables in the Guerry dataset we might be able to see which patterns they can help explain away. This helper function filters out regressions with an \\(R^2\\) of less than 0.1.\nregress_component(comps$rotation[,1])\nregress_component(comps$rotation[,2])\nregress_component(comps$rotation[,3])\nregress_component(comps$rotation[,4])\n\n\n\n# A tibble: 1 × 2\n  variable r.squared\n  &lt;chr&gt;        &lt;dbl&gt;\n1 literacy     0.136\n\n\n# A tibble: 5 × 2\n  variable        r.squared\n  &lt;chr&gt;               &lt;dbl&gt;\n1 donation_clergy     0.276\n2 desertion           0.193\n3 instruction         0.132\n4 literacy            0.121\n5 clergy              0.112\n\n\n\n\n# A tibble: 1 × 2\n  variable      r.squared\n  &lt;chr&gt;             &lt;dbl&gt;\n1 crime_parents     0.114\n\n\n# A tibble: 2 × 2\n  variable    r.squared\n  &lt;chr&gt;           &lt;dbl&gt;\n1 prostitutes     0.319\n2 wealth          0.189\n\n\n\nInterestingly the 3rd and 4th components are the most useful if we were looking for something to predict upon—this is a derived use case where we’re fishing for something that looks good."
  },
  {
    "objectID": "posts/2024-05-22-multivariate-mem.html#univariate-spatial-attribute-comopnent-w-regression",
    "href": "posts/2024-05-22-multivariate-mem.html#univariate-spatial-attribute-comopnent-w-regression",
    "title": "Univariate Spatial Dimensionality Reduction",
    "section": "Univariate Spatial Attribute Comopnent w/ Regression",
    "text": "Univariate Spatial Attribute Comopnent w/ Regression\nLet’s explore this 4rd component a bit more. If we regress prostitutes ~ literacy we see that there is a much weaker model. Surprisingly, in fact. And the residuals are only very mildly autocorrelated. Why is the component such a strong predictor????\n\nmod &lt;- lm(prostitutes ~ literacy, data = guerry)\nsummary(mod)\n\n\nCall:\nlm(formula = prostitutes ~ literacy, data = guerry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-415.6 -122.3  -40.5   57.9 4314.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -208.002    134.721  -1.544  0.12641   \nliteracy       8.981      3.147   2.854  0.00545 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 502.9 on 83 degrees of freedom\nMultiple R-squared:  0.08935,   Adjusted R-squared:  0.07838 \nF-statistic: 8.144 on 1 and 83 DF,  p-value: 0.005455\n\nglobal_moran_test(resid(mod), nb, wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 5.0023, p-value = 2.832e-07\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.142390596      -0.011904762       0.000951401 \n\n\nAdding in the component to the model, the \\(R^2\\) shoots right up! But why?\n\nmod &lt;- lm(prostitutes ~ literacy + comps$rotation[,4], data = guerry)\nsummary(mod)\n\n\nCall:\nlm(formula = prostitutes ~ literacy + comps$rotation[, 4], data = guerry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-766.7 -103.4  -65.6   36.5 3210.5 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -165.523    113.683  -1.456    0.149    \nliteracy               6.326      2.688   2.353    0.021 *  \ncomps$rotation[, 4] 2604.594    440.074   5.919 7.25e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 423.5 on 82 degrees of freedom\nMultiple R-squared:  0.3619,    Adjusted R-squared:  0.3464 \nF-statistic: 23.26 on 2 and 82 DF,  p-value: 9.995e-09\n\nglobal_moran_test(resid(mod), nb, wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 2.3311, p-value = 0.009874\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.100827937      -0.011904762       0.002338716 \n\n\nOften when there is a spatial effect of a variable, we utilize its spatial lag in the model. This is an SLX model but simplified.\n\nmod &lt;- lm(prostitutes ~ literacy + st_lag(x, nb, wt), data = guerry)\nsummary(mod)\n\n\nCall:\nlm(formula = prostitutes ~ literacy + st_lag(x, nb, wt), data = guerry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-500.7 -138.6  -29.5   54.1 4237.2 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)       -155.2878   149.8723  -1.036  0.30319   \nliteracy            10.7142     3.8112   2.811  0.00617 **\nst_lag(x, nb, wt)   -0.3858     0.4764  -0.810  0.42034   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 503.9 on 82 degrees of freedom\nMultiple R-squared:  0.09657,   Adjusted R-squared:  0.07454 \nF-statistic: 4.383 on 2 and 82 DF,  p-value: 0.01554\n\nglobal_moran_test(resid(mod), nb, wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 5.2839, p-value = 6.322e-08\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.167643014      -0.011904762       0.001154633 \n\n\nIs this all autocorrelation? What if we look at including just a spatial component? What is interesting is that if we recreate this process using only the Moran’s Eigenvectors, there is nothing meaningful to be extracted that predicts prostitution as well as the spatial univariate component!\nCodesp_comps &lt;- irlba::prcomp_irlba(\n  scale(scale(m, T, T), T, F), 4\n)\n\ncomps_autocorr(sp_comps)\nregress_component(sp_comps$rotation[,1])\nregress_component(sp_comps$rotation[,2])\nregress_component(sp_comps$rotation[,3])\nregress_component(sp_comps$rotation[,4])\n\n\n\n# A tibble: 4 × 3\n  component `Moran's I` p_val  \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;  \n1 PC1             1.04  &lt;0.0001\n2 PC2             0.981 &lt;0.0001\n3 PC3             0.934 &lt;0.0001\n4 PC4             0.889 &lt;0.0001\n\n\n# A tibble: 10 × 2\n   variable        r.squared\n   &lt;chr&gt;               &lt;dbl&gt;\n 1 desertion           0.366\n 2 suicides            0.258\n 3 crime_pers          0.203\n 4 literacy            0.197\n 5 crime_prop          0.178\n 6 instruction         0.171\n 7 wealth              0.158\n 8 donation_clergy     0.155\n 9 lottery             0.143\n10 commerce            0.138\n\n\n# A tibble: 4 × 2\n  variable    r.squared\n  &lt;chr&gt;           &lt;dbl&gt;\n1 literacy        0.256\n2 instruction     0.230\n3 donations       0.160\n4 commerce        0.151\n\n\n# A tibble: 2 × 2\n  variable    r.squared\n  &lt;chr&gt;           &lt;dbl&gt;\n1 instruction     0.142\n2 literacy        0.121\n\n\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: variable &lt;chr&gt;, r.squared &lt;dbl&gt;"
  },
  {
    "objectID": "posts/2024-05-22-univariate-mem.html",
    "href": "posts/2024-05-22-univariate-mem.html",
    "title": "Univariate Spatial Dimensionality Reduction",
    "section": "",
    "text": "In discussing principal coordinate analysis (PCoA), the question naturally arose of “how could we incorporate non-spatial data into this method?” Well, that is what I wanted to explore.\nIf we include an attribute e.g. crime rate, and embed that into the spatial components generated by PCoA we would truly be doing spatial dimension reduction. Not only would we be encoding spatial patterns, but we would be encoding spatial patterns as they relate to some attribute across the spatial surface.\nThis is already explored in the context of gene expression via {spatialPCA} (website here). This is somewhat different to what I explored in my previous blog post. My brief review of the paper and R package tells me that the spatialPCA method applies the spatial weights matrix into the covariance matrix used in PCA.\nWhat I’m going to explore is a bit different than that."
  },
  {
    "objectID": "posts/2024-05-22-univariate-mem.html#revisiting-the-spatial-lag",
    "href": "posts/2024-05-22-univariate-mem.html#revisiting-the-spatial-lag",
    "title": "Univariate Spatial Dimensionality Reduction",
    "section": "Revisiting the spatial lag",
    "text": "Revisiting the spatial lag\nThe spatial lag is arguably the most fundamental spatial statistic. It is, in essence, the weighted average of a variable \\(x\\) across a neighborhood.\nTo calculate the spatial lag, referred often to as \\(Wy\\), we multiply our spatial weights matrix by our neighboring values. Let’s walk through how this works really quickly.\nWe have the index positions of our neighbors and the weights that are associated with each of them.\nhead(nb)\nhead(wt)\n\n\n\n[[1]]\n[1] 36 37 67 69\n\n[[2]]\n[1]  7 49 57 58 73 76\n\n[[3]]\n[1] 17 21 40 56 61 69\n\n[[4]]\n[1]  5 24 79 80\n\n[[5]]\n[1]  4 24 36\n\n[[6]]\n[1] 24 28 36 40 41 46 80\n\n\n[[1]]\n[1] 1.553402 1.857660 2.062100 1.676694\n\n[[2]]\n[1] 1.801787 1.717777 1.439955 1.721547 1.260566 1.429496\n\n[[3]]\n[1] 1.599532 1.527097 1.376795 1.722723 1.865664 1.350771\n\n[[4]]\n[1] 2.040754 1.356645 1.871658 1.685343\n\n[[5]]\n[1] 2.040754 1.674375 1.689488\n\n[[6]]\n[1] 2.075805 1.679763 1.357435 1.308397 2.009760 1.812262 1.432539\n\n\n\nTo calculate the spatial lag we first find the neighbors’ values for a vector x, multiply them by the weights and sum them up. In this case the variable is literacy.\n\nx &lt;- guerry$literacy\nxj &lt;- find_xj(x, nb)\nhead(xj)\n\n[[1]]\n[1] 29 73 45 32\n\n[[2]]\n[1] 67 63 45 54 54 44\n\n[[3]]\n[1] 13 23 29 20 19 32\n\n[[4]]\n[1] 69 42 23 37\n\n[[5]]\n[1] 46 42 29\n\n[[6]]\n[1] 42 40 29 29 21 27 37\n\n\nWith the neighboring values, we can multiply them by the spatial weights.\n\nxj_wt &lt;- purrr::map2(xj, wt, \\(.xj, .wt) .xj * .wt)\nhead(xj_wt)\n\n[[1]]\n[1]  45.04865 135.60918  92.79449  53.65420\n\n[[2]]\n[1] 120.71975 108.21994  64.79797  92.96353  68.07056  62.89782\n\n[[3]]\n[1] 20.79392 35.12323 39.92704 34.45446 35.44761 43.22467\n\n[[4]]\n[1] 140.81202  56.97907  43.04814  62.35767\n\n[[5]]\n[1] 93.87468 70.32374 48.99514\n\n[[6]]\n[1] 87.18379 67.19052 39.36560 37.94350 42.20495 48.93109 53.00394\n\n\nThe last step in calculating the spatial lag is to sum that all up:\n\nx_lag &lt;- vapply(xj_wt, sum, numeric(1))\nhead(x_lag)\n\n[1] 327.1065 517.6696 208.9709 303.1969 213.1936 375.8234\n\n\nOr, simplified, that is:\n\nhead(st_lag(x, nb, wt))\n\n[1] 327.1065 517.6696 208.9709 303.1969 213.1936 375.8234"
  },
  {
    "objectID": "posts/2024-05-22-univariate-mem.html#stopping-short-of-the-spatial-lag",
    "href": "posts/2024-05-22-univariate-mem.html#stopping-short-of-the-spatial-lag",
    "title": "Univariate Spatial Dimensionality Reduction",
    "section": "Stopping short of the spatial lag",
    "text": "Stopping short of the spatial lag\nWe use the spatial lag to get a univariate estimate of the spatial neighborhoods value. We have a matrix of xj values that we multiply the spatial weights matrix and then perform a row summation.\nWhat if we didn’t perform that summation, and instead applied PCA onto the weighted values of xj?\nWe would then be encoding space and attributes into the components! In the below code chunk I am converting the spatial weights matrix to a sparse matrix and also creating a sparse matrix of the neighboring values. I am also scaling them.\n\nm &lt;- as(listw, \"CsparseMatrix\")\n\nxj &lt;- as(\n  nb2listw(\n    nb, find_xj(scale(x), nb), style = \"B\"\n  ),\n  \"CsparseMatrix\"\n)\n\nNow, we multiply them and perform PCA on the resultant matrix:\n\nCodeplot_comp &lt;- function(comp) {\n    ggplot(guerry, aes(fill = comp)) +\n    geom_sf(color = \"black\", lwd = 0.2) +\n    scale_fill_viridis_c() +\n    theme_void() +\n    theme(legend.position = \"none\")\n}\n\n\ncomps &lt;- prcomp_irlba(xj * m, 4)\n\nplot_comp(comps$rotation[,1])\nplot_comp(comps$rotation[,2])\nplot_comp(comps$rotation[,3])\nplot_comp(comps$rotation[,4])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that there are some meaningful components in here!\nThe original variable’s distribution:\n\nggplot(guerry, aes(fill = literacy)) +\n    geom_sf(color = \"black\", lwd = 0.2) +\n    scale_fill_viridis_c() +\n    labs(title = \"Literacy\") +\n    theme_void()\n\n\n\n\nWe can see that there is some resemblance to original variable in these components."
  },
  {
    "objectID": "posts/2024-05-22-univariate-mem.html#do-they-exhibit-spatial-autocorrelation",
    "href": "posts/2024-05-22-univariate-mem.html#do-they-exhibit-spatial-autocorrelation",
    "title": "Univariate Spatial Dimensionality Reduction",
    "section": "Do they exhibit spatial autocorrelation?",
    "text": "Do they exhibit spatial autocorrelation?\nYes, yes they do!\n\nCodecomps_autocorr &lt;- function(comps) {\n    apply(comps$rotation, 2, function(.x) {\n        broom::tidy(global_moran_test(.x, nb, wt))\n        }, simplify = FALSE) |&gt;\n        dplyr::bind_rows(.id = \"component\") |&gt;\n        dplyr::mutate(\n            estimate1 = round(estimate1, 3), p.value = rstatix::p_format(p.value)\n        ) |&gt;\n        dplyr::select(component, \"Moran's I\" = estimate1, p_val = p.value) \n}\n\n\n\ncomps_autocorr(comps)\n\n# A tibble: 4 × 3\n  component `Moran's I` p_val  \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;  \n1 PC1             0.689 &lt;0.0001\n2 PC2             0.611 &lt;0.0001\n3 PC3             0.409 &lt;0.0001\n4 PC4             0.594 &lt;0.0001\n\n\nHow do we use this, though? That is the part I am not so clear on. We have multiple components and they all exhibit significant autocorrelation. The applicability of each of these components may depend a lot upon what they are used to predict. The interpretation of them is tougher than identifying the applicability.\n\nCoderegress_component &lt;- function(z) {\n  vars &lt;- setdiff(\n    colnames(guerry), \n    c(\"code_dept\", \"count\", \"region\", \"geometry\", \"area\", \"distance\", \"ave_id_geo\", \"main_city\", \"dept\", \"department\")\n  )\n  \n  lapply(vars, \\(var) {\n    broom::glance(summary(lm(guerry[[var]] ~ z)))\n  }) |&gt; \n    setNames(vars) |&gt; \n    dplyr::bind_rows(.id = \"variable\") |&gt; \n    dplyr::arrange(-r.squared) |&gt; \n    dplyr::select(variable, r.squared) |&gt;\n    dplyr::filter(r.squared &gt; 0.1)\n}\n\n\nIf we regress our components upon the variables in the Guerry dataset we might be able to see which patterns they can help explain away. This helper function filters out regressions with an \\(R^2\\) of less than 0.1.\nregress_component(comps$rotation[,1])\nregress_component(comps$rotation[,2])\nregress_component(comps$rotation[,3])\nregress_component(comps$rotation[,4])\n\n\n\n# A tibble: 1 × 2\n  variable r.squared\n  &lt;chr&gt;        &lt;dbl&gt;\n1 literacy     0.136\n\n\n# A tibble: 5 × 2\n  variable        r.squared\n  &lt;chr&gt;               &lt;dbl&gt;\n1 donation_clergy     0.276\n2 desertion           0.193\n3 instruction         0.132\n4 literacy            0.121\n5 clergy              0.112\n\n\n\n\n# A tibble: 1 × 2\n  variable      r.squared\n  &lt;chr&gt;             &lt;dbl&gt;\n1 crime_parents     0.114\n\n\n# A tibble: 2 × 2\n  variable    r.squared\n  &lt;chr&gt;           &lt;dbl&gt;\n1 prostitutes     0.319\n2 wealth          0.189\n\n\n\nInterestingly the 3rd and 4th components are the most useful if we were looking for something to predict upon—this is a derived use case where we’re fishing for something that looks good."
  },
  {
    "objectID": "posts/2024-05-22-univariate-mem.html#univariate-spatial-attribute-comopnent-w-regression",
    "href": "posts/2024-05-22-univariate-mem.html#univariate-spatial-attribute-comopnent-w-regression",
    "title": "Univariate Spatial Dimensionality Reduction",
    "section": "Univariate Spatial Attribute Comopnent w/ Regression",
    "text": "Univariate Spatial Attribute Comopnent w/ Regression\nLet’s explore this 4rd component a bit more. If we regress prostitutes ~ literacy we see that there is a much weaker model. Surprisingly, in fact. And the residuals are only very mildly autocorrelated. Why is the component such a strong predictor????\n\nmod &lt;- lm(prostitutes ~ literacy, data = guerry)\nsummary(mod)\n\n\nCall:\nlm(formula = prostitutes ~ literacy, data = guerry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-415.6 -122.3  -40.5   57.9 4314.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -208.002    134.721  -1.544  0.12641   \nliteracy       8.981      3.147   2.854  0.00545 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 502.9 on 83 degrees of freedom\nMultiple R-squared:  0.08935,   Adjusted R-squared:  0.07838 \nF-statistic: 8.144 on 1 and 83 DF,  p-value: 0.005455\n\nglobal_moran_test(resid(mod), nb, wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 5.0023, p-value = 2.832e-07\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.142390596      -0.011904762       0.000951401 \n\n\nAdding in the component to the model, the \\(R^2\\) shoots right up! But why?\n\nmod &lt;- lm(prostitutes ~ literacy + comps$rotation[,4], data = guerry)\nsummary(mod)\n\n\nCall:\nlm(formula = prostitutes ~ literacy + comps$rotation[, 4], data = guerry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-766.7 -103.4  -65.6   36.5 3210.5 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -165.523    113.683  -1.456    0.149    \nliteracy               6.326      2.688   2.353    0.021 *  \ncomps$rotation[, 4] 2604.594    440.074   5.919 7.25e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 423.5 on 82 degrees of freedom\nMultiple R-squared:  0.3619,    Adjusted R-squared:  0.3464 \nF-statistic: 23.26 on 2 and 82 DF,  p-value: 9.995e-09\n\nglobal_moran_test(resid(mod), nb, wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 2.3311, p-value = 0.009874\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.100827939      -0.011904762       0.002338715 \n\n\nOften when there is a spatial effect of a variable, we utilize its spatial lag in the model. This is an SLX model but simplified.\n\nmod &lt;- lm(prostitutes ~ literacy + st_lag(x, nb, wt), data = guerry)\nsummary(mod)\n\n\nCall:\nlm(formula = prostitutes ~ literacy + st_lag(x, nb, wt), data = guerry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-500.7 -138.6  -29.5   54.1 4237.2 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)       -155.2878   149.8723  -1.036  0.30319   \nliteracy            10.7142     3.8112   2.811  0.00617 **\nst_lag(x, nb, wt)   -0.3858     0.4764  -0.810  0.42034   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 503.9 on 82 degrees of freedom\nMultiple R-squared:  0.09657,   Adjusted R-squared:  0.07454 \nF-statistic: 4.383 on 2 and 82 DF,  p-value: 0.01554\n\nglobal_moran_test(resid(mod), nb, wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 5.2839, p-value = 6.322e-08\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.167643014      -0.011904762       0.001154633 \n\n\nIs this all autocorrelation? What if we look at including just a spatial component? What is interesting is that if we recreate this process using only the Moran’s Eigenvectors, there is nothing meaningful to be extracted that predicts prostitution as well as the spatial univariate component!\nCodesp_comps &lt;- irlba::prcomp_irlba(\n  scale(scale(m, T, T), T, F), 4\n)\n\ncomps_autocorr(sp_comps)\nregress_component(sp_comps$rotation[,1])\nregress_component(sp_comps$rotation[,2])\nregress_component(sp_comps$rotation[,3])\nregress_component(sp_comps$rotation[,4])\n\n\n\n# A tibble: 4 × 3\n  component `Moran's I` p_val  \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;  \n1 PC1             1.04  &lt;0.0001\n2 PC2             0.981 &lt;0.0001\n3 PC3             0.934 &lt;0.0001\n4 PC4             0.889 &lt;0.0001\n\n\n# A tibble: 10 × 2\n   variable        r.squared\n   &lt;chr&gt;               &lt;dbl&gt;\n 1 desertion           0.366\n 2 suicides            0.258\n 3 crime_pers          0.203\n 4 literacy            0.197\n 5 crime_prop          0.178\n 6 instruction         0.171\n 7 wealth              0.158\n 8 donation_clergy     0.155\n 9 lottery             0.143\n10 commerce            0.138\n\n\n# A tibble: 4 × 2\n  variable    r.squared\n  &lt;chr&gt;           &lt;dbl&gt;\n1 literacy        0.256\n2 instruction     0.230\n3 donations       0.160\n4 commerce        0.151\n\n\n# A tibble: 2 × 2\n  variable    r.squared\n  &lt;chr&gt;           &lt;dbl&gt;\n1 instruction     0.142\n2 literacy        0.121\n\n\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: variable &lt;chr&gt;, r.squared &lt;dbl&gt;"
  },
  {
    "objectID": "posts/2024-05-23-spatial-ml-prediction.html",
    "href": "posts/2024-05-23-spatial-ml-prediction.html",
    "title": "Spatial ML: Predicting on out-of-sample data",
    "section": "",
    "text": "The incorporation of spatially dependent variables in a machine learning model can greatly improve the model’s performance. These features can include, but not limited to:"
  },
  {
    "objectID": "posts/2024-05-23-spatial-ml-prediction.html#deriving-spatial-features",
    "href": "posts/2024-05-23-spatial-ml-prediction.html#deriving-spatial-features",
    "title": "Spatial ML: Predicting on out-of-sample data",
    "section": "Deriving spatial features",
    "text": "Deriving spatial features\nThese kinds of spatial variables are dependent upon the features nearby them. To calculate these variable one needs to have a concept of a neighborhood.\n\nN.B. These neighborhoods are typically found using spatial indices such as R* trees for polygons and lines and KD-trees for point features. These spatial indices make it fast to look up nearby features.\n\nCodelibrary(sf)\nlibrary(spdep)\nlibrary(sfdep)\nlibrary(spatialreg)\nlibrary(ggplot2)\nnb &lt;- st_contiguity(guerry)\ngeoms &lt;- guerry$geometry\npnts &lt;- st_centroid(geoms)\nedges &lt;- st_as_edges(guerry$geometry, nb, st_weights(nb))\n\nplot(geoms[c(1, nb[[1]])], main = \"A polygon's neighborhood\")\nplot(edges[edges$from == 1, \"geometry\"], add = TRUE, col = \"blue\")\nwt &lt;- st_kernel_weights(nb, pnts, \"gaussian\")\nlistw &lt;- nb2listw(nb, wt, \"B\")\n\n# function to do PCOA\npcoa &lt;- function(listw, n_comps) {\n  m &lt;- scale(as(listw, \"CsparseMatrix\"), TRUE, FALSE)\n  irlba::prcomp_irlba(m, n_comps, center = TRUE, scale = TRUE)\n}\n\nplot_comp &lt;- function(comp) {\n  ggplot() +\n    geom_sf(aes(fill = comp), geoms, color = \"black\", lwd = 0.15) +\n    theme_void() +\n    scale_fill_viridis_c(option = \"B\") +\n    theme(legend.position = \"none\")\n}\n\ncomps &lt;- pcoa(listw, 5)\n\nplot_comp(comps$rotation[, 1]) + labs(title = \"A Spatial Component\")\n\n\n\n\n\n\n\n\n\n\nGiven an arrangement of features we derive input features from them. For example we use the neighborhood based on contiguity to calculate spatial lags. Or, we use the neighborhoods to create a spatial weights matrix to use as input into a principle coordinate analysis (PCoA) to derive embeddings of spatial relationships."
  },
  {
    "objectID": "posts/2024-05-23-spatial-ml-prediction.html#the-problem",
    "href": "posts/2024-05-23-spatial-ml-prediction.html#the-problem",
    "title": "Spatial ML: Predicting on out-of-sample data",
    "section": "The Problem",
    "text": "The Problem\nAn issue arises when we want to use these models outside of the data that we used to train the model.\nHow do we incorporate space with out-of-sample data?\nThere are three approaches we can take, I believe.\n\nUse the original spatial features to derive the spatial varaibles on the out of sample data.\nGrow the spatial index trees\nCalculate the spatial variables on the context of the new out-of-sample data.\n\nThere are issues with each of these approaches."
  },
  {
    "objectID": "posts/2024-05-23-spatial-ml-prediction.html#using-original-spatial-features-for-test-samples",
    "href": "posts/2024-05-23-spatial-ml-prediction.html#using-original-spatial-features-for-test-samples",
    "title": "Spatial ML: Predicting on out-of-sample data",
    "section": "Using Original Spatial Features for test samples",
    "text": "Using Original Spatial Features for test samples\nLet us take the example of the spatial lag. To calculate the spatial lag of a feature outside of our sample, we would need to retain the existing R* and KD trees. This could be memory intensive. The process would be somewhat like this:\n\nIdentify the neighborhood of the new feature\nCalculate the spatial lag of the neighborhood in the context of the training dataset\nUse those variables for prediction\n\nUse case:\nOne would use this approach when they believe that their training data has covered the complete spatial extent of what they intend to model.\nCons:\nThis would require storing the spatial indices that were used to create the variables along with the variables that are used. In the case of PCoA, you would need to maintain the loadings so that incoming data can be projected onto it."
  },
  {
    "objectID": "posts/2024-05-23-spatial-ml-prediction.html#growing-the-spatial-feature-space",
    "href": "posts/2024-05-23-spatial-ml-prediction.html#growing-the-spatial-feature-space",
    "title": "Spatial ML: Predicting on out-of-sample data",
    "section": "Growing the Spatial Feature Space",
    "text": "Growing the Spatial Feature Space\nAnother example would be that for the entire out-of-sample dataset we insert it into our spatial index then calculate the neighborhood for each feature. This means that each out-of-sample location can have a neighborhood that consists of features in the original training dataset or the test set.\n\nInsert out-of-sample features into spatial index\nIdentify neighborhood of out-of-sample features\nCalculate spatial lag in the context of both training and testing dataset\nUse those newly calculated spatial variables in the prediction set.\n\nImportant:\nFrom this, you can either, keep the newly inserted features in the spatial index so that they are available for later predictions or discard them after having identified your neigborhoods. If you choose the former, it makes the model mutable meaning that the spatial features generated from it would learn from each test set."
  },
  {
    "objectID": "posts/2024-05-23-spatial-ml-prediction.html#use-case-1",
    "href": "posts/2024-05-23-spatial-ml-prediction.html#use-case-1",
    "title": "Spatial ML: Predicting on out-of-sample data",
    "section": "Use case:",
    "text": "Use case:\nYou would use this approach when the original training data does not cover the complete spatial extent of what is intended to be modeled."
  },
  {
    "objectID": "posts/2024-05-23-spatial-ml-prediction.html#cons-1",
    "href": "posts/2024-05-23-spatial-ml-prediction.html#cons-1",
    "title": "Spatial ML: Predicting on out-of-sample data",
    "section": "Cons",
    "text": "Cons\nThis would require storing the original spatial indicies and variables that were used to create the spatial variables. Additionally, this would require a mutable spatial index. In the case that the out-of-sample are not retained, the spatial index must be cloned which can be memory intensive depending on the size of it."
  },
  {
    "objectID": "posts/2024-05-23-spatial-ml-prediction.html#deriving-spatial-variables-from-the-test-set",
    "href": "posts/2024-05-23-spatial-ml-prediction.html#deriving-spatial-variables-from-the-test-set",
    "title": "Spatial ML: Predicting on out-of-sample data",
    "section": "Deriving Spatial Variables from the test set",
    "text": "Deriving Spatial Variables from the test set\nThis last approach is the most straight forward. If there are spatial features that are needed for prediction, you generate them entirely from the test dataset. In the case of the spatial lag you would:\n\nCreate a new spatial index for the test set\nIdentify the neighborhood of each test feature in the test-set\nCalculate the spatial lag with these features\nUse those newly calculated spatial variables in the prediction set."
  },
  {
    "objectID": "posts/2024-05-23-spatial-ml-prediction.html#use-case-2",
    "href": "posts/2024-05-23-spatial-ml-prediction.html#use-case-2",
    "title": "Spatial ML: Predicting on out-of-sample data",
    "section": "Use case:",
    "text": "Use case:\nYou would use this approach when the model is intended to predict a specific phenomenon and is ambivalent to the spatial extent. The model is also intended to be predicted upon an entire target spatial extent. For example, the model is trained to detect housing prices in urban areas. The model might be trained on data in California but is intended to be used in urban areas in Ohio, Michigan, Massachusettes, etc."
  },
  {
    "objectID": "posts/2024-05-23-spatial-ml-prediction.html#cons-2",
    "href": "posts/2024-05-23-spatial-ml-prediction.html#cons-2",
    "title": "Spatial ML: Predicting on out-of-sample data",
    "section": "Cons",
    "text": "Cons\nCalculating a spatial index can be time-consuming. It would require a test set that covers the entire spatial extent that is intended to be predicted upon. This would not be good for small batch or individual record prediction."
  },
  {
    "objectID": "posts/2024-05-24-duckdb-and-r.html",
    "href": "posts/2024-05-24-duckdb-and-r.html",
    "title": "{duckdb} or {duckplyr}?",
    "section": "",
    "text": "I’ve been diving pretty deep into DuckDB. It has shown that it has great utility for the vast majority of mid to large scale data analysis tasks—I’m talking Gigabytes not Petabytes. In particular, Kirill Müller of Cynkra, has been doing great work in bringing DuckDB to the R community.\nToday, this takes the form of two R packages:\nI think the R community would benefit greatly by adopting DuckDB into their analytic workflows. It can used to make highly performant shiny applications or just speed up your workflow.\nFor example, here is a demo of a Shiny application filtering, plotting, and visualizing 4.5 million records very quickly!"
  },
  {
    "objectID": "posts/2024-05-24-duckdb-and-r.html#yall-keep-asking-me-duckdb-or-duckplyr",
    "href": "posts/2024-05-24-duckdb-and-r.html#yall-keep-asking-me-duckdb-or-duckplyr",
    "title": "{duckdb} or {duckplyr}?",
    "section": "Y’all keep asking me {duckdb} or {duckplyr}\n",
    "text": "Y’all keep asking me {duckdb} or {duckplyr}\n\nand before I tell you what my answer is, I’ll tell you why I’m bullish on DuckDB. I won’t ramble on details.\n\n\n\n\n\n\nJargon giraffe 🦒: bullish!\n\n\n\n\n\nBullish is a term that is associated with a growing stock market. Think of the upward motion of their horns. People who are “bullish” would spend more money in the stock market expecting its prices to continue to rise and thus make more moneyyy 💸💸💸"
  },
  {
    "objectID": "posts/2024-05-24-duckdb-and-r.html#why-duckdb",
    "href": "posts/2024-05-24-duckdb-and-r.html#why-duckdb",
    "title": "{duckdb} or {duckplyr}?",
    "section": "Why DuckDB?",
    "text": "Why DuckDB?\n\nSupports larger-than-memory workloads\nColumnar vectorized operations means operating only on the data you need to and more of it and faster!\nTight Apache Arrow integration!\nSupports Substrait for database agnostic query plans\nRuns in the browser (think ShinyLive + DuckDB means fast compute all running in the browser without a Shiny server)\n_ It is stupid fast_"
  },
  {
    "objectID": "posts/2024-05-24-duckdb-and-r.html#my-verdict",
    "href": "posts/2024-05-24-duckdb-and-r.html#my-verdict",
    "title": "{duckdb} or {duckplyr}?",
    "section": "My verdict?",
    "text": "My verdict?\nThe thing that is most important, in my opinion, for DuckDBs ability to be useful to the R community is its ability to work on data that is larger than RAM. Read this awesome blog post.\n\n\nUse duckdb!!!\n\n\n{duckplyr}\nThe R package duckplyr is a drop in replacement for dplyr. duckplyr operates only on data.frame objects and, as of today, only works with in memory data. This means it is limited to the size of your machine’s RAM.\n{duckdb}\nduckdb, on the other hand, is a {DBI} extension package. This means that you can use DBI functions to write standard SQL. But it also means that you can use use tables in your DuckDB database with dplyr (via dbplyr).\nduckdb allows you to write standard dplyr code and create lazy tables that can be combined to make even lazier code! Moreover, you can utilize the out-of-core processing capabilities with DuckDB using duckdb and, to me, that is the whole selling point.\nIf performance is your objective and you, for some reason, refuse to use the out-of-core capabilities of DuckDB, you should just use data.table via dtplyr."
  },
  {
    "objectID": "posts/2024-05-24-duckdb-and-r.html#getting-started-with-duckdb-r",
    "href": "posts/2024-05-24-duckdb-and-r.html#getting-started-with-duckdb-r",
    "title": "{duckdb} or {duckplyr}?",
    "section": "Getting started with DuckDB & R",
    "text": "Getting started with DuckDB & R\nUsing DuckDB as a database backend for dplyr is pretty much the same as anything other backend you might use. Very similar code to what I’ll show you can be used to run code on Apache Spark or Postgres.\n\n\n\n\n\n\n😭 * crying * just use postgres\n\n\n\n\n\n\n\nme, sobbing: just use postgres https://t.co/rJ4JcZJ4Zj\n\n— Jacob Matson (@matsonj) May 23, 2024\n\n|￣￣￣￣￣￣￣￣￣￣￣￣￣￣|\n    Just use Postgres    \n|＿＿＿＿＿＿＿＿＿＿＿＿＿＿|\n       \\ (•◡•) / \n        \\     /\n\n\n\nCreate a DuckDB driver\n\nLoad duckdb: library(duckdb)\n\nCreate a database driver duckdb()\n\n\n\nlibrary(duckdb)\n\nLoading required package: DBI\n\n# This uses **in memory** database which is limited by RAM\ndrv &lt;- duckdb()\n\n# this creates a persistent database which allows DuckDB to\n# perform **larger-than-memory** workloads\ndrv &lt;- duckdb(tempfile(fileext = \".duckdb\"))\ndrv\n\n&lt;duckdb_driver dbdir='/private/var/folders/wd/xq999jjj3bx2w8cpg7lkfxlm0000gn/T/Rtmpt7T0Xr/file84ac2528edf1.duckdb' read_only=FALSE bigint=numeric&gt;\n\n\n\nCreate a database connection object\n\n\ncon &lt;- dbConnect(drv)\ncon\n\n&lt;duckdb_connection 949a0 driver=&lt;duckdb_driver dbdir='/private/var/folders/wd/xq999jjj3bx2w8cpg7lkfxlm0000gn/T/Rtmpt7T0Xr/file84ac2528edf1.duckdb' read_only=FALSE bigint=numeric&gt;&gt;\n\n\n\nImport some data from somewhere\n\nHere we will download a medium sized csv and import it.\n\ntmp &lt;- tempfile(fileext = \".csv\")\ndownload.file(\n  \"https://raw.githubusercontent.com/flrsh-dev/flrsh-lessons/main/data/houses1990.csv\",\n  tmp\n)\n\nhousing &lt;- tbl_file(con, tmp)\nhousing\n\n# Source:   SQL [?? x 9]\n# Database: DuckDB v0.10.2 [root@Darwin 23.4.0:R 4.4.0//private/var/folders/wd/xq999jjj3bx2w8cpg7lkfxlm0000gn/T/Rtmpt7T0Xr/file84ac2528edf1.duckdb]\n   houseValue income houseAge rooms bedrooms population households latitude\n        &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1     452600   8.33       41   880      129        322        126     37.9\n 2     358500   8.30       21  7099     1106       2401       1138     37.9\n 3     352100   7.26       52  1467      190        496        177     37.8\n 4     341300   5.64       52  1274      235        558        219     37.8\n 5     342200   3.85       52  1627      280        565        259     37.8\n 6     269700   4.04       52   919      213        413        193     37.8\n 7     299200   3.66       52  2535      489       1094        514     37.8\n 8     241400   3.12       52  3104      687       1157        647     37.8\n 9     226700   2.08       42  2555      665       1206        595     37.8\n10     261100   3.69       52  3549      707       1551        714     37.8\n# ℹ more rows\n# ℹ 1 more variable: longitude &lt;dbl&gt;\n\n\n\nRun some dplyr code on the table\n\n\nlibrary(dplyr)\n\n\navg_price_by_age &lt;- housing |&gt;\n  group_by(houseAge) |&gt;\n  summarise(\n    avg_val = mean(houseValue)\n  )\n\nshow_query(avg_price_by_age)\n\n&lt;SQL&gt;\nSELECT houseAge, AVG(houseValue) AS avg_val\nFROM (FROM '/var/folders/wd/xq999jjj3bx2w8cpg7lkfxlm0000gn/T//Rtmpt7T0Xr/file84ac12eaa270.csv') q01\nGROUP BY houseAge\n\n\n\nBring the results into memory\n\nUse dplyr::collect() to bring the results into memory as an actual tibble!\n\navg_price_df &lt;- collect(avg_price_by_age)\navg_price_df\n\n# A tibble: 52 × 2\n   houseAge avg_val\n      &lt;dbl&gt;   &lt;dbl&gt;\n 1       17 190494.\n 2       35 207299.\n 3       37 207361.\n 4       44 216233.\n 5       30 200253.\n 6       14 189597.\n 7        5 208418.\n 8        9 186673.\n 9       21 200157.\n10       25 220414.\n# ℹ 42 more rows"
  },
  {
    "objectID": "posts/2024-05-27-indexmap-rs.html",
    "href": "posts/2024-05-27-indexmap-rs.html",
    "title": "IndexMap instead of BTreeMap",
    "section": "",
    "text": "HashMap&lt;K, V&gt; is useful when you have a value that you need to fetch frequently based on a specific key. With a hashmap, the order does not matter.\nBTreeMap&lt;K, V&gt; is a hashmap but it keeps track of the order of the keys. In a BtreeMap, order matters.\nIndexMap&lt;K,V&gt; is like a BTreeMap but the order is defined by insertion order.\n\n\n\nWhen I am programming in Rust, I often need to use either a HashMap&lt;K, V&gt; or a BTreeMap&lt;K, V&gt;. In the case of a BTreeMap, the order is based on the key values. For example if they are strings, the ordering is done alphabetically. Or if the value is numeric, it is done based on that. Or whatever other Ord trait you may have implemented or derived.\n\n\n\nIn another usecase, I want to fetch keys or values based on the order they were inserted. That is where IndexMap is helpful! IndexMap will iterate through the keys or values in the same order they were inserted.\nIn my current usecase, I am creating a lazy_static IndexMap that contains course content for flrsh.dev (pronounced flourish).\nlazy_static::lazy_static! {\n    pub static ref COURSE: IndexMap&lt;String, String&gt; = {\n        let mut m = IndexMap::new();\n        let content = serde_json::from_str::&lt;CourseContent&gt;(body::COURSE_BODY).unwrap();\n\n        content.0.into_iter().for_each(|exercise| {\n            m.insert(exercise.slug, exercise.body);\n        });\n        m\n    };\n}\nThe IndexMap is created by parsing the JSON file using serde_json.\nThe JSON file looks roughly like this:\n[\n    {\"slug\": \"String\", \"body\": \"String\"},\n    {\"slug\": \"String\", \"body\": \"String\"},\n    {\"slug\": \"String\", \"body\": \"String\"},\n]\nThis gives me a Vec&lt;CourseContent&gt; which has two fields and the slug is the key and the body is the exercise. This is great because the JSON has the content in order and I need to be able to fetch it in order.\nOrdering matters to me because I am using this IndexMap to update the navigationbar. We want to make sure that the next slug is not random!"
  },
  {
    "objectID": "posts/2024-05-27-indexmap-rs.html#btreemaps-ordering",
    "href": "posts/2024-05-27-indexmap-rs.html#btreemaps-ordering",
    "title": "IndexMap instead of BTreeMap",
    "section": "",
    "text": "When I am programming in Rust, I often need to use either a HashMap&lt;K, V&gt; or a BTreeMap&lt;K, V&gt;. In the case of a BTreeMap, the order is based on the key values. For example if they are strings, the ordering is done alphabetically. Or if the value is numeric, it is done based on that. Or whatever other Ord trait you may have implemented or derived."
  },
  {
    "objectID": "posts/2024-05-27-indexmap-rs.html#when-insertion-is-important",
    "href": "posts/2024-05-27-indexmap-rs.html#when-insertion-is-important",
    "title": "IndexMap instead of BTreeMap",
    "section": "",
    "text": "In another usecase, I want to fetch keys or values based on the order they were inserted. That is where IndexMap is helpful! IndexMap will iterate through the keys or values in the same order they were inserted.\nIn my current usecase, I am creating a lazy_static IndexMap that contains course content for flrsh.dev (pronounced flourish).\nlazy_static::lazy_static! {\n    pub static ref COURSE: IndexMap&lt;String, String&gt; = {\n        let mut m = IndexMap::new();\n        let content = serde_json::from_str::&lt;CourseContent&gt;(body::COURSE_BODY).unwrap();\n\n        content.0.into_iter().for_each(|exercise| {\n            m.insert(exercise.slug, exercise.body);\n        });\n        m\n    };\n}\nThe IndexMap is created by parsing the JSON file using serde_json.\nThe JSON file looks roughly like this:\n[\n    {\"slug\": \"String\", \"body\": \"String\"},\n    {\"slug\": \"String\", \"body\": \"String\"},\n    {\"slug\": \"String\", \"body\": \"String\"},\n]\nThis gives me a Vec&lt;CourseContent&gt; which has two fields and the slug is the key and the body is the exercise. This is great because the JSON has the content in order and I need to be able to fetch it in order.\nOrdering matters to me because I am using this IndexMap to update the navigationbar. We want to make sure that the next slug is not random!"
  },
  {
    "objectID": "posts/2024-06-06-designing-arcgisgeocode.html",
    "href": "posts/2024-06-06-designing-arcgisgeocode.html",
    "title": "Making a Ridiculously Fast™ API Client",
    "section": "",
    "text": "I recently had the pleasure of publishing the R package {arcgisgeocode}. It is an R interface to the ArcGIS World Geocoder. You could say it is the “official” Esri geocoding R package.\nTo my knowledge, it is the fastest geocoding library available in the R ecosystem. The ArcGIS World Geocoder is made avialable through {tidygeocoder} as well as {arcgeocoder}.\n{arcgisgeocode} provides the full functionality of the World Geocoder which includes bulk geocoding functionality which the other two do not. The other two packages provide an interface to the /findAddressCandidates and /reverseGeocode API endpoints. The former provides single address forward geocoding and the latter provides reverse geocoding.\n{arcgisgeocode} is ~17x faster when performing single address geocoding and ~40x faster when performing reverse geocoding when compared to the community counterparts. There are 2 primary reasons why this is.\nThe prolific Kyle Barron responded to one of my tweets a few months ago.\nThis statement is true in an aboslute sense. But then if it is only the server that is the bottle neck, why does {arcgisgeocode} out-perform two other packages calling the exact same API endpoints?\nThe reasons are primarily two-fold."
  },
  {
    "objectID": "posts/2024-06-06-designing-arcgisgeocode.html#handling-json-with-serde",
    "href": "posts/2024-06-06-designing-arcgisgeocode.html#handling-json-with-serde",
    "title": "Making a Ridiculously Fast™ API Client",
    "section": "Handling JSON with serde",
    "text": "Handling JSON with serde\nserde_json is a Rust crate that handles serialization and deserialization of Rust structs. It takes the guess work out of encoding and decoding JSON responses because it requires that we specify what the json will look like. {arcgisgeocode} uses serde_json to perform JSON serialization and deserialization.\nFor example I have the following struct definition\npub struct Address {\n    objectid: i32,\n    #[serde(rename = \"singleLine\")]\n    single_line: Option&lt;String&gt;,\n    address: Option&lt;String&gt;,\n    address2: Option&lt;String&gt;,\n    address3: Option&lt;String&gt;,\n    neighborhood: Option&lt;String&gt;,\n    city: Option&lt;String&gt;,\n    subregion: Option&lt;String&gt;,\n    region: Option&lt;String&gt;,\n    postal: Option&lt;String&gt;,\n    #[serde(rename = \"postalExt\")]\n    postal_ext: Option&lt;String&gt;,\n    #[serde(rename = \"countryCode\")]\n    country_code: Option&lt;String&gt;,\n    location: Option&lt;EsriPoint&gt;,\n}\nThese struct definitions plus serde_json all coupled with the extendr library means that I can process and create JSON extremely fast!"
  },
  {
    "objectID": "projects/pkgs/rsgeo.html",
    "href": "projects/pkgs/rsgeo.html",
    "title": "rsgeo (deprecated)",
    "section": "",
    "text": "rsgeo (deprecated)\nrsgeo is an interface to the Rust libraries geo-types and geo. geo-types implements pure rust geometry primitives. The geo library adds additional algorithm functionalities on top of geo-types. This package lets you harness the speed, safety, and memory efficiency of these libraries. geo-types does not support Z or M dimensions. There is no support for CRS at this moment.\nGitHub Repo\n\nNote that this has been deprecated in favor of development towards geoarrow-rust integration {geoarrowrs} which also uses geo under the hood for some calculations."
  },
  {
    "objectID": "projects/pkgs/spdep.html",
    "href": "projects/pkgs/spdep.html",
    "title": "spdep (contributor)",
    "section": "",
    "text": "spdep (contributor)\nspdep is an absolute powerhouse of an R package. spdep was first released in 2002 and is one of, if not the, first package to implement many of the most important spatial statistics for aerial data.\nMy contributions include:\n\nLocal Geary C (univariate and multivariate)\nLocal bivariate Moran’s I\nGlobal bivariate Moran’s I\nLocal univariate join count\nLocal bivariate join count"
  },
  {
    "objectID": "projects/pkgs/h3o.html",
    "href": "projects/pkgs/h3o.html",
    "title": "h3o for H3 indexing",
    "section": "",
    "text": "{h3o} is an R package that offers high-performance geospatial indexing using the H3 grid system. The package is built using {extendr} and provides bindings to the Rust library of the same name.\nThe Rust community built h3o which is a pure rust implementation of Uber’s H3 hierarchical hexagon grid system. Since h3o is a pure rust library it is typically safer to use, just as fast, and dependency free."
  },
  {
    "objectID": "projects/pkgs/h3o.html#benefits-of-h3o",
    "href": "projects/pkgs/h3o.html#benefits-of-h3o",
    "title": "h3o for H3 indexing",
    "section": "Benefits of h3o",
    "text": "Benefits of h3o\nSince h3o is built purely in Rust and R it is system dependency free and can be compiled for multiple platforms including Linux, MacOS, and Windows, making it easy to use across different OS.\nh3o benefits greatly from the type safety of Rust and provides robust error handling often returning 0 length vectors or NA values when appropriate where errors would typically occur using another H3 library.\nAnd moreover, it is very fast!"
  },
  {
    "objectID": "projects/pkgs/h3o.html#features",
    "href": "projects/pkgs/h3o.html#features",
    "title": "h3o for H3 indexing",
    "section": "Features",
    "text": "Features\nh3o supports all of the functionality that is provided by the C library and the Rust library h3o.\n\n\nIf there are any features missing, please make an issue on GitHub and I’ll be sure to address it!\nh3o was built with sf objects and the tidyverse in mind. h3o objects can be created from sf objects and vice versa. Compatibility with the tidyverse is accomplished via the vctrs package.\n\n\nsf::st_as_sfc() methods for H3 and H3Edge vectors\nautomatic nesting by creating lists of H3 and H3Edge vectors\n\nvectorized output will never return more objects than inputs\n\n\n\nExample\nCreate some points in the bounding box of Wyoming.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(h3o)\n\n# create a bounding box\nbbox_raw &lt;- setNames(\n  c(-111.056888, 40.994746, -104.05216, 45.005904),\n  c(\"xmin\", \"ymin\", \"xmax\", \"ymax\")\n)\n\n# create some points\npnts &lt;- st_bbox(bbox_raw) |&gt; \n  st_as_sfc() |&gt; \n  st_set_crs(4326) |&gt; \n  st_sample(25)\n\n# convert to H3 index\nhexs &lt;- h3_from_points(pnts, 4) \nhexs\n\n&lt;H3[25]&gt;\n [1] 8428967ffffffff 8426b39ffffffff 8426a63ffffffff 8426b07ffffffff\n [5] 8426a39ffffffff 8426b63ffffffff 8426b0bffffffff 8426b41ffffffff\n [9] 842799bffffffff 8426b37ffffffff 8426b69ffffffff 842686bffffffff\n[13] 8426a23ffffffff 8426b27ffffffff 8426a33ffffffff 8426b37ffffffff\n[17] 8427995ffffffff 8426ae1ffffffff 8426b41ffffffff 8426b19ffffffff\n[21] 8427987ffffffff 8426a3dffffffff 8426869ffffffff 8426a4bffffffff\n[25] 84278b5ffffffff\n\n\nThe H3 vectors can be easily visualized by converting to sf objects. The st_as_sfc() method is defined for H3 vectors. While you may be familair with st_as_sf() the _sfc variant is used for creating columns and should be used on a vector not a dataframe. This way you can use it in a dplyr pipe.\n\npolys &lt;- st_as_sfc(hexs)\npolys\n\nGeometry set for 25 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -111.3223 ymin: 40.98183 xmax: -104.0128 ymax: 45.25314\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOLYGON ((-110.4193 44.57636, -110.1531 44.7197...\n\n\nPOLYGON ((-109.2683 41.89614, -109.0075 42.0412...\n\n\nPOLYGON ((-106.3176 43.2172, -106.0491 43.35331...\n\n\nPOLYGON ((-109.6219 42.21408, -109.3608 42.3595...\n\n\nPOLYGON ((-105.2868 42.23801, -105.0199 42.3733...\n\n\nThis can be plotted.\n\nplot(polys)\n\n\n\n\nTo illustrate tidyverse compatibility lets create an sf object and create a column of H3 indexes.\n\nlibrary(dplyr, warn.conflicts = FALSE)\n\nhexs &lt;- tibble(geometry = pnts) |&gt; \n  st_as_sf() |&gt; \n  mutate(h3 = h3_from_points(geometry, 4))\n\nhexs\n\nSimple feature collection with 25 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -111.0541 ymin: 41.15866 xmax: -104.2527 ymax: 44.95915\nGeodetic CRS:  WGS 84\n# A tibble: 25 × 2\n               geometry              h3\n *          &lt;POINT [°]&gt;            &lt;H3&gt;\n 1 (-110.5547 44.90545) 8428967ffffffff\n 2 (-109.1383 42.13601) 8426b39ffffffff\n 3 (-106.4981 43.56823) 8426a63ffffffff\n 4 (-109.6964 42.39072) 8426b07ffffffff\n 5   (-105.276 42.5727) 8426a39ffffffff\n 6 (-110.1881 43.04803) 8426b63ffffffff\n 7 (-108.7864 42.90369) 8426b0bffffffff\n 8 (-109.6165 43.81228) 8426b41ffffffff\n 9 (-105.9806 44.87154) 842799bffffffff\n10  (-109.811 41.48857) 8426b37ffffffff\n# ℹ 15 more rows\n\n\nAfterwards, lets create a K = 3 disk around each grid cell, create a compact disk by compacting the cells, then unnest into a longer data frame, and update our geometries.\n\ncompact_hexs &lt;- hexs |&gt; \n  mutate(\n    disks = grid_disk(h3, 3),\n    compact_disks = purrr::map(disks, compact_cells)\n  ) |&gt; \n  tidyr::unnest_longer(compact_disks) |&gt; \n  mutate(geometry = st_as_sfc(compact_disks)) |&gt; \n  st_as_sf() \n\nUse ggplot2 to make a simple visualization.\n\nlibrary(ggplot2)\n\nggplot(compact_hexs) +\n  geom_sf(fill = NA) +\n  theme_void()"
  },
  {
    "objectID": "projects/pkgs/valve.html",
    "href": "projects/pkgs/valve.html",
    "title": "Valve",
    "section": "",
    "text": "Valve\nvalve creates multi-threaded Plumber APIs powered by Rust’s tokio and axum web frameworks. Plumber connections are automatically spawned, pooled, and terminated using deadpool. App connections are automatically pooled by hyper.\nWebsite"
  },
  {
    "objectID": "projects/pkgs/valve.html#motivation",
    "href": "projects/pkgs/valve.html#motivation",
    "title": "Valve",
    "section": "",
    "text": "Plumber is an R package that creates RESTful APIs from R functions. It is limited in that each API is a single R process and thus a single thread. Multiple queries are executed in the sequence that they came in. Scaling plumber APIs is not easy. The intention of valve is to be able to make scaling plumber APIs, and thus R itself, easier. We can make R better by leveraging Rust’s “fearless concurrency.”"
  },
  {
    "objectID": "projects/pkgs/valve.html#installation",
    "href": "projects/pkgs/valve.html#installation",
    "title": "Valve",
    "section": "",
    "text": "To install the executable only run\ncargo install valve-rs --no-default-features\n\n\n\nThere is an R package to simplify the use of Valve for those who are not familiar with Rust or CLI tools. It is available as a binary for Windows, Linux, and MacOSX thanks to R-universe.\ninstall.packages(\"valve\", repos = c(\"https://josiahparry.r-universe.dev\", \"https://cloud.r-project.org\"))\nWhen the R package is built it also includes the binary executable at inst/valve. So if you ever find yourself needing the executable system.file(\"valve\", package = \"valve\") will point you right to it! This will always be the version of the executable that your R package is using.\nYou can verify the binary works for your machine by running the below. If you have a Windows machine include system.file(\"valve.exe\", package = \"valve\") for the executable.\n# get executable path and included api paths\nvalve_executable &lt;- system.file(\"valve\", package = \"valve\")\nplumber_api_path &lt;- system.file(\"plumber.R\", package = \"valve\")\n\n# check that they exist\nfile.exists(c(valve_executable, plumber_api_path))\n\n# run Valve from the R-package's executable\nprocessx::run(\n  valve_executable,\n  args = c(\"-f\", plumber_api_path),\n  echo = TRUE\n)"
  },
  {
    "objectID": "projects/pkgs/valve.html#creating-a-valve-app",
    "href": "projects/pkgs/valve.html#creating-a-valve-app",
    "title": "Valve",
    "section": "",
    "text": "To run a plumber API concurrently using the R package, use valve_run(). The most important argument is filepath which determines which Plumber API will be executed as well as specifying the host and port to determine where your app will run. Additional configuration can be done with the n_max, workers, check_unused, and max_age argument to specify how your app will scale. By default, the app will be run on host 127.0.0.1 and on port 3000.\nlibrary(valve)\n# get included plumber API path\nplumber_api_path &lt;- system.file(\"plumber.R\", package = \"valve\")\n\nvalve_run(plumber_api_path, n_max = 5)\n#&gt; Docs hosted at &lt;http://127.0.0.1:3000/__docs__/&gt;\nUsing the cli:\nvalve -f plumber.R -n 5 \n\n\nThe arguments that you provide determines how Valve will scale up and down the application is requests come in.\n\nhost (--host):\n\ndefaults to 127.0.0.1. Defines which host the Axum app and the plumber API will be hosted on.\n\nport (--port):\n\ndefaults to 3000. Defines which port the main Axum app will be listening on.\n\nfile (--file):\n\ndefaults to plumber.R. The path to the R script that defines the plumber API.\n\nworkers (--workers):\n\ndefault 3. Determines how many workers are set in the Tokio Runtime. These workers handle incoming requests and return responses.\n\nn_max (--n-max):\n\ndefault 3. Refers to the maximum number of background Plumber APIs that can be spawned whereas workers specifies how many main worker threads are available to handle incoming requests. Generally, the number of workers should be equal to the number of plumber APIs since because plumber is single threaded. This is the default. If workers is less than n_max, you’ll never spawn the maximum number of APIs.\n\ncheck_unused (--check-unused):\n\ndefault 10. The time interval, in seconds, to check for unused connections.\n\nmax_age (--max-age):\n\ndefault 300 (five minutes). Specifies how long a connection can go unused without being terminated. If a connection reaches this age it will be terminated in the next pool check (interval determined by check_unused).\n\nn_min:\n\ndefault 1. The minimum number of connections to keep active at all times. Connections will not be pruned if there are this many connections open."
  },
  {
    "objectID": "projects/pkgs/valve.html#example-calling-valve-with-multiple-workers",
    "href": "projects/pkgs/valve.html#example-calling-valve-with-multiple-workers",
    "title": "Valve",
    "section": "",
    "text": "The way valve works is by accepting requests on a main port (3000 by default) and then distributing the requests to a plumber API connection pool. Requests are captured by axum and proxied to a plumber API process.\nYou can run the example plumber API included with Valve in the background in R using this code chunk:\n# create temp file\ntmp &lt;- tempfile(fileext = \".R\")\n\n# create script lines\nvalve_script &lt;- '\nplumber_api_path &lt;- system.file(\"plumber.R\", package = \"valve\")\nvalve::valve_run(plumber_api_path, workers = 5)\n'\n# write to temp\nwriteLines(valve_script, tmp)\n\n# run in the background\nrstudioapi::jobRunScript(tmp)\nOr launch it directly from the terminal via:\nvalve -f $(Rscript -e 'cat(system.file(\"plumber.R\", package = \"valve\"))')\nOnce the Valve app is running in the background we can begin the example. First I’m going to define a function to call the /sleep endpoint. The function will take two parameters: the port and the duration of sleep. The port will be used to change between the valve app and a single plumber API.\nsleep &lt;- function(port, secs) {\n  httr2::request(\n        paste0(\"127.0.0.1:\", port, \"/sleep?zzz=\", secs)\n    ) |&gt; \n    httr2::req_perform() |&gt; \n    httr2::resp_body_string()\n}\nUsing this function we’ll create 5 total R sessions each will make a request to sleep for 2 seconds.\nlibrary(furrr)\nplan(multisession, workers = 5)\nFirst, we’ll ping the main valve app which will distribute requests. The first time this is ran might be slow since there will not be any plumber APIs in the pool yet.\nstart &lt;- Sys.time()\nmulti_sleep &lt;- future_map(1:5, ~ sleep(3000, 2))\nmulti_total &lt;- Sys.time() - start\nNext, we select only one of the available plumber APIs and query it.\nstart &lt;- Sys.time()\nsingle_sleep &lt;- furrr::future_map(1:5, ~ sleep(53869, 2))\nsingle_total &lt;- Sys.time() - start\nNotice the performance difference.\nprint(paste0(\"Multiple Plumber APIs: \", round(multi_total, 2)))\n#&gt; [1] \"Multiple Plumber APIs: 2.63\"\nprint(paste0(\"One Plumber API: \", round(single_total, 2)))\n#&gt; [1] \"One Plumber API: 10.08\"\nIn the former each worker gets to make the request in approximately the same amount of time. The latter has to wait for each subsequent step to finish before the next one can occur. So we’ve effectively distributed the work load."
  },
  {
    "objectID": "projects/pkgs/valve.html#how-valve-works",
    "href": "projects/pkgs/valve.html#how-valve-works",
    "title": "Valve",
    "section": "",
    "text": "The architecture, at a high level, is captured by this diagram.\n\nThere are really three key components to this:\n\nthe Tokio Runtime,\nthe Axum Router,\nand the connection Pool.\n\n\n\nThe tokio Runtime is what allows Valve to be asynchronous. It handles I/O, tasks, and all that jazz. It is also what backs Axum. In Valve, we define an asynchronous runtime with a pre-defined number of workers. These workers are what handle the incoming requests.\nWhen a request is picked up, it is then sent to the Axum Router. The router takes the incoming requests and sends them to the appropriate endpoint.\nThe routes that are defined are / and /*key. / is a permanent redirect to the plumber API documentation. Whereas /*key captures every other request. These requests have a special handler that, in short, act as a reverse proxy between Axum and a plumber API. The handler captures the request and grabs a Plumber connection from the Pool. The Plumber struct contains the host and port that the APIs live on. The request is then parsed, and redirected to the plumber API. The response is captured and returned as a response to the Axum router.\n\n\n\nValve implements a custom managed Pool for plumber APIs. The pool consists of Plumber struct which contain the host, port, and the child process.\nWhen Deadpool spawns a new connection for the pool, it thus spawns a new plumber API. This is done using Command::new() to create a detached child process. A random port is generated, checked, and then assigned to the plumber API. Then the process is started by calling R -e \"plumber::plumb('{filepath}')$run(host = '{host}', port = {port})\" via Command. This means that R must be on the path and that if there are multiple installs of R, whichever one is on the path will be used.\nTo prevent plumber APIs being spawned too frequently they are kept alive for duration defined by max_age. A connection can be unused for that duration. If it exceeds that age without being used, Deadpool will prune the connection and terminate the process. This check happens on a separate thread occurring every check_unused seconds."
  },
  {
    "objectID": "projects/pkgs/valve.html#benchmarks-with-drill",
    "href": "projects/pkgs/valve.html#benchmarks-with-drill",
    "title": "Valve",
    "section": "",
    "text": "Simple benchmarks using drill can be found in inst/bench-sleep-plumber.yml and bench-sleep-valve.yml.\nThe bench mark calls the /sleep endpoint and sleeps for 500ms for 100 times with 5 concurrent threads. This alone can illustrate how much we can speed up a single plumber API’s response time with valve.\nPlumber’s benchmark:\nTime taken for tests      50.7 seconds\nTotal requests            100\nSuccessful requests       100\nFailed requests           0\nRequests per second       1.97 [#/sec]\nMedian time per request   2540ms\nAverage time per request  2482ms\nSample standard deviation 272ms\n99.0'th percentile        2556ms\n99.5'th percentile        2556ms\n99.9'th percentile        2556ms\nValve’s benchmark:\nTime taken for tests      10.2 seconds\nTotal requests            100\nSuccessful requests       100\nFailed requests           0\nRequests per second       9.78 [#/sec]\nMedian time per request   510ms\nAverage time per request  510ms\nSample standard deviation 2ms\n99.0'th percentile        516ms\n99.5'th percentile        518ms\n99.9'th percentile        518ms\n\n\nvalve is best suited for light to medium sized work loads. Each background plumber API will hold their own copy of their R objects. So if you are serving a machine learning model that is a GB big, that model will have to be copied into each thread and that can be quickly bloat up your ram. So be smart! If you have massive objects in your R session, try and reduce the clutter and thin it out."
  },
  {
    "objectID": "projects/pkgs/sfdep.html",
    "href": "projects/pkgs/sfdep.html",
    "title": "sfdep",
    "section": "",
    "text": "sfdep is an R package that acts as a tidy interface to the spdep package. In addition it provides some statistics that are not available elsewhere in the R ecosystem such as the neighbor match test.\n\n\n\nEmerging Hot Spot Analysis\nColocation Quotients\nsfnetworks integrations\nLocal Neighbor Match Test\n\n\n\n\n\n GitHub\nRStudio Conf 2022L\nNew York Open Statistical Programming Meetup"
  },
  {
    "objectID": "projects/pkgs/sfdep.html#package-highlights",
    "href": "projects/pkgs/sfdep.html#package-highlights",
    "title": "sfdep",
    "section": "",
    "text": "Emerging Hot Spot Analysis\nColocation Quotients\nsfnetworks integrations\nLocal Neighbor Match Test"
  },
  {
    "objectID": "projects/pkgs/sfdep.html#resources",
    "href": "projects/pkgs/sfdep.html#resources",
    "title": "sfdep",
    "section": "",
    "text": "GitHub\nRStudio Conf 2022L\nNew York Open Statistical Programming Meetup"
  },
  {
    "objectID": "projects/pkgs/r-arcgis-bridge.html",
    "href": "projects/pkgs/r-arcgis-bridge.html",
    "title": "R-ArcGIS Bridge",
    "section": "",
    "text": "R-ArcGIS Bridge\nThe R-ArcGIS Bridge is a suite of R packages designed to integrate the ArcGIS into the R ecosystem. I have developed a (growing) number of R packages to integrate ArcGIS location and data services to the R community. They are written in Rust and in R.\nSee the R-ArcGIS Bridge website for more detailed documentation.\nI have built\n\n{arcgisutils}: provides authentication, custom json serialization (written in Rust) and parsing, and other developer oriented utilities.\n{arcgislayers}: read and write data in ArcGIS data services such as ArcGIS Online, Enterprise, Platform, and Hub.\n{arcgisgeocode}: Rust based R package that interfaces to the ArcGIS World Geocoding service. Possibly the fastest geocoding client in the R ecosystem.\n{arcgisplaces}: Rust based R package built upon serde_esri to interact with the ArcGIS Places Service\n{arcpbf}: Rust based R package to process protocol buffers from ArcGIS Feature Services. This results in a dramatically faster parsing time than processing JSON.\n{calcite}: Shiny extension package providing bindings to the Calcite Design System React components."
  },
  {
    "objectID": "projects.html#rust-libraries",
    "href": "projects.html#rust-libraries",
    "title": "projects",
    "section": "",
    "text": "rust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\npackage\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npackage\n\n\nmusic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nspatial\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npackage\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npackage\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwriting\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwriting\n\n\nurban-informatics\n\n\ntextbook\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwriting\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "projects"
    ]
  },
  {
    "objectID": "projects/rust/valve.html",
    "href": "projects/rust/valve.html",
    "title": "Valve",
    "section": "",
    "text": "Valve\nvalve creates multi-threaded Plumber APIs powered by Rust’s tokio and axum web frameworks. Plumber connections are automatically spawned, pooled, and terminated using deadpool. App connections are automatically pooled by hyper.\nWebsite"
  },
  {
    "objectID": "projects/rust/sfconversions.html",
    "href": "projects/rust/sfconversions.html",
    "title": "sfconversions",
    "section": "",
    "text": "sfconversions\nA minimal Rust library to convert geometry objects from the R package {sf} into geo-types geometry primitives using extendr.\nProvides simple conversions between sfg, sfc objects from sf, and geometry primitives from geo_types that can be used with other georust libraries powered by extendr.\nDue to the orphan rule conversion directly from extendr Lists to geo_types is not possible. For that reason a simple struct Geom is implemented with a single field geom which contains a geo_types Geometry enum.\nsfconversions"
  },
  {
    "objectID": "projects/rust/arrow-extendr.html",
    "href": "projects/rust/arrow-extendr.html",
    "title": "arrow_extendr",
    "section": "",
    "text": "arrow_extendr\narrow-extendr is a crate that facilitates the transfer of Apache Arrow memory between R and Rust. It utilizes extendr, the {nanoarrow} R package, and arrow-rs.\narrow_extendr"
  },
  {
    "objectID": "projects/rust/serde-esri.html",
    "href": "projects/rust/serde-esri.html",
    "title": "serde_esri",
    "section": "",
    "text": "serde_esri\nserde_esri provides struct definitions to represent Esri JSON objects with serde::Deserialize and serde::Serialize trait implementations.\nserde_esri has additional features:\n\ngeo implements From for the Esri JSON objects.\ngeoarrow provides compatibility with arrow and geoarrow by implementing geoarrow geometry traits as well as providing a utility function featureset_to_geoarrow() which converts a FeatureSet to an arrow GeoTable.\nplaces-client provides an API client for the Places Service REST API.\n\nbuilt ontop of reqwest"
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "Josiah Parry",
    "section": "posts",
    "text": "posts\n\n\n\n\n\nBuilding local packages for WebR\n\n\n\n\n\n\nwebr\n\n\nr\n\n\nwasm\n\n\n\n\n\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\nCreate and edit TOML in R with {tomledit}\n\n\n\n\n\n\nr\n\n\ntoml\n\n\nrust\n\n\nextendr\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n\n\n\n\n\n\n🎄dvent Day 1: Rust and R solutions\n\n\n\n\n\n\nr\n\n\nrust\n\n\naoc\n\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\n\n\n\n\n\nImplementing OpenID Connect (OIDC) in R\n\n\n\n\n\n\nr\n\n\nhttr2\n\n\nauth\n\n\noidc\n\n\n\n\n\n\n\n\n\nNov 28, 2024\n\n\n\n\n\n\n\nS7 & Options objects\n\n\nreimagining readr::read_csv()\n\n\n\npkg-dev\n\n\nr\n\n\ns7\n\n\n\n\n\n\n\n\n\nNov 21, 2024\n\n\n\n\n\n\n\nAdd syntax highlighting to leptos\n\n\n\n\n\n\nrust\n\n\nleptos\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\n\n\n\n\n\nGet notified of failing CRAN checks\n\n\nA GitHub Action for package developers\n\n\n\ncran\n\n\nr\n\n\npkg-dev\n\n\n\n\n\n\n\n\n\nSep 1, 2024\n\n\n\n\n\n\n\nType safe(r) R code\n\n\nRobust type checking with r-lib\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\nRead a CSV in a production API\n\n\n{plumber} and multipart request #RinProd\n\n\n\nplumber\n\n\nr\n\n\nprod\n\n\n\n\n\n\n\n\n\nJun 25, 2024\n\n\n\n\n\n\n\nStatic file server in R\n\n\n{plumber} is (always) the answer\n\n\n\nr\n\n\nplumber\n\n\napi\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\n\n\n\n\n\nCaching WebR from CDN\n\n\nNotes from developing webr-js-rs\n\n\n\nwebr\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\n\n\n\n\n\nEvaluate strings as code\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\n\n\n\n\n\nDocker: keep your secrets secret\n\n\n\n\n\n\nproduction\n\n\ndocker\n\n\n\n\n\n\n\n\n\nJun 11, 2024\n\n\n\n\n\n\n\nMaking a Ridiculously Fast™ API Client\n\n\nDesign choices for a highly performant R package\n\n\n\n\n\n\n\n\nJun 6, 2024\n\n\n\n\n\n\n\nIndexMap instead of BTreeMap\n\n\n\n\n\n\nrust\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\n\n\n\n\n\n{duckdb} or {duckplyr}?\n\n\nDuckDB and the R ecosystem\n\n\n\nr\n\n\nprod\n\n\nduckdb\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\n\n\n\n\n\nSpatial ML: Predicting on out-of-sample data\n\n\n3 approaches to using spatially derived features\n\n\n\nspatial\n\n\nml\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\n\n\n\n\n\nUnivariate Spatial Dimensionality Reduction\n\n\nExtening PCoA and Moran Eigenvector Maps to include attributes\n\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\n\n\n\n\n\nEncoding spatial patterns as variables\n\n\nPrincipal Coordinate Analysis & Moran Eigenvectors\n\n\n\nspatial\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\n\n\n\n\n\nDatabases for Data Scientist\n\n\nAnd why you probably dont need one\n\n\n\nproduction\n\n\narrow\n\n\nduckdb\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\n\n\n\n\n\nWhy do we sleep on factors?\n\n\nAnd how I wish things may behave?\n\n\n\nr\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\nShared segment of parallel lines\n\n\n\n\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\n\n\n\n\n\nLeptos & wasm-bindgen\n\n\nnote to self: its tricksy\n\n\n\nrust\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\n\n\n\n\n\nWriting S3 head() methods\n\n\na note to self for later\n\n\n\npkg-dev\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n\n\n\n\n\nBuilding a DataFusion CSV reader with arrow-extendr\n\n\nextending R with Arrow and Rust\n\n\n\nrust\n\n\npkg-dev\n\n\nextendr\n\n\narrow\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\n\n\n\n\n\nEnums in R: towards type safe R\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\n\n\n\n\n\nExport Python functions in R packages\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\nWhere am I in the sky?\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\nSpatial Data Science Across Languages\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\nValve: putting R in production\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\nR is still fast: a salty reaction to a salty blog post\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\nWhat’s so special about arrays?\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\n\n\n\n\n\nFeeling rusty: counting characters\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\n\n\n\n\n\nRust traits for R users\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\n\n\n\n\n\nlearning rust\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\nJHU talk (slides)\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\nRaw strings in R\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\nProgramatically Create Formulas in R\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\nYouTube Videos & what not\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\nFishnets and overlapping polygons\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\nComplete spatial randomness\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n\nspacetime representations aren’t good—yet\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\nMake your R scripts Databricks notebooks\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\n\n\n\n\n\nExploratory Spatial Data Analysis in R\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\n\n\n\n\n\nMy new IDE theme: xxEmoCandyLandxx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\n\n\n\n\n\nActually identifying R package System Requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\nInstalling Python on my M1 in under 10 minutes\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\nThe heck is a statistical moment??\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\nSLICED! a brief reflection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n{cpcinema} & associated journey\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\n\n\n\n\n\nOSINT in 7 minutes\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\n\n\n\n\n\nAPIs: the language agnostic love story\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\nPython & R in production — the API way\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\nColor Palette Cinema\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\n\n\n\n\n\nSecure R Package Environments\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\n\n\n\n\n\nWhat is critical race theory, anyways?\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n\nDemographic Change, White Fear, and Social Construction of Race\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\n\n\n\n\n\nMedium Data and Production API Pipeline\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\n\n\n\n\n\nThe Red Queen Effect\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n\nExcel in pRod\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\nDesign Paradigms in R\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\n\n\n\n\n\nR Security Concerns and 2019 CRAN downloads\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\n\n\n\n\n\nNon-interactive user tokens with googlesheets4\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n\nFinding an SPSS {haven}\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\n\n\n\n\n\nIntro to Tidy Modeling\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\nWater Quality Analysis\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\n\n\n\n\n\n∑ { my parts }\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\n\n\n\n\n\nWeb-scraping for Campaigns\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\nGoogle Trends for Campaigns\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\nIntroducing trendyy\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\n\n\n\n\n\ngenius tutorial\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n\ngenius Plumber API\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\n\n\n\n\n\nThe Fallacy of one person, one vote\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\nThe Cost of Gridlock\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\n\n\n\n\n\nxgboost feature importance\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\n\n\n\n\n\n[Not so] generic functions\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\n\n\n\n\n\nUS Representation: Part I\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\n\n\n\n\n\nIntroducing: Letters to a layperson\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\n\n\n\n\n\nChunking your csv\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\nReading Multiple csvs as 1 data frame\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\nCoursera R-Programming: Week 2 Problems\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\n\n\n\n\n\nIntroducing geniusR\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Want my help?",
    "section": "",
    "text": "Whether you need help with R package development, Shiny app or plumber API deployment, or making your analytics scale better, I’m here to lend a hand.\nI’ve got a small block of time each week that I can dedicate to consulting. It’s not a lot, but it’s enough to make a big impact on your project.\n\nWriting performant R packages for data analysis\nPlanning deployments of Shiny apps and Plumber APIs\nScaling analytics code for large datasets and complex calculations\nDeveloping bespoke training programs tailored to your team’s needs\nTraining developers on Rust and extending R packages with Rust\n\nHave something different in mind? Email me at josiah.parry@gmail.com.",
    "crumbs": [
      "*want my help?*"
    ]
  },
  {
    "objectID": "posts/2024-06-11-docker-env-vars/index.html",
    "href": "posts/2024-06-11-docker-env-vars/index.html",
    "title": "Docker: keep your secrets secret",
    "section": "",
    "text": "You’ve written a shiny app, plumber API, or an ETL process. Your orchestrating that work with Docker. In order for the application to work, you need to be able to use secret values."
  },
  {
    "objectID": "posts/2024-06-11-docker-env-vars/index.html#example-dockerfile",
    "href": "posts/2024-06-11-docker-env-vars/index.html#example-dockerfile",
    "title": "Docker: keep your secrets secret",
    "section": "Example Dockerfile",
    "text": "Example Dockerfile\nHere is a very simple Dockerfile. Say the container is called supersecret. When we run the Docker container we print a single environment variable.\nFROM rhub/r-minimal\n\nCMD [ \"R\", \"--slave\", \"-e\", \"cat(Sys.getenv('SECRET_USER'))\"]\nRun this with docker run --rm -t supersecret and you’ll see nothing printed to the console. This is because the environment variable is not actually available to the container.\nHow can you set the environment variables used by a container?"
  },
  {
    "objectID": "posts/2024-06-11-docker-env-vars/index.html#the-env-instruction",
    "href": "posts/2024-06-11-docker-env-vars/index.html#the-env-instruction",
    "title": "Docker: keep your secrets secret",
    "section": "The ENV instruction",
    "text": "The ENV instruction\nThe ENV Docker instruction is used to specify environment variables You can specify environment variables directly into the Dockerfile like so:\nFROM rhub/r-minimal\nENV SECRET_USER=josiah\n\nCMD [ \"R\", \"--slave\", \"-e\", \"cat(Sys.getenv('SECRET_USER'))\"]\nRunning docker run --rm -t supersecret will print josiah to the console! So that worked.\nThis is fine for things that dont need to be secret. For example maybe you have something like ENV DEBUG=true to specify that this is a debug build.\nBut if you have a secret, you shouldn’t place your secrets directly in the code of the Dockerfile."
  },
  {
    "objectID": "posts/2024-06-11-docker-env-vars/index.html#using---env",
    "href": "posts/2024-06-11-docker-env-vars/index.html#using---env",
    "title": "Docker: keep your secrets secret",
    "section": "Using --env",
    "text": "Using --env\nAnother way to specify environment variables is to specify the environment variables at run time using the --env flag. This accepts key-value pairs for the environment variables.\nFor example\ndocker run --env SECRET_USER=\"ricky bobby\" --rm -t supersecret\nwill print ricky bobby to the console.\nThis will work but it requires that you manually specify the environment variables at run time when using docker run. And that can be cumbersome and require some finagling.\nAnd again, you dont want to write a bash script that hard codes those values into a docker run call.\nSo what else can you do?"
  },
  {
    "objectID": "posts/2024-06-11-docker-env-vars/index.html#using-a-separate-file-with---env-file",
    "href": "posts/2024-06-11-docker-env-vars/index.html#using-a-separate-file-with---env-file",
    "title": "Docker: keep your secrets secret",
    "section": "Using a separate file with --env-file",
    "text": "Using a separate file with --env-file\nYou shouldn’t store secrets in your R code. You should use a .Renviron file. This looks like\nKEY=value\nSECRET_USER=josiah\nSECRET_USER_PASSWORD=super-duper-very-secret\n\n\n\n\n\n\nTip\n\n\n\nIn many other languages and ecosystem, using a .env file with the same structure is used to set environment variables.\n\n\nThis would make the environment variables KEY, SECRET_USER and SECRET_USER_PASSWORD available to your R session by running Sys.getenv().\nNow, you don’t want to actually copy this file into the docker container. What if you accidentally made the file available? Yikes!\nInstead, you can pass the file directly using the --env-file flag. This will capture the environment variables written in a file as a KEY=value pair and make them available in your container.\n\ndocker run with file\nGiven the following files which define a Docker image called supersecret\n\n\n\n\n\n\n\n\n.env\n\nSECRET_USER=shhhh-dont-tell\n\n\n\n\n\nDockerfile\n\nFROM rhub/r-minimal\n\nCMD [ \"R\", \"--slave\", \"-e\", \"cat(Sys.getenv('SECRET_USER'))\"]\n\n\n\n\nYou will need to run docker run --env-file .env supersecret to set your environment variables appropriately."
  },
  {
    "objectID": "posts/2024-06-13-caching-webr.html",
    "href": "posts/2024-06-13-caching-webr.html",
    "title": "Caching WebR from CDN",
    "section": "",
    "text": "I am developing Rust bindings to WebR. Because WebR is not compiled for WASI and only WebAssembly, native Rust bindings are not possible. Instead, bindings are done through wasm-bindgen which creates bindings to JavaScript.\nThe WIP Rust crate is called webr-js-rs.\n\n\n\n\n\n\nTip\n\n\n\nI’m building these bindings to support flrsh.dev.\nSign up for an account to be notified when I launch our first course (a deep dive on DuckDB)!\n\n\nwebr-js-rs works only on wasm targets.\n\n\n\n\n\n\nNote\n\n\n\n\nSee this informative blog post from Mozilla on what WASI is.\nThere is an outstanding issue on the WebR GitHub.\n\n\n\nThe problem I was encountering:\nWebR wasn’t caching the binaries!\nTurns out that this is because my code had this:\n#[wasm_bindgen(module = \"https://webr.r-wasm.org/latest/webr.mjs\")]\nI brought this issue up in a GitHub issue. @GeorgeStagg pointed out\n\n“The webR CDN assets under /latest/ are intentionally served with Cache-Control: no-cache so that the latest commit is always downloaded by the browser.”\n\nThis makes sense! It means since there is no cache instruction, it will fetch the binaries every time! Instead he recommended to use a tagged version\n\n“The longer-term builds under e.g. /v0.3.3/ are served with the HTTP header cache-control: max-age=604800, and so the webR assets should automatically be cached by browsers for 1 week.”\n\n👆🏼 emphasis mine.\nThis works! So I’ve changed webr-js-rs to use a fixed version.\nThe one challenge with this, though, is that even though the binaries are cached, the R session will be restarted from scratch if the browser is refreshed. So that is something I need to figure out next!"
  },
  {
    "objectID": "posts/2024-06-13-eval-strings.html",
    "href": "posts/2024-06-13-eval-strings.html",
    "title": "Evaluate strings as code",
    "section": "",
    "text": "Prompted by a post on Mastodon, I wanted to explore how to evaluate an R string as code.\nThis is generally a pretty common pattern that I have myself encountered in the past and had to work through a solution for—many times."
  },
  {
    "objectID": "posts/2024-06-13-eval-strings.html#the-problem",
    "href": "posts/2024-06-13-eval-strings.html#the-problem",
    "title": "Evaluate strings as code",
    "section": "The Problem",
    "text": "The Problem\nHow can I programatically create and execute valid R code?"
  },
  {
    "objectID": "posts/2024-06-13-eval-strings.html#a-solution",
    "href": "posts/2024-06-13-eval-strings.html#a-solution",
    "title": "Evaluate strings as code",
    "section": "A solution",
    "text": "A solution\nIn this case, the problem space is quite simple:\n\ngiven a package name and\na dataset name\nextract the dataset as an object\n\nYou can typically extract datasets from a package’s namespace. This looks like {pkgname}::{dataset}.\nWe can create this string simply like so:\n\npkg &lt;- \"dplyr\"\ndataset &lt;- \"starwars\"\ndataset_str &lt;- paste0(pkg, \"::\", dataset)\n\nEvaluating R code\nThen, we need to be able to evaluate this code. I find {rlang} to be very handy.\nTo convert a string into an expression, use rlang::parse_expr()\n\nlibrary(rlang)\nto_eval &lt;- parse_expr(dataset_str)\nto_eval\n\ndplyr::starwars\n\n\nThis creates a language type object.\nWe can now pass this into rlang::eval_bare() to evaluate the string and run the R code and store the result into an R object.\n\nresult &lt;- rlang::eval_bare(to_eval)\nresult\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;"
  },
  {
    "objectID": "posts/2024-06-13-eval-strings.html#alternative-solution",
    "href": "posts/2024-06-13-eval-strings.html#alternative-solution",
    "title": "Evaluate strings as code",
    "section": "Alternative solution",
    "text": "Alternative solution\nHere is an alternative solution which uses the data() function. Then, assuming the name of the dataset is created in the environment, fetches it using get().\n\nenglue(\"data({dataset}, package = '{pkg}')\") |&gt;\n  parse_expr() |&gt;\n  eval_bare()\n\nres &lt;- get(dataset)\nres\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nThere are issues with this in that you can also end up overwriting things. We can create a new environment if we’d like as well.\n\n# create a custom environment to store stuff\nmy_env &lt;- rlang::env()\n\nenglue(\"data({dataset}, package = '{pkg}')\") |&gt;\n  parse_expr() |&gt;\n  eval_bare(my_env)\n\n# get it from the environment\nres &lt;- get(dataset, envir = my_env)\nres\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;"
  },
  {
    "objectID": "posts/2024-06-24-file-server/index.html",
    "href": "posts/2024-06-24-file-server/index.html",
    "title": "Static file server in R",
    "section": "",
    "text": "Plumber, ambiorix, and opencpu are the keys to putting R into production.\nSometimes all an API needs to do is statically serve files. Making a static file server with R is insanely easy.\nFor this example, I have a folder called /public which I want to serve files from at the API path /static.\nTo do this we create a plumber API using the pr_static() function or #* @assets if using the other plumber declaration format."
  },
  {
    "objectID": "posts/2024-06-24-file-server/index.html#making-the-file-server",
    "href": "posts/2024-06-24-file-server/index.html#making-the-file-server",
    "title": "Static file server in R",
    "section": "Making the file server",
    "text": "Making the file server\n\nlibrary(plumber)\n\npr() |&gt;\n  pr_static(\"/static\", \"./public\") |&gt;\n  pr_run()"
  },
  {
    "objectID": "posts/2024-06-24-file-server/index.html#calling-the-file-server",
    "href": "posts/2024-06-24-file-server/index.html#calling-the-file-server",
    "title": "Static file server in R",
    "section": "calling the file server",
    "text": "calling the file server\nWe can call this api using a GET request:\n\nlibrary(httr2)\n\niris_csv &lt;- request(\"http://127.0.0.1:3000/static/iris.csv\") |&gt;\n  req_perform() |&gt;\n  resp_body_string()\n\nreadr::read_csv(iris_csv)\n\nRows: 150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Species\ndbl (4): Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows"
  },
  {
    "objectID": "posts/2024-06-24-file-server/index.html#alternative-plumber-format",
    "href": "posts/2024-06-24-file-server/index.html#alternative-plumber-format",
    "title": "Static file server in R",
    "section": "Alternative plumber format",
    "text": "Alternative plumber format\n#* @assets ./public /static\nlist()"
  },
  {
    "objectID": "posts/2024-06-25-handle-files-plumber/index.html",
    "href": "posts/2024-06-25-handle-files-plumber/index.html",
    "title": "Read a CSV in a production API",
    "section": "",
    "text": "Deploying RESTful APIs is the way to put any language into production. R is not any different.\nOne challenge when making APIs is handling files.\nUploading files is done typically with a multipart request."
  },
  {
    "objectID": "posts/2024-06-25-handle-files-plumber/index.html#handling-multipart-requests-in-r",
    "href": "posts/2024-06-25-handle-files-plumber/index.html#handling-multipart-requests-in-r",
    "title": "Read a CSV in a production API",
    "section": "Handling multipart requests in R",
    "text": "Handling multipart requests in R\nYou can process them using the {mime} package.\n\n\nNamed after “mime types” not Mr. Mime\n{plumber} provides access to the body of a request using the req argument.\n#* @post /upload\nupload &lt;- function(req, res) {\n    # body\n}\nTo access the structure of request use mime::parse_multipart(req).\nModifying the function like so will return json from the API\n#* @post /upload\nupload &lt;- function(req, res) {\n    mp &lt;- mime::parse_multipart(req)\n    mp\n}\nSave this as plumber.R\nRun your API\nIn your terminal (from the same working directory as plumber.R) run R -e 'plumber::plumb(\"plumber.R\")$run(port = 3000)'\nThis will give you a background API to call."
  },
  {
    "objectID": "posts/2024-06-25-handle-files-plumber/index.html#making-a-multipart-request",
    "href": "posts/2024-06-25-handle-files-plumber/index.html#making-a-multipart-request",
    "title": "Read a CSV in a production API",
    "section": "Making a multipart request",
    "text": "Making a multipart request\nUse httr2 to create the multipart request.\n\nStart the request with request()\n\nBase the request object to req_body_multipart() to add data\nUse key-value pairs to req_body_multipart(...) to add data\n\nNote that values must be a string so create the json yourself\n\n\nSend the request using req_perform()\n\n\nHere we give it a unique ID and add a sample of data\n\nlibrary(httr2)\n\nresp &lt;- request(\"http://127.0.0.1:3000/upload\") |&gt;\n  req_body_multipart(\n    id = ulid::ulid(),\n    sample = jsonify::to_json(sample(1:100, 10), unbox = TRUE)\n  ) |&gt;\n  req_perform()\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\nresp\n\n&lt;httr2_response&gt;\n\n\nPOST http://127.0.0.1:3000/upload\n\n\nStatus: 200 OK\n\n\nContent-Type: application/json\n\n\nBody: In memory (81 bytes)\n\n\nWe extract the data using resp_body_string() and process it using\n\nresp_body_string(resp) |&gt;\n  RcppSimdJson::fparse()\n\n$id\n[1] \"01J1QF6TA2VN2Z5WSFJ8DMJJ5W\"\n\n$sample\n[1] \"[42,85,18,65,14,10,9,21,27,93]\""
  },
  {
    "objectID": "posts/2024-06-25-handle-files-plumber/index.html#adding-files",
    "href": "posts/2024-06-25-handle-files-plumber/index.html#adding-files",
    "title": "Read a CSV in a production API",
    "section": "Adding files",
    "text": "Adding files\nWe’ll create a tempory file containing the iris data.frame and send this to the API endpoint.\nThese two lines:\n\nCreate a temporary csv file\nWrite the data frame to the temporary file\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a very handy trick that you might be able to adapt to many other circumstances. Temporary files are very useful.\n\n\n\ntmp &lt;- tempfile(fileext = \".csv\")\nreadr::write_csv(head(iris), tmp)\n\nNext we need to upload the file to our request. Do this using curl::form_file(). You need to provide a path to the file. In this case, it will be the temporary file.\n\nresp &lt;- request(\"http://127.0.0.1:3000/upload\") |&gt;\n  req_body_multipart(\n    file = curl::form_file(tmp)\n  ) |&gt;\n  req_perform()\n\nresp_body_string(resp) |&gt;\n  jsonify::pretty_json()\n\n{\n    \"file\": [\n        {\n            \"name\": \"filef10c3faec0e9.csv\",\n            \"size\": 192,\n            \"type\": \"application/octet-stream\",\n            \"datapath\": \"/var/folders/wd/xq999jjj3bx2w8cpg7lkfxlm0000gn/T//RtmphrlFYJ/filef0d54bf0c8ce\"\n        }\n    ]\n}\n\n\nIn this case file is a named list. mime stores the file in a temporary path accessible via datapath. So let’s try adding an API endpoint to read a csv file."
  },
  {
    "objectID": "posts/2024-06-25-handle-files-plumber/index.html#read-csv-in-plumber-api",
    "href": "posts/2024-06-25-handle-files-plumber/index.html#read-csv-in-plumber-api",
    "title": "Read a CSV in a production API",
    "section": "Read CSV in Plumber API",
    "text": "Read CSV in Plumber API\nHere we read the csv from the path. We would probably need to add some better checks here. Like checking that the field actually exists in mp but the error will be propagates as a 500 status anyways.\nSomething is always better than nothing. Just like this blog post.\n#* @post /read_csv\nfunction(req, res) {\n  mp &lt;- mime::parse_multipart(req)\n  readr::read_csv(mp$file$datapath)\n}"
  },
  {
    "objectID": "posts/2024-06-25-handle-files-plumber/index.html#send-csv-to-api",
    "href": "posts/2024-06-25-handle-files-plumber/index.html#send-csv-to-api",
    "title": "Read a CSV in a production API",
    "section": "Send CSV to API",
    "text": "Send CSV to API\nHere is how we can send the csv to the API\n\nresp &lt;- request(\"http://127.0.0.1:3000/read_csv\") |&gt;\n  req_body_multipart(\n    file = curl::form_file(tmp)\n  ) |&gt;\n  req_perform()\n\nresp_body_string(resp) |&gt;\n  jsonify::pretty_json()\n\n[\n    {\n        \"Sepal.Length\": 5.1,\n        \"Sepal.Width\": 3.5,\n        \"Petal.Length\": 1.4,\n        \"Petal.Width\": 0.2,\n        \"Species\": \"setosa\"\n    },\n    {\n        \"Sepal.Length\": 4.9,\n        \"Sepal.Width\": 3,\n        \"Petal.Length\": 1.4,\n        \"Petal.Width\": 0.2,\n        \"Species\": \"setosa\"\n    },\n    {\n        \"Sepal.Length\": 4.7,\n        \"Sepal.Width\": 3.2,\n        \"Petal.Length\": 1.3,\n        \"Petal.Width\": 0.2,\n        \"Species\": \"setosa\"\n    },\n    {\n        \"Sepal.Length\": 4.6,\n        \"Sepal.Width\": 3.1,\n        \"Petal.Length\": 1.5,\n        \"Petal.Width\": 0.2,\n        \"Species\": \"setosa\"\n    },\n    {\n        \"Sepal.Length\": 5,\n        \"Sepal.Width\": 3.6,\n        \"Petal.Length\": 1.4,\n        \"Petal.Width\": 0.2,\n        \"Species\": \"setosa\"\n    },\n    {\n        \"Sepal.Length\": 5.4,\n        \"Sepal.Width\": 3.9,\n        \"Petal.Length\": 1.7,\n        \"Petal.Width\": 0.4,\n        \"Species\": \"setosa\"\n    }\n]\n\n\nNote that the response is just nice json.\nWe can parse that back doing a full round trip:\n\nresp_body_string(resp) |&gt;\n  RcppSimdJson::fparse()\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa"
  },
  {
    "objectID": "posts/2024-06-25-handle-files-plumber/index.html#scale-your-apis",
    "href": "posts/2024-06-25-handle-files-plumber/index.html#scale-your-apis",
    "title": "Read a CSV in a production API",
    "section": "Scale your APIs",
    "text": "Scale your APIs\nUse Valve to scale and deploy your applications to production.\nIt kicks ass tbh."
  },
  {
    "objectID": "posts/2024-06-30-type-safety/index.html",
    "href": "posts/2024-06-30-type-safety/index.html",
    "title": "Type safe(r) R code",
    "section": "",
    "text": "Type safety is all the rage these days. It’s the (one of the many) reason why people love Rust , TypeScript, and Pydantic.\nKnowing what type of data is coming in and going out of a function is critical! It means fewer bugs and more robust code.\nI will introduce you to the r-lib standalone checks. Here is a peek of some code fromarcgisgeocode that helps make the function more type safe."
  },
  {
    "objectID": "posts/2024-06-30-type-safety/index.html#introduction-to-r-lib-type-safety-checks",
    "href": "posts/2024-06-30-type-safety/index.html#introduction-to-r-lib-type-safety-checks",
    "title": "Type safe(r) R code",
    "section": "",
    "text": "Type safety is all the rage these days. It’s the (one of the many) reason why people love Rust , TypeScript, and Pydantic.\nKnowing what type of data is coming in and going out of a function is critical! It means fewer bugs and more robust code.\nI will introduce you to the r-lib standalone checks. Here is a peek of some code fromarcgisgeocode that helps make the function more type safe."
  },
  {
    "objectID": "posts/2024-06-30-type-safety/index.html#what-is-type-safety",
    "href": "posts/2024-06-30-type-safety/index.html#what-is-type-safety",
    "title": "Type safe(r) R code",
    "section": "What is type safety?",
    "text": "What is type safety?\nA type safe language is one where each variable has a known and validated type. R is not type safe.\nWhen you define a function in a type safe language, you have to specify the input types and the output types.\nHere is a function that scales one numeric variable by another.\n\nscale_by &lt;- function(x, y) {\n  x / y\n}\n\nThis is not type safe. I can pass in a character vector a list, NULL, or even a POSIXct class. Sometimes R will do the appropriate conversions for us. But other times it wont.\n\nscale_by(100, \"10\")\n\nError in x/y: non-numeric argument to binary operator\n\n\nYou want to be in control of your function!"
  },
  {
    "objectID": "posts/2024-06-30-type-safety/index.html#why-type-safety-is-important",
    "href": "posts/2024-06-30-type-safety/index.html#why-type-safety-is-important",
    "title": "Type safe(r) R code",
    "section": "Why type safety is important",
    "text": "Why type safety is important\nType safety allows us to catch and prevent errors early and thus prevent unintended bugs. Without type safety, R may perform silent coercions or your code may run as R intended—but not as you intended.\n💡 A type coercion is a type conversion that occurs because one type does not match the other and is done silently. Casting is when you explicitly change the type—e.g. calling as.integer() on doubles()\nAdding type guarantees ensures that your code functions as intended."
  },
  {
    "objectID": "posts/2024-06-30-type-safety/index.html#type-safety-in-other-languages",
    "href": "posts/2024-06-30-type-safety/index.html#type-safety-in-other-languages",
    "title": "Type safe(r) R code",
    "section": "Type safety in other languages",
    "text": "Type safety in other languages\nType safety is becoming an increasingly common and more important aspect of programming. People love Rust for its type safety among other things. Rust (and C/++ and Java and Scala etc) is a statically typed language.\n💡 A statically typed language requires you to specify the type of object that are used in a function and elsewhere.\nRust’s static typing\nIn Rust, you define a type and that type is unique.\nstruct Person {\n    name: String,\n    age: u8\n}\nTo create a person you would write Person { name: \"Josiah\".to_string(), age: 28 } . This is recognized as a Person struct. In Rust, a function must know its argument types, for example:\nfn calculate_birth_year(person: &Person) -&gt; i32 {\n    // use chrono::DateLike\n    let now = chrono::Utc::now();\n    (now.year() - person.age as i32) \n}\nThis function takes a reference to a Person and calculates (roughly) what year they were born in. If I had another struct called Me with the same exact fields, this wouldn’t work.\nstruct Me {\n    name: String,\n    age: u8\n}\nEven though Me and Person have the exact same field types, they are recognized as different types.\nThis is different than how JavaScript does this.\nTypeScript Interfaces\nThe JavaScript folks now have TypeScript which is pseudo-type safety. TypeScript uses duck typing.\n💡 If it looks like a duck, swims like a duck, and quacks like a duck, this it probably is a duck.\nIf I understand TypeScript correctly, they use a type interface. These feel similar to struct definitions in Rust.\ninterface Person {\n    name: string;\n    age: number;\n}\nIn TypeScript, these interfaces are a way to standardizes what a type looks like. But not an actual type themself! This is (I think), the equivalent JavaScript code to calculate the birth year of an individual.\nfunction calculateBirthYear(person: Person) {\n        Date().getFullYear() - person.age\n}\nWith this, though, you don’t actually need to have an instance of Person . Instead, you can have a normal JavaScript object that looks (and quacks) just like the Person type.\nconst john: Person = {\n    name: 'John Doe',\n    age: 30\n}\n\nlet jane = { name: 'Jane Doe', age: 28 }\n\nconsole.log(calculateBirthYear(john));\nconsole.log(calculateBirthYear(jane));\nThese both work."
  },
  {
    "objectID": "posts/2024-06-30-type-safety/index.html#type-safety-in-r",
    "href": "posts/2024-06-30-type-safety/index.html#type-safety-in-r",
    "title": "Type safe(r) R code",
    "section": "Type safety in R",
    "text": "Type safety in R\nLike JavaScript, and Python (yes I know about type hinting, thats opt in and different), R doesn’t do any validation of arguments. TypeScript can add a layer of Duck Typing checks to the functions which is great for them. But what about us?\nHow can we make our R functions safer? In R, (almost) everything is a vector. The r-lib team has (very quietly) created what I think is the greatest contribution to the tidyverse ecosystem in a long time in the form of standalone type check functions."
  },
  {
    "objectID": "posts/2024-06-30-type-safety/index.html#stand-alone-type-checks",
    "href": "posts/2024-06-30-type-safety/index.html#stand-alone-type-checks",
    "title": "Type safe(r) R code",
    "section": "Stand-alone type checks",
    "text": "Stand-alone type checks\nThe standalone functions are quite unique. I’ve never seen anything quite like them. They’re literally standalone R files with a bunch of handy R functions. It’s like adding a package but without adding it as a dependency.\nThese are functions prefixed with check_ that test inputs for the most common types. They provide beautiful error messages and have commonly needed flexibility.\nAdd type checks to your project\nThe usethis package has a handy function use_standalone() which will add these functions for you.\n\nusethis::use_standalone(\"r-lib/rlang\", file = \"types-check\")\n\nThis is supposed to be used in the context of an R package but can still be used in any R script. THe function requires an R directory to be found at the root."
  },
  {
    "objectID": "posts/2024-06-30-type-safety/index.html#standalone-type-checks",
    "href": "posts/2024-06-30-type-safety/index.html#standalone-type-checks",
    "title": "Type safe(r) R code",
    "section": "Standalone type checks",
    "text": "Standalone type checks\nWe can get really far in enhancing type safety\nhttps://usethis.r-lib.org/reference/use_standalone.html\nSince this isn’t an R package, I will source the functions. Otherwise, run devtools::load_all() for the functions to become available.\n\ngetwd()\n\n[1] \"/Users/josiahparry/github/quarto-site/posts/2024-06-30-type-safety\"\n\nsource(\"R/import-standalone-obj-type.R\", local = TRUE)\nsource(\"R/import-standalone-types-check.R\", local = TRUE)\nlibrary(rlang)\n\n\n\n\n\n\n\nNote\n\n\n\nThese standalone checks require that rlang be an imported package. Use usethis::use_package(\"rlang\"). It is a very small package and has no dependencies. Very little to lose by adding it."
  },
  {
    "objectID": "posts/2024-06-30-type-safety/index.html#scalar-checks",
    "href": "posts/2024-06-30-type-safety/index.html#scalar-checks",
    "title": "Type safe(r) R code",
    "section": "Scalar checks",
    "text": "Scalar checks\nR doesn’t have the concept of a scalar. Though using a scalar is still very useful in R.\nThe standalone checks provide helpers for checking scalar values. There a quite a few but the ones I use most commonly are:\n\ncheck_string()\ncheck_bool()\ncheck_number_whole()\ncheck_number_decimal()\n\nUsage\nEach of these functions provide the arguments:\n\nallow_na\nallow_null\n\nThis is helpful because using NULL is often used as a default argument for optional arguments.\nFor example we can check that something is a string:\n\ncheck_string(\"a\")\n\nBut when it is a character vector:\n\ncheck_string(c(\"hello\", \"world\"))\n\n\n\nError:\n! `c(\"hello\", \"world\")` must be a single string, not a character vector.\n\n\nThis gives us an informative error telling the user what type was found and expected.\nIn the case of NULLs we can provide the allow_null argument which allows the test to pass.\n\ncheck_string(NULL)\ncheck_string(NULL, allow_null = TRUE)"
  },
  {
    "objectID": "posts/2024-06-30-type-safety/index.html#vector-checks",
    "href": "posts/2024-06-30-type-safety/index.html#vector-checks",
    "title": "Type safe(r) R code",
    "section": "Vector checks",
    "text": "Vector checks\nIn addition to scalar checks, there are many handy vectorized checks.\nThere are vector checks these are:\n\ncheck_character()\ncheck_logical()\ncheck_data_frame()\n\n\ncheck_character(1:2)\ncheck_logical(c(\"a\", \"b\"))\ncheck_data_frame(list(a = 1, b = 2))\n\n\n\nError:\n! `1:2` must be a character vector, not an integer vector.\n\n\nError:\n! `c(\"a\", \"b\")` must be a logical vector, not a character vector.\n\n\nError:\n! `list(a = 1, b = 2)` must be a data frame, not a list."
  },
  {
    "objectID": "posts/2024-09-01-cran-checks.html",
    "href": "posts/2024-09-01-cran-checks.html",
    "title": "Get notified of failing CRAN checks",
    "section": "",
    "text": "CRAN performs checks on all CRAN packages quite frequently.\nIf a package has a warning or an error you have a week or two to fix it.\nUnfixed packages get removed.\nIf your package depends on the removed package it also gets removed.\nYou will not be notified if your package is removed.\nThat sucks."
  },
  {
    "objectID": "posts/2024-09-01-cran-checks.html#background",
    "href": "posts/2024-09-01-cran-checks.html#background",
    "title": "Get notified of failing CRAN checks",
    "section": "",
    "text": "CRAN performs checks on all CRAN packages quite frequently.\nIf a package has a warning or an error you have a week or two to fix it.\nUnfixed packages get removed.\nIf your package depends on the removed package it also gets removed.\nYou will not be notified if your package is removed.\nThat sucks."
  },
  {
    "objectID": "posts/2024-09-01-cran-checks.html#get-informed-cran-checks-github-action",
    "href": "posts/2024-09-01-cran-checks.html#get-informed-cran-checks-github-action",
    "title": "Get notified of failing CRAN checks",
    "section": "Get informed: CRAN checks GitHub Action",
    "text": "Get informed: CRAN checks GitHub Action\nThere is a new GitHub Action that you can use with your CRAN package. It will run once a day. If there are any WARN or ERROR statuses in any of the check flavors, then the GitHub Action will fail.\n\nAdd the action\nusethis::use_github_action(\n  url = \"https://github.com/ricochet-rs/cran-checks/blob/main/check-pkg/cran-checks.yaml\",\n  open = TRUE\n)\nThis will open a yaml file for you with the following:\nname: Check CRAN status\n\non:\n  schedule:\n    # Runs daily at 4:00 PM UTC (9:00 AM PST)\n    - cron: '0 16 * * *'  \n  # allows for manually running of the check\n  workflow_dispatch:\n\njobs:\n  check_cran_status:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Get CRAN checks\n        uses: ricochet-rs/cran-checks/check-pkg@main\n        with:\n          pkg: YOUR-PKG-NAME\nReplace YOUR-PKG-NAME with the name of your package (no quotes). Commit the file to GitHub and voilá.\n\n\nHow this works\nSince CRAN can’t be bothered to email us when they remove a package, we can utilized GitHub Actions to be our messenger.\nWhenever a GitHub Action fails, an email is sent to you so you know. Isn’t that nice?\nThe ricochet-rs/cran-checks repository fetches the status of all packages daily. Then stores them as a json file that is hosted on GitHub pages. They can be accessed from url https://ricochet-rs.github.io/cran-checks/b64.json.\nThe GitHub Action checks for your package’s json file and looks at all of the statuses. If the package isn’t found or if there is a single warning or error, it fails."
  },
  {
    "objectID": "posts/2024-09-01-cran-checks.html#motivation-packages-removed-without-warning",
    "href": "posts/2024-09-01-cran-checks.html#motivation-packages-removed-without-warning",
    "title": "Get notified of failing CRAN checks",
    "section": "Motivation: packages removed without warning",
    "text": "Motivation: packages removed without warning\nOn August 19th, 2024, CRAN removed 3 of my packages without so much as an email. I was notified by a colleague who was giving a live workshop to the US Forest Service on August 21st. When the attendants tried to install the packages, they weren’t on CRAN. It was a very bad look.\nHow were they supposed to know the packages we’re gone? They were there when they tested. I didn’t get any warning so I couldn’t help them find a way to provide a work around."
  },
  {
    "objectID": "posts/2024-09-01-cran-checks.html#solution-always-check-your-cran-checks",
    "href": "posts/2024-09-01-cran-checks.html#solution-always-check-your-cran-checks",
    "title": "Get notified of failing CRAN checks",
    "section": "Solution: always check your CRAN checks",
    "text": "Solution: always check your CRAN checks\nI’ve been told that I should check the status of my CRAN package checks on every flavor every day. CRAN tests on r-devel, so frequently there are false positives. Also, I’ve got a lot going on and I can hardly remember to talk myself on a walk some days. Other package managers will let you know if your package is failing, why doesn’t CRAN?"
  },
  {
    "objectID": "posts/2024-09-01-cran-checks.html#a-future-of-cran-check-messaging",
    "href": "posts/2024-09-01-cran-checks.html#a-future-of-cran-check-messaging",
    "title": "Get notified of failing CRAN checks",
    "section": "A future of CRAN check messaging?",
    "text": "A future of CRAN check messaging?\nIdeally, I would like to have an opt-in service that provides these checks daily and will email you directly if any of your R packages are failing. However, this would require funding which I don’t have.\nIf you are interested in sponsoring this project further, email me at josiah.parry at gmail dot com and we can discuss this. I think the R community could benefit quite greatly."
  },
  {
    "objectID": "posts/2024-10-10-leptos-highlight-js.html",
    "href": "posts/2024-10-10-leptos-highlight-js.html",
    "title": "Add syntax highlighting to leptos",
    "section": "",
    "text": "I’ve been building a thing with Leptos and Tailwind CSS for a while.\nOne challenge I’ve had is adding syntax highlighting to my code chunks.\n\n\n\n\n\n\nNote\n\n\n\nI am using pulldown-cmark to take a README.md to process it in html. Then adding the contents to a div. Something like:\nfn md_to_html(content: &str) -&gt; String {\n    let parser = pulldown_cmark::Parser::new(content);\n    // Write to a new String buffer.\n    let mut html_output = String::new();\n    pulldown_cmark::html::push_html(&mut html_output, parser);\n    html_output\n}\n\n#[server]\nasync fn fetch_readme(fp: String) -&gt; Result&lt;String, ServerFnError&gt; {\n    let file = std::fs::File::open(fp)?;\n    let reader = std::io::BufReader::new(file)?;\n    Ok(std::io::read_to_string(reader)?)\n}\n\n#[component]\nfn MyRenderedReadMe(\n    let readme_content = create_resource(move || (), move |_| {\n        fetch_readme(\"/path/to/README.md\")\n    });\n\n    view! {\n        &lt;Transition&gt;\n            if let Some(item) = readme_content.get() {\n                if let Ok(it) = item {\n                    view! { &lt;div inner_html={md_to_html(&it)}/&gt; }.into_view()\n                } else {\n                    ().into_view()\n                }\n            } else {\n                ().into_view()\n            }\n        &lt;/Transition&gt;\n    }\n)\n\n\nAt first I thought adding syntax highlighting to leptos was going to involve wasm-bindgen and other pain, but it doesn’t.\n\nGo to https://highlightjs.org/\nClick Download\nSelect the languages you want to support\nClick Download\n\nOnce you’ve downloaded the highlight folder. Move it into your leptos project at {leptos-root}/public so the path is {leptos-root}/public/highlight with everything in there.\nThen add the following to your App component just below your router\n&lt;script src=\"/highlight/highlight.min.js\"&gt;&lt;/script&gt;\n&lt;link rel=\"stylesheet\" href=\"/highlight/styles/nord.css\"/&gt; \n&lt;script&gt;hljs.highlightAll();&lt;/script&gt;\n\n\nNote 👈🏼 that I chose the nord.css file. You can choose ant of the ones provided or just use default.css.\nThis will look like\n#[component]\npub fn App() -&gt; impl IntoView {\n\n    view! {\n        &lt;Stylesheet id=\"leptos\" href=\"/pkg/your-project-name.css\"/&gt;\n        &lt;Title text=\"Welcome to Leptos\"/&gt;\n\n        &lt;Router fallback=|| {\n            let mut outside_errors = Errors::default();\n            outside_errors.insert_with_default_key(AppError::NotFound);\n            view! { &lt;ErrorTemplate outside_errors/&gt; }.into_view()\n        }&gt;\n            &lt;main&gt;\n                &lt;Routes&gt;\n                 // your routes here\n                &lt;/Routes&gt;\n            &lt;/main&gt;\n        &lt;/Router&gt;\n\n        // add syntax highlighting here: \n        &lt;script src=\"/highlight/highlight.min.js\"&gt;&lt;/script&gt;\n        &lt;link rel=\"stylesheet\" href=\"/highlight/styles/nord.css\"/&gt;\n        &lt;script&gt;hljs.highlightAll();&lt;/script&gt;\n    }\n}"
  },
  {
    "objectID": "posts/2024-11-21-s7-options-objects.html",
    "href": "posts/2024-11-21-s7-options-objects.html",
    "title": "S7 & Options objects",
    "section": "",
    "text": "One scenario I have encountered is the case case of readr::read_delim(). The argument col_names = TRUE by default, can be FALSE, or it can be a character vector of the names to provide to the columns it is reading.\nThis is a bit stinky 😷. But it actually makes a lot of sense.\nThis is a little confusing when we think deeply about the character vector option.\nThere are two scenarios here:\nLets explore how this works in practice a bit. Here we write iris to a temporary file.\ntmp &lt;- tempfile(fileext = \".csv\")\nreadr::write_csv(iris, tmp)"
  },
  {
    "objectID": "posts/2024-11-21-s7-options-objects.html#rethinking-the-arguments",
    "href": "posts/2024-11-21-s7-options-objects.html#rethinking-the-arguments",
    "title": "S7 & Options objects",
    "section": "Rethinking the arguments",
    "text": "Rethinking the arguments\nTo me, I think these arguments can be made less complected.\nTo me, there are two arguments burried in col_names:\n\nheader = TRUE\ncol_names = NULL\n\nThe imaginary header argument should be used to determine if there is a header line to be used.\nThe col_names, which defaults to NULL can be used to provide an alternative set of column names.\nThis approach would reduce the cognitive overload of col_names argument.\nHowever, there are\n\nlength(formals(readr::read_csv))\n\n[1] 20\n\n\narguments already….so… additional ones? That could be quite a bit."
  },
  {
    "objectID": "posts/2024-11-21-s7-options-objects.html#options-objects-with-s7",
    "href": "posts/2024-11-21-s7-options-objects.html#options-objects-with-s7",
    "title": "S7 & Options objects",
    "section": "Options objects with S7",
    "text": "Options objects with S7\nOne alternative to having every option as a function argument is to create an options object.\nThis is very common in the Rust ecosystem. There is a struct that is used to define common settings. That object is then passed into methods and functions.\nWe could consider doing something similar for the readr::read_csv() function.\nLets take a look at the arguments for readr::read_csv()\n\nrlang::fn_fmls_names(readr::read_csv)\n\n [1] \"file\"            \"col_names\"       \"col_types\"       \"col_select\"     \n [5] \"id\"              \"locale\"          \"na\"              \"quoted_na\"      \n [9] \"quote\"           \"comment\"         \"trim_ws\"         \"skip\"           \n[13] \"n_max\"           \"guess_max\"       \"name_repair\"     \"num_threads\"    \n[17] \"progress\"        \"show_col_types\"  \"skip_empty_rows\" \"lazy\"           \n\n\nMany of these are booleans or scalars. I think we can improve this by using S7 to store our options as a standalone object.\nLooking at the arguments for read_csv() I think our options object can be used for the following options:\n\nlocale\nna\nquote\ncomment\ntrim_ws\nskip\nn_max\nguess_max\nname_repair\nnum_threads\nprogress\nshow_col_types\nskip_empty_rows\nlazy\n\nThis will take 14 of the less commonly used arguments out of the function!\nThe first thing we will do is define properties for each of these values. It looks like a lot of code, but it is not so bad! This boilerplate is going to give us a strongly typed object that will catch errors early!"
  },
  {
    "objectID": "posts/2024-11-21-s7-options-objects.html#s7-object-properties",
    "href": "posts/2024-11-21-s7-options-objects.html#s7-object-properties",
    "title": "S7 & Options objects",
    "section": "S7 object properties",
    "text": "S7 object properties\nFor each of the arguments we want to ensure that we:\n\nhave a good default\nvalidate any input\n\nFirst we’re looking at the locale. This one is quite a lot of checking.\nProperty validation\nIdeally, the locale would be an S7 object so we could provide a class_locale as our propery but we don’t have that luxury. So here, we validate each of the components of the locale object.\n\nlibrary(S7)\n\n.locale &lt;- new_property(\n  class_list,\n  default = readr::default_locale(),\n  validator = function(value) {\n    dnames &lt;- value$date_names\n    invalid &lt;- !rlang::is_character(dnames$mon, n = 12) ||\n      !rlang::is_character(dnames$mon_ab, n = 12) ||\n      !rlang::is_character(dnames$day, n = 7) ||\n      !rlang::is_character(dnames$day_ab, n = 7) ||\n      !rlang::is_character(dnames$am_pm, n = 2) || !rlang::is_scalar_character(value$date_format) || !rlang::is_scalar_character(value$time_format) || !rlang::is_scalar_character(value$decimal_mark) || !rlang::is_scalar_character(value$grouping_mark) || !rlang::is_scalar_character(value$tz) || !rlang::is_scalar_character(value$encoding)\n\n    if (invalid) {\n      \"expected `locale` object\"\n    }\n  }\n)\n\nSimilarly, the argument for name_repair is not at all straight forward. It can be one of any known strategy or it can be a function that is applied to the names via vctrs::vec_as_names().\n\n.name_repair &lt;- new_property(\n  class_any,\n  default = \"unique\",\n  validator = function(value) {\n    known_strategies &lt;- c(\"minimal\", \"unique\", \"check_unique\", \"unique_quiet\", \"universal\", \"universal_quiet\")\n\n    if (rlang::is_function(value)) {\n      fmls &lt;- rlang::fn_fmls(value)\n      if (sum(vapply(fmls,inherits, logical(1),  \"name\")) &gt; 1) {\n        \"name repair function must only have one required argument\"\n      }\n      return(NULL)\n    }\n\n    if (!rlang::is_scalar_character(value)) {\n      \"`name_repair` must be one of minimal, unique, check_unique, unique_quiet, universal, universal_quiet or a function\"\n    }\n\n    if (!value %in% known_strategies) {\n      sprintf(\"%s is not a valid `name_repair` value\")\n    }\n  }\n)\n\nHere we define the validators for the rest of the options. These are all quite straight forward and are mostly scalars.\n\nCode.na &lt;- new_property(\n  class_character,\n  default = c(\"\", \"NA\")\n)\n\n.quote &lt;- new_property(\n  class_logical, \n  default = TRUE,\n  validator = function(value) {\n      if (!rlang::is_scalar_logical(value)) {\n          \"`quote` must be a scalar character\"\n      }\n  }\n)\n\n\n.comment &lt;- new_property(\n  class_character,\n  default = \"\\\"\",\n  validator = function(value) {\n    if (!rlang::is_scalar_character(value)) {\n      \"`comment` must be a scalar character\"\n    }\n  }\n)\n\n\n.trim_ws &lt;- new_property(\n  class_logical, \n  default = TRUE,\n  validator = function(value) {\n      if (!rlang::is_scalar_logical(value)) {\n          \"`trim_ws` must be a scalar character\"\n      }\n  }\n)\n\n.skip &lt;- new_property(\n  class_numeric,\n  default = 0L,\n  validator = function(value) {\n      if (!rlang::is_scalar_integerish(value)) {\n          \"`skip` must be a scalar numeric\"\n      }\n  }\n)\n\n.n_max &lt;- new_property(\n  class_numeric,\n  default = Inf,\n  validator = function(value) {\n      if (!rlang::is_scalar_integerish(value)) {\n          \"`n_max` must be a scalar numeric\"\n      }\n  }\n)\n\n.guess_max &lt;- new_property(\n  class_numeric,\n  default = 1000L,\n  validator = function(value) {\n      if (!rlang::is_scalar_integerish(value)) {\n          \"`guess_max` must be a scalar numeric\"\n      }\n\n  }\n)\n\n.n_max &lt;- new_property(\n  class_numeric,\n  default = Inf,\n  validator = function(value) {\n      if (!rlang::is_scalar_integerish(value)) {\n          \"`n_max` must be a scalar numeric\"\n      }\n  }\n)\n\n.num_threads &lt;- new_property(\n  class_numeric,\n  default = readr::readr_threads(),\n  validator = function(value) {\n      if (!rlang::is_scalar_integerish(value)) {\n          \"`num_threads` must be a scalar numeric\"\n      }\n  }\n)\n\n.progress &lt;- new_property(\n    class_logical, \n    default = readr::show_progress(),\n    validator = function(value) {\n         if (!rlang::is_scalar_logical(value)) {\n          \"`progress` must be a scalar logical\"\n      }\n    }\n)\n\n.show_col_types &lt;- new_property(\n    class_logical, \n    default = readr::should_show_types() %||% TRUE,\n    validator = function(value) {\n         if (!rlang::is_scalar_logical(value)) {\n          \"`show_col_types` must be a scalar logical\"\n      }\n    }\n)\n\n.skip_empty_rows &lt;- new_property(\n    class_logical, \n    default = TRUE,\n    validator = function(value) {\n         if (!rlang::is_scalar_logical(value)) {\n          \"`skip_empty_rows` must be a scalar logical\"\n      }\n    }\n)\n\n\n.lazy &lt;- new_property(\n    class_logical, \n    default = readr::should_read_lazy(),\n    validator = function(value) {\n         if (!rlang::is_scalar_logical(value)) {\n          \"`lazy` must be a scalar logical\"\n      }\n    }\n)"
  },
  {
    "objectID": "posts/2024-11-21-s7-options-objects.html#s7-readr_opts-class",
    "href": "posts/2024-11-21-s7-options-objects.html#s7-readr_opts-class",
    "title": "S7 & Options objects",
    "section": "S7 readr_opts class",
    "text": "S7 readr_opts class\nNow can actually define the S7 object class by passing in all of our new property objects to the properties argument. Because we defined defaults for every property we can construct a default option object.\n\nclass_readr_opts &lt;- new_class(\n  \"readr_opts\",\n  properties = list(\n    locale = .locale,\n    na = .na,\n    quote = .quote,\n    comment = .comment,\n    trim_ws = .trim_ws,\n    skip = .skip,\n    n_max = .n_max,\n    guess_max = .guess_max,\n    name_repair = .name_repair,\n    num_threads = .num_threads,\n    progress = .progress,\n    show_col_types = .show_col_types,\n    skip_empty_rows = .skip_empty_rows,\n    lazy = .lazy\n  )\n)\n\nopts &lt;- class_readr_opts()\nopts\n\n&lt;readr_opts&gt;\n @ locale         :List of 7\n .. $ date_names   :List of 5\n ..  ..$ mon   : chr [1:12] \"January\" \"February\" \"March\" \"April\" ...\n ..  ..$ mon_ab: chr [1:12] \"Jan\" \"Feb\" \"Mar\" \"Apr\" ...\n ..  ..$ day   : chr [1:7] \"Sunday\" \"Monday\" \"Tuesday\" \"Wednesday\" ...\n ..  ..$ day_ab: chr [1:7] \"Sun\" \"Mon\" \"Tue\" \"Wed\" ...\n ..  ..$ am_pm : chr [1:2] \"AM\" \"PM\"\n ..  ..- attr(*, \"class\")= chr \"date_names\"\n .. $ date_format  : chr \"%AD\"\n .. $ time_format  : chr \"%AT\"\n .. $ decimal_mark : chr \".\"\n .. $ grouping_mark: chr \",\"\n .. $ tz           : chr \"UTC\"\n .. $ encoding     : chr \"UTF-8\"\n .. - attr(*, \"class\")= chr \"locale\"\n @ na             : chr [1:2] \"\" \"NA\"\n @ quote          : logi TRUE\n @ comment        : chr \"\\\"\"\n @ trim_ws        : logi TRUE\n @ skip           : int 0\n @ n_max          : num Inf\n @ guess_max      : int 1000\n @ name_repair    : chr \"unique\"\n @ num_threads    : int 8\n @ progress       : logi FALSE\n @ show_col_types : logi FALSE\n @ skip_empty_rows: logi TRUE\n @ lazy           : logi FALSE\n\n\nWe can access each of these properties using the @ accessor. For example, if we want the locale:\n\nopts@locale\n\n&lt;locale&gt;\nNumbers:  123,456.78\nFormats:  %AD / %AT\nTimezone: UTC\nEncoding: UTF-8\n&lt;date_names&gt;\nDays:   Sunday (Sun), Monday (Mon), Tuesday (Tue), Wednesday (Wed), Thursday\n        (Thu), Friday (Fri), Saturday (Sat)\nMonths: January (Jan), February (Feb), March (Mar), April (Apr), May (May),\n        June (Jun), July (Jul), August (Aug), September (Sep), October\n        (Oct), November (Nov), December (Dec)\nAM/PM:  AM/PM"
  },
  {
    "objectID": "posts/2024-11-21-s7-options-objects.html#simplifying-readrread_csv",
    "href": "posts/2024-11-21-s7-options-objects.html#simplifying-readrread_csv",
    "title": "S7 & Options objects",
    "section": "Simplifying readr::read_csv()\n",
    "text": "Simplifying readr::read_csv()\n\nNow, imagine if we can use this as a way to simplify the readr::read_csv() function. The function definition can now look like:\n\nread_csv &lt;- function(\n  file,\n  col_names = TRUE,\n  col_types = NULL,\n  col_select = NULL,\n  id = NULL,\n  options = class_readr_opts()\n) {\n    # function logic\n}\n\nThis greatly reduces the cognitive load for end users and it consolides options specification into a single object."
  },
  {
    "objectID": "posts/2024-11-28-httr2-oidc.html",
    "href": "posts/2024-11-28-httr2-oidc.html",
    "title": "Implementing OpenID Connect (OIDC) in R",
    "section": "",
    "text": "I am working on a rust project that I want to use OpenID Connect for. I’m struggling to wrap my head around it, so naturally, I implemented it in R to understand it better."
  },
  {
    "objectID": "posts/2024-11-28-httr2-oidc.html#what-is-oidc",
    "href": "posts/2024-11-28-httr2-oidc.html#what-is-oidc",
    "title": "Implementing OpenID Connect (OIDC) in R",
    "section": "What is OIDC?",
    "text": "What is OIDC?\nOpenID Connect (OIDC) is an authentication standard based on OAuth 2.0. The hope is that most identity providers (IDP) can have an implementation of OIDC so that plugging in their authentication system is pretty straight forward."
  },
  {
    "objectID": "posts/2024-11-28-httr2-oidc.html#oidc-discovery",
    "href": "posts/2024-11-28-httr2-oidc.html#oidc-discovery",
    "title": "Implementing OpenID Connect (OIDC) in R",
    "section": "OIDC discovery",
    "text": "OIDC discovery\nEach OIDC provider has an {issuer_url}/.well-known/openid-configuration URL which contains information about the authentication provider. This is a public facing document that can be used to find endpoints and other information\nFor this example, I’ve created a free account at Auth0 and made an application. I’ll store the url in a variable called issuer_url\n\nissuer_url &lt;- \"https://dev-2ts7ytkts28hfj4o.us.auth0.com\"\n\nAccessing the openid-configuration is a simple get request. We’ll create an oidc_discovery() function. This will return a list and we will give it a class oidc_provider\n\nlibrary(httr2)\n\noidc_discovery &lt;- function(issuer_url) {\n  res &lt;- request(issuer_url) |&gt;\n    req_url_path_append(\".well-known\", \"openid-configuration\") |&gt; \n    req_perform() |&gt;\n    resp_body_json()\n  structure(res, class = c(\"oidc_provider\", \"list\"))\n}\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nI’ve also given this object a nicer print method based on the httr2_oauth_client class in {httr2}.\n\nprint.oidc_provider &lt;- function(x, ...) {\n    # adapted from httr2:::print.httr2_request\n    cli::cli_text(cli::style_bold(\"&lt;\", paste(class(x)[1], collapse = \"/\"), \"&gt;\"))\n    lines &lt;- vapply(\n        x,\n        \\(.x) {\n        if (is.atomic(.x) && length(.x) == 1) {\n            if (is.character(.x)) {\n                paste0(\"'\", .x, \"'\")\n            }\n            else {\n                format(.x)\n            }\n        }\n        else {\n            class(.x)[1]\n        }\n        },\n        character(1)\n    )\n    cli::cli_dl(lines)\n    invisible(x)\n}\n\n\n\n\nUsing this gives us a very informative list that we will use for identifying our authorization endpoints.\n\nprovider &lt;- oidc_discovery(issuer_url)\n\n&lt;oidc_provider&gt;\nissuer: 'https://dev-2ts7ytkts28hfj4o.us.auth0.com/'\nauthorization_endpoint: 'https://dev-2ts7ytkts28hfj4o.us.auth0.com/authorize'\ntoken_endpoint: 'https://dev-2ts7ytkts28hfj4o.us.auth0.com/oauth/token'\ndevice_authorization_endpoint: 'https://dev-2ts7ytkts28hfj4o.us.auth0.com/oauth/device/code'\nuserinfo_endpoint: 'https://dev-2ts7ytkts28hfj4o.us.auth0.com/userinfo'\nmfa_challenge_endpoint: 'https://dev-2ts7ytkts28hfj4o.us.auth0.com/mfa/challenge'\njwks_uri: 'https://dev-2ts7ytkts28hfj4o.us.auth0.com/.well-known/jwks.json'\nregistration_endpoint: 'https://dev-2ts7ytkts28hfj4o.us.auth0.com/oidc/register'\nrevocation_endpoint: 'https://dev-2ts7ytkts28hfj4o.us.auth0.com/oauth/revoke'\nscopes_supported: list\nresponse_types_supported: list\ncode_challenge_methods_supported: list\nresponse_modes_supported: list\nsubject_types_supported: list\ntoken_endpoint_auth_methods_supported: list\nclaims_supported: list\nrequest_uri_parameter_supported: FALSE\nrequest_parameter_supported: FALSE\nid_token_signing_alg_values_supported: list\ntoken_endpoint_auth_signing_alg_values_supported: list\nend_session_endpoint:\n'https://dev-2ts7ytkts28hfj4o.us.auth0.com/oidc/logout'\nThe information in this object will be used for our oauth flows with httr2."
  },
  {
    "objectID": "posts/2024-11-28-httr2-oidc.html#oidc-client-object",
    "href": "posts/2024-11-28-httr2-oidc.html#oidc-client-object",
    "title": "Implementing OpenID Connect (OIDC) in R",
    "section": "OIDC Client Object",
    "text": "OIDC Client Object\nIn httr2, we create an httr2_oauth_client object to be used for our authentication flows. We will generalize that approac and create oidc_client().\nIn this function, we will store the redirect_uri into the client itself as well as tack on the oidc_client subclass. This will give us a nicer print method and prevent us from having to put in the redirect_uri multiple times.\n\noidc_client &lt;- function(\n  oidc_provider,\n  client_id = Sys.getenv(\"OIDC_CLIENT\"),\n  client_secret = Sys.getenv(\"OIDC_SECRET\"),\n  redirect_uri = oauth_redirect_uri()\n) {\n  client &lt;- oauth_client(\n    id = client_id,\n    secret = client_secret,\n    token_url = oidc_provider[[\"token_endpoint\"]]\n  )\n  client[[\"redirect_uri\"]] &lt;- redirect_uri\n  class(client) &lt;- c(\"oidc_client\", class(client))\n  client\n}\n\nThis function fetches the client id and secret from environment variables. This is because we do not want to store these variables directly in our code.\n\n\n\n\n\n\nTip\n\n\n\nUse usethis::edit_r_environ() to set these variables globally. Alternatively, you can use something like config to have a config.yml file or an alternative environment management system. But at the end of the day just please do not store your credentials in your code!!!!\n\n\nFor Auth0, you have to specify which redirect URIs can be trusted. In my case I set it to http://localhost:3000/oauth/callback in my application settings.\n\nclient &lt;- oidc_client(\n    provider,\n    redirect_uri = \"http://localhost:3000/oauth/callback\"\n)\n\n&lt;oidc_client/httr2_oauth_client&gt;\nname: bf83aacb811320e5da430601736f1286\nid:\nsecret: &lt;REDACTED&gt;\ntoken_url: https://dev-2ts7ytkts28hfj4o.us.auth0.com/oauth/token\nauth: oauth_client_req_auth_body\nredirect_uri: http://localhost:3000/oauth/callback\nThis client will now be used for our authentication steps."
  },
  {
    "objectID": "posts/2024-11-28-httr2-oidc.html#oauth2-code-flow",
    "href": "posts/2024-11-28-httr2-oidc.html#oauth2-code-flow",
    "title": "Implementing OpenID Connect (OIDC) in R",
    "section": "OAuth2 Code Flow",
    "text": "OAuth2 Code Flow\nThe most secure method of authentication with OAuth2 is the code flow. This is also the most common when building web applications. It will send you to the external provider to authenticate there, then return you to the app when complete with an access_token and an id_token.\nHere we create the oidc_flow_auth_code() function. The authorization endpoint will likely be different for providers. This is why we fetch it from the provider itself.\n\noidc_flow_auth_code &lt;- function(\n  client,\n  provider,\n  scope = \"openid profile email\"\n) {\n  oauth_flow_auth_code(\n    client,\n    provider$authorization_endpoint,\n    scope = scope,\n    redirect_uri = client$redirect_uri\n  )\n}\n\n\n\n\n\n\n\nNote\n\n\n\nNow that I’m looking at this again, it may be worth storing the the authorization endpoint into the client too…\n\n\nWhen we authenticate with OIDC we most also provide the openid scope. This indicates to the provider that the OIDC protocol will be used. Additionally, OIDC uses something called json web-tokens (JWT).\nJWTs have “claims” associated with them. This is basic informations about the user that is authenticated. These get stored alongside the access_token as an id_token.\nThe standard claim profile will give you a lot of basic information about an end-user. It wraps up the name, family_name, given_name, middle_name, nickname, preferred_username, profile, picture, website, gender, birthdate, zoneinfo, and locale claims.\nSpecify the claim you want after openid in the scope argument\n\ntoken &lt;- oidc_flow_auth_code(\n  client, provider,\n  scope = \"openid profile\"\n)\n\n&lt;httr2_token&gt;\ntoken_type: Bearer\naccess_token: &lt;REDACTED&gt;\nexpires_at: 2024-11-29 11:07:12\nid_token: &lt;REDACTED&gt;\nscope: openid profile\nWith this you’ve now authenticated using OIDC. Though you may want to access the user information in the token. We can do that by decoding the id_token."
  },
  {
    "objectID": "posts/2024-11-28-httr2-oidc.html#accessing-claims",
    "href": "posts/2024-11-28-httr2-oidc.html#accessing-claims",
    "title": "Implementing OpenID Connect (OIDC) in R",
    "section": "Accessing Claims",
    "text": "Accessing Claims\nHere we create a function parse_id_token() which takes the contents of token$id_token and parses it into something human representable.\n\ntoken$id_token\n\n\"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6ImV0WWVUTjhseGJ6VENZblBMNnhxSyJ9.eyJnaXZlbl9uYW1lIjoiSm9zaWFoIiwiZmFtaWx5X25hbWUiOiJQYXJyeSIsIm5pY2tuYW1lIjoiam9zaWFoLnBhcnJ5IiwibmFtZSI6Ikpvc2lhaCBQYXJyeSIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5...truncate\"\nThis is base64 encoded nonsense. Below is an opinionated way to decode this. I utilize the {b64} package for fast decoding. Then use {yyjsonr} for fast json parsing.\n\nparse_id_token &lt;- function(token) {\n  parts &lt;- strsplit(token$id_token, \"\\\\.\")[[1]][1:2]\n  b64::decode(parts, eng = b64::engine(\"url_safe_no_pad\")) |&gt; \n    lapply(rawToChar) |&gt; \n    rlang::set_names(c(\"header\", \"payload\")) |&gt; \n    lapply(yyjsonr::read_json_str)\n}\n\n$header\n$header$alg\n[1] \"RS256\"\n\n$header$typ\n[1] \"JWT\"\n\n$header$kid\n[1] \"redacted\"\n\n$payload\n$payload$given_name\n[1] \"Josiah\"\n\n$payload$family_name\n[1] \"Parry\"\n\n$payload$nickname\n[1] \"josiah.parry\"\n\n$payload$name\n[1] \"Josiah Parry\"\n\n$payload$picture\n[1] \"redacted\"\n\n$payload$updated_at\n[1] \"2024-11-28T00:46:24.934Z\"\n\n$payload$iss\n[1] \"https://dev-2ts7ytkts28hfj4o.us.auth0.com/\"\n\n$payload$aud\n[1] \"mi3FRXJuJarrM7rFBDr0N270l84ANSXo\"\n\n$payload$iat\n[1] 1732820832\n\n$payload$exp\n[1] 1732856832\n\n$payload$sub\n[1] \"redacted\"\n\n$payload$sid\n[1] \"redacted\""
  },
  {
    "objectID": "posts/2024-11-28-httr2-oidc.html#authenticating-requests-with-oidc",
    "href": "posts/2024-11-28-httr2-oidc.html#authenticating-requests-with-oidc",
    "title": "Implementing OpenID Connect (OIDC) in R",
    "section": "Authenticating requests with OIDC",
    "text": "Authenticating requests with OIDC\nHowever, you may want to wrap your requests with your OIDC auth provider.\n\nreq_auth_oidc &lt;- function(req, provider, client, scope = \"openid profile email\") {\n  req |&gt; \n    req_oauth_auth_code(\n      client = client,\n      auth_url = provider$authorization_endpoint,\n      scope = scope,\n      redirect_uri = client$redirect_uri\n    )\n}"
  },
  {
    "objectID": "posts/2024-11-28-httr2-oidc.html#accessing-userinfo",
    "href": "posts/2024-11-28-httr2-oidc.html#accessing-userinfo",
    "title": "Implementing OpenID Connect (OIDC) in R",
    "section": "Accessing UserInfo\n",
    "text": "Accessing UserInfo\n\nEach OIDC provider also has a UserInfo endpoint that can be accessed for user-level claims.\nWe can wrap this up as well:\n\noidc_user_info &lt;- function(provider, token) {\n  request(provider[[\"userinfo_endpoint\"]]) |&gt; \n    req_auth_bearer_token(token$access_token) |&gt; \n    req_perform() |&gt; \n    resp_body_json()\n}\n\nNote that this will only give you the user information that is associated with the claims used to authenticate with as well."
  },
  {
    "objectID": "posts/2024-12-01-advent-day-1/index.html",
    "href": "posts/2024-12-01-advent-day-1/index.html",
    "title": "🎄dvent Day 1: Rust and R solutions",
    "section": "",
    "text": "I give you the solutions to Day 1 of Advent of Code in R, Rust, and Rust in R for part 2 using {extendr}.\n\nI tend to always do just the first day of the advent of code. It is not my cup of tea. I don’t enjoy word problems, or sudoku, or the like. But I do like it when people learn. I find many people use this as a time to learn a new language.\nThis morning I did the Advent of Code Day 1 in both R and Rust. I’ll discuss my approaches to the challenges.\n\n\n\n\n\n\nImportant\n\n\n\nIf you care a lot about the Advent of Code and want to do it yourself, do not read any further. I am giving away the answers."
  },
  {
    "objectID": "posts/2024-12-01-advent-day-1/index.html#part-1",
    "href": "posts/2024-12-01-advent-day-1/index.html#part-1",
    "title": "🎄dvent Day 1: Rust and R solutions",
    "section": "Part 1",
    "text": "Part 1\nThe objective of part one is to calculate the distance between column 1 and column 2 in acending order. Note that the distance is in absolute values. This is not mentioned but I figured it out after my first submission was wrong.\nThe approach:\n\nread input\nsort each column independently\ncalculate the different from column 2 and column 1\ncalculate the absolute value\nsum it all up\n\nR\nThis was a one liner:\n\nsum(abs(do.call(`-`, lapply(read.table(\"day1.txt\"), sort))))\n\n[1] 2057374\n\n\nLet’s try rewriting it using a pipe so it can be a bit easier to process:\n\nread.table(\"day1.txt\") |&gt; \n  lapply(sort) |&gt; \n  do.call(`-`, args = _) |&gt; \n  abs() |&gt; \n  sum()\n\n[1] 2057374\n\n\nThere are two things here that may be novel to you. The first is that we can use lapply() with a data.frame.\nTo quote myself:\n\n“Data frames are actually just lists masquerading as rectangles.”\n\nSource: Finding and SPSS {haven}\nThis returns a list where each element is the sorted input vector.\nNext, we can compute the different between the two columns by using do.call() with the function being -. do.call() takes a list of arguments and splices them into the function call.\nSince our funciton, -, has two arguments it works perfectly. Then we wrap the results in sum(abs()) and voila.\nRust\nThe hardest part of the rust solution is reading the file to be completely honest. I’m still terrible with using readers in Rust so I used ChatGPTs help. I’m not going to lie about it.\nReading the input\nThe first thing to note is that we are returning Result&lt;(Vec&lt;i32&gt;, Vec&lt;i32&gt;), Box&lt;dyn Error&gt;&gt; from the function. We return a Result&lt;&gt; because there are multiple places where the function can error. Using a Result&lt;&gt; gives us the ability to unwrap anything inside of the body of the function that is in a Result&lt;&gt; itself. If there is an error, it will be returned—thus, “gracefully” handling the errors.\nTypically, if you’re a Rust hardo, you will define your own custom Error type. That is too much work for me—and I’m not good at knowing all of the types of errors that I may want. Instead we use Box&lt;dyn Error&gt;. Box&lt;dyn Error&gt; is a fancy way of saying we can accept anything that implements the Error trait.\nNext it is important to use a BufReader which allows us to read the file line by line. Always use a BufReader when possible. It will make your code so much faster.\nNext, we are going to instantiate two vectors that we will use to store the results. Then we iterate through the lines of the reader and parse the contents and shove them into the vector. Voila.\n\nuse std::error::Error;\nuse std::fs::File;\nuse std::io::{BufRead, BufReader};\n\nfn read_day1(path: &str) -&gt; Result&lt;(Vec&lt;i32&gt;, Vec&lt;i32&gt;), Box&lt;dyn Error&gt;&gt; {\n    let file = File::open(path)?;\n    let reader = BufReader::new(file);\n\n    // Create two vectors to hold the integers\n    let mut vec1 = Vec::new();\n    let mut vec2 = Vec::new();\n\n    // Read the file line by line\n    for line in reader.lines() {\n        let line = line?;\n        let mut nums = line\n            .split_whitespace() // Split by whitespace\n            .map(|s| s.parse::&lt;i32&gt;()); // Parse each number into i32\n\n        // Collect the numbers into the two vectors\n        if let (Some(Ok(num1)), Some(Ok(num2))) = (nums.next(), nums.next()) {\n            vec1.push(num1);\n            vec2.push(num2);\n        } else {\n            eprintln!(\"Skipping malformed line: {}\", line);\n        }\n    }\n\n    Ok((vec1, vec2))\n}\n\nSorting and summing\nNext, we define a little handy wrapper function. We can use destructuring assignment here to put the results of read_day1() into two items at once. If you’re an R user, this is like using {dotty} or {zeallot}. My preference is for dotty, personally.\n\npub fn day_one_part_one(path: &str) -&gt; Result&lt;i32, Box&lt;dyn Error&gt;&gt; {\n    // read the input. needs to be mutable to sort\n    let (mut x, mut y) = read_day1(path)?;\n\n    // sort the input\n    x.sort();\n    y.sort();\n\n    // calculate the sum\n    let res = x.iter().zip(y.iter()).fold(0, |mut acc, (xx, yy)| {\n        acc += (yy - xx).abs();\n        acc\n    });\n\n    Ok(res)\n}\n\nWe iterate through x and why by creating a zipped iterator. When you zip an iterator you get a tuple of elements. We will iterate through these two items together and calculate the absolute difference and accumulate it along the way.\nWe accumulate the results using .fold() which takes two arguments :\n\nThe initial value to accumulate\nA closure that has two arguments:\nThe accumulating value\nThe current value of the iterator\n\nA closure is like an anonymous function in R that is defined like \\(.x, .y) or using the purrr tilde syntax like ~ .x + .y.\nIt is also important that the closure must return the same type as the initial value.\nIn our closure we say that the acc (you can choose any name you’d like here, it is just a function argument) must be mutable so we can change its value at each step. We use the shortcut += operator so that we dont have to write acc = acc + (yy - xx).abs().\nAll together\nSince these are just functions, we need to wrap them all up in our main.rs file.\n\n\n\nmain.rs\n\nuse std::error::Error;\n\nfn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let d1p1 = day_one_part_one(\"input/day1.txt\")?;\n    println!(\"Part 1: {d1p1}\");\n    Ok(())\n}"
  },
  {
    "objectID": "posts/2024-12-01-advent-day-1/index.html#part-2",
    "href": "posts/2024-12-01-advent-day-1/index.html#part-2",
    "title": "🎄dvent Day 1: Rust and R solutions",
    "section": "Part 2",
    "text": "Part 2\nPart two was quite fun to do, actually. For it, we want to count the number of times that each value in the second column occurs. Each of these values correspond to a value in the first column. Our sum is now the value in column 1 multiplied by the number of times it occurs in column two. To approach this we will do the following:\n\nread the input\ncount the number of times each value in column 2 occurs\ncalculate the “score” for each value in column 1\nsum up the scores\n\nR\nThe R solution is quite straight forward as well but again, might use techniques you’re not familiar with. Here is the solution in all of its (surprisingly fast) glory.\nYou may think it is ugly but I assure you, it is very fast.\n\nx &lt;- read.table(\"day1.txt\")\nsum(x$V1 * table(x$V2)[as.character(x$V1)], na.rm = TRUE)\n\n[1] 23177084\n\n\nLet’s break it up. The most important part is the table() call. This calculates how many times each value in x$V2 occurs. We can use this table as a lookup vector.\n\nlookup &lt;- table(x$V2)\n\nUsing a lookup vector is a very efficient approach that people tend to not think about. Since this is a named vector, we can extract it’s elements by name.\n\nlookup[\"92252\"]\n\n92252 \n   19 \n\n\nNow, all we need to do is do this for every value in x$V1. We have to cast x$V1 as a character vector otherwise it will attempt to do the lookup by position.\n\nx_counts &lt;- lookup[as.character(x$V1)]\ntail(x_counts)\n\n\n &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; 61539 \n                                 11 \n\n\nIf there is not any occurrences in x$V2 the value is NA which is very handy because an NA just like a 0 will propagate in multiplication. All we need to do now is multiple and sum!\n\nsum(x$V1 * x_counts, na.rm = TRUE)\n\n[1] 23177084\n\n\nRust\nI quite enjoyed writing this rust solution—frankly more than either R or Rust solution. Any time I get to use a BtreeMap I’m giddy.\nCounting unique values in Rust is a little bit different. We typically use a Map of some variety. Think of these as named lists. Typically you will hear reference about a HashMap. HashMap are key-value stores that do not have any sense of order in the key. BTreeMap is different because the key must be ordered. Since we will be performing a lookup based on an integer value, I feel BTreeMap may be better here—though only bench marks can prove it one way or another.\nHere is the solution:\n\npub fn day_one_part_two(path: &str) -&gt; Result&lt;i32, Box&lt;dyn Error&gt;&gt; {\n    // read the inputs\n    let (x, y) = read_day1(path)?;\n\n    // Create an empty BTreeMap to count the occurrences\n    let mut counts = BTreeMap::new();\n\n    // Iterate through Y to populate the BTreeMap and increment\n    // each time we see an entry\n    for yi in y {\n        let entry = counts.entry(yi).or_insert(0);\n        *entry += 1;\n    }\n\n    // Iterate through x and get the value from the btreemap.\n    // if it doesn't exist we get use the default value of 0\n    // that is what the unwrap_or() is for\n    let res = x.iter().fold(0, |mut acc, next| {\n        let multiplier = counts.get(next).unwrap_or(&0);\n        acc += next * multiplier;\n        acc\n    });\n\n    Ok(res)\n}\n\nWe instantiate an empty BTreeMap then we populate it. We do this using the below code. This will grab the entry with the key yi from the map. If it doesn’t exist, it will insert the value 0. Then we add the value 1 to it. Notice that *entry. We do this because we are assinging to a mutable reference. This lets the value inside of the counts BTreeMap be updated.\n\n  for yi in y {\n      let entry = counts.entry(yi).or_insert(0);\n      *entry += 1;\n  }\n\nThe next part is quite like our part 1 solution. We use .fold() to perform the sum for us. We iterate through each value of x—stored in the value of next in the closure. We then try and get the lookup value from our counts map. If there is no associated value, we provide a value of 0 and store it in our multiplier variable. Then we multiply xi (or next in the closure) and add it the the accumulator!\n\nlet res = x.iter().fold(0, |mut acc, next| {\n    let multiplier = counts.get(next).unwrap_or(&0);\n    acc += next * multiplier;\n    acc\n});\n\nThat’s it. While it is much more code, it feels much easier to read and a bit cleaner than the R solution."
  },
  {
    "objectID": "posts/2024-12-01-advent-day-1/index.html#tldr",
    "href": "posts/2024-12-01-advent-day-1/index.html#tldr",
    "title": "🎄dvent Day 1: Rust and R solutions",
    "section": "",
    "text": "I give you the solutions to Day 1 of Advent of Code in R, Rust, and Rust in R for part 2 using {extendr}.\n\nI tend to always do just the first day of the advent of code. It is not my cup of tea. I don’t enjoy word problems, or sudoku, or the like. But I do like it when people learn. I find many people use this as a time to learn a new language.\nThis morning I did the Advent of Code Day 1 in both R and Rust. I’ll discuss my approaches to the challenges.\n\n\n\n\n\n\nImportant\n\n\n\nIf you care a lot about the Advent of Code and want to do it yourself, do not read any further. I am giving away the answers."
  },
  {
    "objectID": "posts/2024-12-01-advent-day-1/index.html#rextendr",
    "href": "posts/2024-12-01-advent-day-1/index.html#rextendr",
    "title": "🎄dvent Day 1: Rust and R solutions",
    "section": "Bonus: R + Rust via {rextendr}\n",
    "text": "Bonus: R + Rust via {rextendr}\n\nWe can take the part 2 solution and tidy it up into a Rust function that can be called from R using rextendr.\n\n\n\n\n\n\nNote\n\n\n\nThis isn’t optimized to be fast code and we’re not even using R native types so we will incur an overhead cost to go from integer() vector to Vec&lt;i32&gt;.\n\n\nTo do this you will need rextendr installed. Do so with pak::pak(\"extendr/rextendr\").\n\ntally_code &lt;- r\"-{\n  fn tally_day1(x: Vec&lt;i32&gt;, y: Vec&lt;i32&gt;) -&gt; Result&lt;i32&gt; {\n    let mut counts = std::collections::BTreeMap::new();\n    for yi in y {\n        let entry = counts.entry(yi).or_insert(0);\n        *entry += 1;\n    }\n\n    let res = x.iter().fold(0, |mut acc, next| {\n        let multiplier = counts.get(next).unwrap_or(&0);\n        acc += next * multiplier;\n        acc\n    });\n\n    Ok(res)\n}\n\n}-\"\n\nrextendr::rust_function(\n  tally_code, \n  profile = \"release\",\n  quiet = TRUE\n)\n\nNow we can call this code directly from R:\n\ntally_day1(x$V1, x$V2)\n\n[1] 23177084\n\n\nLet’s perform a small bench mark between this and the R solution:\n\nbench::mark(\n  r = sum(x$V1 * table(x$V2)[as.character(x$V1)], na.rm = TRUE),\n  rust_simple = tally_day1(x$V1, x$V2)\n)\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 r            353.1µs  376.5µs     2453.  226.78KB     10.5\n2 rust_simple   51.6µs   62.4µs    14025.    4.84KB      0"
  },
  {
    "objectID": "posts/2024-12-01-advent-day-1/index.html#rust-solutioncode",
    "href": "posts/2024-12-01-advent-day-1/index.html#rust-solutioncode",
    "title": "🎄dvent Day 1: Rust and R solutions",
    "section": "Rust solutioncode",
    "text": "Rust solutioncode\nBelow is all of the code I used for the rust solution.\n\n\nmain.rs\nday1.rs\n\n\n\n\n\n\nmain.rs\n\nuse std::error::Error;\n\nmod day1;\nuse day1::*;\nfn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let d1p1 = day_one_part_one(\"input/day1.txt\")?;\n    let d1p2 = day_one_part_two(\"input/day1.txt\")?;\n\n    println!(\"Day 1 results:\\n  Part 1: {d1p1}\\n  Part 2: {d1p2}\");\n    Ok(())\n}\n\n\n\n\n\n\n\nday1.rs\n\nuse std::collections::BTreeMap;\nuse std::error::Error;\nuse std::fs::File;\nuse std::io::{BufRead, BufReader};\n\nfn read_day1(path: &str) -&gt; Result&lt;(Vec&lt;i32&gt;, Vec&lt;i32&gt;), Box&lt;dyn Error&gt;&gt; {\n    let file = File::open(path)?;\n    let reader = BufReader::new(file);\n\n    // Create two vectors to hold the integers\n    let mut vec1 = Vec::new();\n    let mut vec2 = Vec::new();\n\n    // Read the file line by line\n    for line in reader.lines() {\n        let line = line?;\n        let mut nums = line\n            .split_whitespace() // Split by whitespace\n            .map(|s| s.parse::&lt;i32&gt;()); // Parse each number into i32\n\n        // Collect the numbers into the two vectors\n        if let (Some(Ok(num1)), Some(Ok(num2))) = (nums.next(), nums.next()) {\n            vec1.push(num1);\n            vec2.push(num2);\n        } else {\n            eprintln!(\"Skipping malformed line: {}\", line);\n        }\n    }\n\n    Ok((vec1, vec2))\n}\n\npub fn day_one_part_one(path: &str) -&gt; Result&lt;i32, Box&lt;dyn Error&gt;&gt; {\n    // read the input. needs to be mutable to sort\n    let (mut x, mut y) = read_day1(path)?;\n\n    // sort the input\n    x.sort();\n    y.sort();\n\n    // calculate the sum\n    let res = x.iter().zip(y.iter()).fold(0, |mut acc, (xx, yy)| {\n        acc += (yy - xx).abs();\n        acc\n    });\n\n    Ok(res)\n}\n\npub fn day_one_part_two(path: &str) -&gt; Result&lt;i32, Box&lt;dyn Error&gt;&gt; {\n    // read the inputs\n    let (x, y) = read_day1(path)?;\n\n    // Create an empty BTreeMap to count the occurrences\n    let mut counts = BTreeMap::new();\n\n    // Iterate through Y to populate the BTreeMap and increment\n    // each time we see an entry\n    for yi in y {\n        let entry = counts.entry(yi).or_insert(0);\n        *entry += 1;\n    }\n\n    // Iterate through x and get the value from the btreemap.\n    // if it doesn't exist we get use the default value of 0\n    // that is what the unwrap_or() is for\n    let res = x.iter().fold(0, |mut acc, next| {\n        let multiplier = counts.get(next).unwrap_or(&0);\n        acc += next * multiplier;\n        acc\n    });\n\n    Ok(res)\n}"
  },
  {
    "objectID": "posts/2024-12-01-advent-day-1/index.html#rust-solution-code",
    "href": "posts/2024-12-01-advent-day-1/index.html#rust-solution-code",
    "title": "🎄dvent Day 1: Rust and R solutions",
    "section": "Rust solution code",
    "text": "Rust solution code\nBelow is all of the code I used for the rust solution.\n\n\nmain.rs\nday1.rs\n\n\n\n\n\n\nmain.rs\n\nuse std::error::Error;\n\nmod day1;\nuse day1::*;\nfn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {\n    let d1p1 = day_one_part_one(\"input/day1.txt\")?;\n    let d1p2 = day_one_part_two(\"input/day1.txt\")?;\n\n    println!(\"Day 1 results:\\n  Part 1: {d1p1}\\n  Part 2: {d1p2}\");\n    Ok(())\n}\n\n\n\n\n\n\n\nday1.rs\n\nuse std::collections::BTreeMap;\nuse std::error::Error;\nuse std::fs::File;\nuse std::io::{BufRead, BufReader};\n\nfn read_day1(path: &str) -&gt; Result&lt;(Vec&lt;i32&gt;, Vec&lt;i32&gt;), Box&lt;dyn Error&gt;&gt; {\n    let file = File::open(path)?;\n    let reader = BufReader::new(file);\n\n    // Create two vectors to hold the integers\n    let mut vec1 = Vec::new();\n    let mut vec2 = Vec::new();\n\n    // Read the file line by line\n    for line in reader.lines() {\n        let line = line?;\n        let mut nums = line\n            .split_whitespace() // Split by whitespace\n            .map(|s| s.parse::&lt;i32&gt;()); // Parse each number into i32\n\n        // Collect the numbers into the two vectors\n        if let (Some(Ok(num1)), Some(Ok(num2))) = (nums.next(), nums.next()) {\n            vec1.push(num1);\n            vec2.push(num2);\n        } else {\n            eprintln!(\"Skipping malformed line: {}\", line);\n        }\n    }\n\n    Ok((vec1, vec2))\n}\n\npub fn day_one_part_one(path: &str) -&gt; Result&lt;i32, Box&lt;dyn Error&gt;&gt; {\n    // read the input. needs to be mutable to sort\n    let (mut x, mut y) = read_day1(path)?;\n\n    // sort the input\n    x.sort();\n    y.sort();\n\n    // calculate the sum\n    let res = x.iter().zip(y.iter()).fold(0, |mut acc, (xx, yy)| {\n        acc += (yy - xx).abs();\n        acc\n    });\n\n    Ok(res)\n}\n\npub fn day_one_part_two(path: &str) -&gt; Result&lt;i32, Box&lt;dyn Error&gt;&gt; {\n    // read the inputs\n    let (x, y) = read_day1(path)?;\n\n    // Create an empty BTreeMap to count the occurrences\n    let mut counts = BTreeMap::new();\n\n    // Iterate through Y to populate the BTreeMap and increment\n    // each time we see an entry\n    for yi in y {\n        let entry = counts.entry(yi).or_insert(0);\n        *entry += 1;\n    }\n\n    // Iterate through x and get the value from the btreemap.\n    // if it doesn't exist we get use the default value of 0\n    // that is what the unwrap_or() is for\n    let res = x.iter().fold(0, |mut acc, next| {\n        let multiplier = counts.get(next).unwrap_or(&0);\n        acc += next * multiplier;\n        acc\n    });\n\n    Ok(res)\n}"
  },
  {
    "objectID": "posts/2025-04-10-tomledit.html",
    "href": "posts/2025-04-10-tomledit.html",
    "title": "Create and edit TOML in R with {tomledit}",
    "section": "",
    "text": "{tomledit} v0.1.1 has found its way onto CRAN.\ntomledit is a package for creating and editing TOML files from R with support for reading as well.\nThe most basic use of tomledit is via toml(). toml() creates a Toml object from named arguments passed to ....\nlibrary(tomledit)\n\ntoml(person = list(age = 30L, name = \"Wilma\"))\n\n&lt;Toml&gt;\n[person]\nage = 30\nname = \"Wilma\""
  },
  {
    "objectID": "posts/2025-04-10-tomledit.html#v0.1.1-features",
    "href": "posts/2025-04-10-tomledit.html#v0.1.1-features",
    "title": "Create and edit TOML in R with {tomledit}",
    "section": "v0.1.1 Features",
    "text": "v0.1.1 Features\nThis newest release supports the use of arrays with inline tables. This feature comes as a request from @dpastoor to support the experimental rproject.toml file for rv.\n\n\nI’m bullish on rv as a new alternative to renv. I think it will be a great addition to the R community.\nThis new feature allows us to have a list of unnamed lists inside of our TOML.\nBelow we create an item called repositories which is an array of inline tables containing the alias and url to a CRAN-like repository.\nSimilarly, the dependencies item is an array of both inline-tables and strings. This new feature adds more flexibility to the type of TOML that we can create.\n\nr_proj_toml &lt;- list(\n  name = \"upgrade\", \n  r_version = \"4.4\",\n  repositories = list(\n    list(alias = \"gh-pkg-mirror\", url = \"https://a2-ai.github.io/gh-pkg-mirror/2024-02-22\"),\n    list(alias = \"RSPM\", url = \"https://packagemanager.posit.co/cran/2024-02-22\"),\n    list(alias = \"new-mirror\", url = \"https://a2-ai.github.io/gh-pkg-mirror/2024-12-04\"),\n    list(alias = \"new-rspm\", url = \"https://packagemanager.posit.co/cran/2024-12-04\")\n  ),\n  dependencies = list(\n    list(name = \"pmplots\", repository = \"new-mirror\"),\n    \"pmtables\",\n    \"bbr\",\n    list(name = \"ggplot2\", repository = \"new-rspm\")\n  )\n) \n\nas_toml(list(project = r_proj_toml))\n\n&lt;Toml&gt;\n[project]\nname = \"upgrade\"\nr_version = \"4.4\"\nrepositories = [\n    { alias = \"gh-pkg-mirror\", url = \"https://a2-ai.github.io/gh-pkg-mirror/2024-02-22\" },\n    { alias = \"RSPM\", url = \"https://packagemanager.posit.co/cran/2024-02-22\" },\n    { alias = \"new-mirror\", url = \"https://a2-ai.github.io/gh-pkg-mirror/2024-12-04\" },\n    { alias = \"new-rspm\", url = \"https://packagemanager.posit.co/cran/2024-12-04\" }\n]\ndependencies = [\n    { name = \"pmplots\", repository = \"new-mirror\" },\n    \"pmtables\",\n    \"bbr\",\n    { name = \"ggplot2\", repository = \"new-rspm\" }\n]"
  },
  {
    "objectID": "posts/2025-04-24-building-for-webr.html",
    "href": "posts/2025-04-24-building-for-webr.html",
    "title": "Building local packages for WebR",
    "section": "",
    "text": "We’ve recently added WebR support for extendr.\nTypically, folks rely on R-universe to build their packages for WebR. However, compiled packages require a bit more work. Figuring out how to build a package for WebR locally was surprsingly confusing and also very straightfoward.\nI’ve releaned how to do this a few times now. Here is how it is done:"
  },
  {
    "objectID": "posts/2025-04-24-building-for-webr.html#how-to-build-for-webr",
    "href": "posts/2025-04-24-building-for-webr.html#how-to-build-for-webr",
    "title": "Building local packages for WebR",
    "section": "How to build for WebR",
    "text": "How to build for WebR\nThe easiest way to build a local package for WebR is using the Docker image.\nTo do so, cd into your R package you want to build. Then spin up the Docker image and then enter R from inside of the container.\ndocker run -it --rm -v ${PWD}/output:/output -w /output ghcr.io/r-wasm/webr:main\nR\nThen from R inside of the container, run rwasm::build(\".\").\nThat’s it!\nThanks to George Stagg for helping with this!"
  }
]