[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "Welcome to my personal website. I always aspire to keep pumping out content, but I don’t.\nHere you will find my “recent” blog posts, possibly outdated biographical information, my socials, and links to projects I have made or worked on.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "employment: Senior Product Engineer @ Esri\neducation:\n\nMS Urban Informatic, Northeastern University (2020)\nBA Sociology, Plymouth State University\n\nMinor, General Mathematics\nProfessional Certificate GIS\nI am a Senior Product Engineer on the Spatial Analysis team at Esri. Previously, I was at The NPD Group as a Research Analyst where I worked to modernize our data science infrastructure to use Databricks, Docker, and Spark. Before that, I was at RStudio, PBC on the customer success team enabling public sector adoption of data science tools. In 2020 I received my master’s degree in Urban Informatics from Northeastern University following my bachelor’s degree in sociology with focuses in geographic information systems and general mathematics from Plymouth State University in 2018."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "recent posts",
    "section": "",
    "text": "Feeling rusty: counting characters\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nRust traits for R users\n\n\nand how they’ll make your package better\n\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nlearning rust\n\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nJHU talk (slides)\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nRaw strings in R\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nProgramatically Create Formulas in R\n\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nYouTube Videos & what not\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nFishnets and overlapping polygons\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nComplete spatial randomness\n\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nspacetime representations aren’t good—yet\n\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nMake your R scripts Databricks notebooks\n\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nExploratory Spatial Data Analysis in R\n\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nMy new IDE theme: xxEmoCandyLandxx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nActually identifying R package System Requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nThe heck is a statistical moment??\n\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nInstalling Python on my M1 in under 10 minutes\n\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nSLICED! a brief reflection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\n{cpcinema} & associated journey\n\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nOSINT in 7 minutes\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nAPIs: the language agnostic love story\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nPython & R in production — the API way\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nColor Palette Cinema\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nSecure R Package Environments\n\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nWhat is critical race theory, anyways?\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nDemographic Change, White Fear, and Social Construction of Race\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nMedium Data and Production API Pipeline\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nThe Red Queen Effect\n\n\nUnderstanding the Narrow Corridor\n\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nExcel in pRod\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nDesign Paradigms in R\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nR Security Concerns and 2019 CRAN downloads\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nNon-interactive user tokens with googlesheets4\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nFinding an SPSS {haven}\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nIntro to Tidy Modeling\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nWater Quality Analysis\n\n\nResource created for training at EPA\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\n∑ { my parts }\n\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nGoogle Trends for Campaigns\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nWeb-scraping for Campaigns\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing trendyy\n\n\nA tidy wrapper for gtrendsR\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\ngenius tutorial\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\ngenius Plumber API\n\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nThe Fallacy of one person, one vote\n\n\nQuantifying constituency representation\n\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nThe Cost of Gridlock\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nxgboost feature importance\n\n\nExtracting and plotting feature importance\n\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\n[Not so] generic functions\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nUS Representation: Part I\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing: Letters to a layperson\n\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nChunking your csv\n\n\nWriting data subsets\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nReading Multiple csvs as 1 data frame\n\n\nReading chunked csv files\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nCoursera R-Programming: Week 2 Problems\n\n\nThe Tidy Approach\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing geniusR\n\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-11-13-making-my-quarto-site.html",
    "href": "posts/2022-11-13-making-my-quarto-site.html",
    "title": "Creating my quarto blog",
    "section": "",
    "text": "The documentation is thin. The Google brain hasn’t yet propagated the best blog posts to the top, yet.\nThe github discussions use the words “just” and “simply” quite a bit as if it’s easy and simple. Like you should have just known to do that. It’s so obvious, clearly!\nhttps://github.com/quarto-dev/quarto-cli/discussions/2949#discussioncomment-3924396"
  },
  {
    "objectID": "posts/a-post.html",
    "href": "posts/a-post.html",
    "title": "Creating my quarto blog",
    "section": "",
    "text": "more stuff\nThis really feels a lot more like the general programming exerpeicen I’ve gotten in every language that isn’t R.\nThe documentation is thin. The Google brain hasn’t yet propagated the best blog posts to the top, yet.\nThe github discussions use the words “just” and “simply” quite a bit as if it’s easy and simple. Like you should have just known to do that. It’s so obvious, clearly!\nhttps://github.com/quarto-dev/quarto-cli/discussions/2949#discussioncomment-3924396\nhttps://github.com/quarto-dev/quarto-cli/discussions/2389#discussioncomment-3613262"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Home Page",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "Packages & contributions\n\n\n\n\n\n\n\nR package system requirements\n\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenius (retired)\n\n\n\npackage\n\n\nmusic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nh3o for H3 indexing\n\n\n\nspatial\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspdep (contributor)\n\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nThings I’ve written\n\n\n\n\n\n\n\nR for Progressive Campaigns\n\n\n\nwriting\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban Informatics Toolkit\n\n\n\nwriting\n\n\nurban-informatics\n\n\ntextbook\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmirrr - song genre classification\n\n\n\nwriting\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/csr.html",
    "href": "posts/csr.html",
    "title": "Complete spatial randomness",
    "section": "",
    "text": "< this is a cliche about Tobler’s fist law and things being related in space>. Because of Tobler’s first law, spatial data tend to not follow any specific distribution. So, p-values are sort of…not all that accurate most of the time. P-values in spatial statistics often take a “non-parametric” approach instead of an “analytical” one.\nConsider the t-test. T-tests make the assumption that data are coming from a normal distribution. Then p-values are derived from the cumulative distribution function. The alternative hypothesis, then, is that the true difference in means is not 0.\nIn the spatial case, our alternative hypothesis is generally “the observed statistic different than what we would expect under complete spatial randomness?” But what really does that mean? To know, we have to simulate spatial randomness.\nThere are two approaches to simulating spatial randomness that I’ll go over. One is better than the other. First, I’m going to describe the less good one: bootstrap sampling.\nLoad the super duper cool packages. We create queen contiguity neighbors and row-standardized weights.\nLet’s generate some spatially autocorrelated data. This function is a little slow, but it works."
  },
  {
    "objectID": "posts/csr.html#bootstrap-sampling",
    "href": "posts/csr.html#bootstrap-sampling",
    "title": "Complete spatial randomness",
    "section": "Bootstrap sampling",
    "text": "Bootstrap sampling\nUnder the bootstrap approach we are sampling from existing spatial configurations. In our case there are 144 existing neighborhoods. For our simulations, we will randomly sample from existing neighborhoods and then recalculate our statistic. It helps us by imposing randomness into our statistic. We can then repeat the process nsim times. There is a limitation, however. It is that there are only n - 1 possible neighborhood configurations per location.\nHere we visualize a random point and it’s neighbors.\n\n# for a given location create vector indicating position of\n# neighbors and self\ncolor_block <- function(i, nb) {\n  res <- ifelse(1:length(nb) %in% nb[[i]], \"neighbors\", NA)\n  res[i] <- \"self\"\n  res\n}\n\nPlot a point and its neighbors\n\ngrid |> \n  mutate(block = color_block(sample(1:n(), 1), nb)) |> \n  ggplot(aes(fill = block)) +\n  geom_sf() +\n  labs(title = \"Point and it's neighbors\")\n\n\n\n\nFor bootstrap we grab a point and then the neighbors from another point. This function will randomize a nb list object.\n\ncolor_sample_block <- function(i, nb) {\n  index <- 1:length(nb)\n  not_i <- index[-i]\n  \n  sample_block_focal <- sample(not_i, 1)\n  \n  res <- rep(NA, length(index))\n  \n  res[nb[[sample_block_focal]]] <- \"neighbors\"\n  res[i] <- \"self\"\n  res\n}\n\n# visualize it\ngrid |> \n  mutate(block = color_sample_block(sample(1:n(), 1), nb)) |> \n  ggplot(aes(fill = block)) +\n  geom_sf() +\n  labs(title = \"Point and random point's neighbors\")\n\n\n\n\nOften, we will want to create a reference distribution by creating a large number of simulations—typically 999. As the simulations increase in size, we are limited in the amount of samples we can draw. The number of neighborhoods becomes limiting!\nSay we want to look at income distribution in Boston and the only data we have is at the census tract level. I happen to know that Boston has 207 tracts. If we want to do 999 simulations, after the 206th simulation, we will likely have gone through all over the neighborhood configurations!\nHow can we do this sampling? For each observation, we can sample another location, grab their neighbors, and assign them as the observed location’s neighbors."
  },
  {
    "objectID": "posts/csr.html#bootstrap-simulations",
    "href": "posts/csr.html#bootstrap-simulations",
    "title": "Complete spatial randomness",
    "section": "Bootstrap simulations",
    "text": "Bootstrap simulations\nIn sfdep, we use spdep’s nb object. These are lists that store the row position of the neighbors as integer vectors at each element.\n\nIf you want to learn more about neighbors I gave a talk at NY Hackr MeetUp a few months ago that might help.\n\nHere I define a function that samples from the positions (index), then uses that sample to shuffle up the existing neighborhoods and return a shuffled nb object. Note that I add the nb class back to the list.\n\nbootstrap_nbs <- function(nb) {\n  # create index\n  index <- 1:length(nb)\n  # create a resampled index\n  resampled_index <- sample(index, replace = TRUE)\n  # shuffle the neighbors and reassign class\n  structure(nb[resampled_index], class = c(\"nb\", \"list\"))\n}\n\nLet’s compare some observations\n\nnb[1:3]\n\n[[1]]\n[1]  2 13 14\n\n[[2]]\n[1]  1  3 13 14 15\n\n[[3]]\n[1]  2  4 14 15 16\n\nbootstrap_nbs(nb)[1:3]\n\n[[1]]\n[1]  7  9 19 20 21\n\n[[2]]\n[1]  3  4  5 15 17 27 28 29\n\n[[3]]\n[1]  92  93  94 104 106 116 117 118\n\n\nHere we can see the random pattern. Look’s like there is fair amount of clustering of like values.\n\ngrid |> \n  mutate(x = classInt::classify_intervals(x, 7)) |> \n  ggplot(aes(fill = x)) +\n  geom_sf(color = NA, lwd = 0) +\n  scale_fill_brewer(type = \"div\", palette = 5, direction = -1) +\n  theme_void() \n\n\n\n\nWith the weights and the neighbors we can calculate the global Moran. I’ll refer to this as the “observed.” Store it into an object called obs. We’ll need this to calculate a simulated p-value later.\n\nobs <- global_moran(x, nb, wt)\nobs[[\"I\"]]\n\n[1] 0.346877\n\n\n0.35 is a fair amount of positive spatial autocorrelation indicating that like values tend to cluster. But is this due to random chance, or does it depend on where these locations are? Now that we have the observed value of Moran’s I, we can simulate the value under spatial randomness using the bootstrapped sampling. To do so, we bootstrap sample our neighbors, recalculate the weights and then the global Moran. Now, if you’ve read my vignette on conditional permutation, you know what is coming next. We need to create a reference distribution of the global Moran under spatial randomness. To do that, we apply our boot strap nsim times and recalculate the global Moran with each new neighbor list. I love the function replicate() for these purposes.\n\nnsim = 499 \n\n\nAlso, a thing I’ve started doing is assigning scalars / constants with an equals sign because they typically end up becoming function arguments.\n\n\nreps <- replicate(\n  nsim, {\n    nb_sim <- bootstrap_nbs(nb)\n    wt_sim <- st_weights(nb_sim)\n    global_moran(x, nb_sim, wt_sim)[[\"I\"]]\n  }\n)\n\nhist(reps, xlim = c(min(reps), obs[[\"I\"]]))\nabline(v = obs[[\"I\"]], lty = 2)\n\n\n\n\nBootstrap limitations\nThat’s all well and good, but let’s look at this a bit more. Since we’re using the bootstrap approach, we’re limited in the number of unique combinations that are possible. Let’s try something. Let’s calculate the spatial lag nsim times and find the number of unique values that we get.\n\nlags <- replicate(\n  nsim, {\n    # resample the neighbors list\n    nb_sim <- bootstrap_nbs(nb)\n    # recalculate the weights\n    wt_sim <- st_weights(nb_sim)\n    # calculate the lag\n    st_lag(x, nb_sim, wt_sim)\n  }\n)\n\n# cast from matrix to vector\nlags_vec <- as.numeric(lags)\n\n# how many are there?\nlength(lags_vec)\n\n[1] 71856\n\n# how many unique?\nlength(unique(lags_vec))\n\n[1] 144\n\n\nSee this? There are only 144 unique value! That isn’t much! Don’t believe me? Run table(lags_vec). For each location there are only a limited number of combinations that can occur."
  },
  {
    "objectID": "posts/csr.html#conditional-permutation",
    "href": "posts/csr.html#conditional-permutation",
    "title": "Complete spatial randomness",
    "section": "Conditional Permutation",
    "text": "Conditional Permutation\nNow, here is where I want to introduce what I view to be the superior alternative: conditional permutation. Conditional permutation was described by Luc Anselin in his seminal 1995 paper. The idea is that we hold an observation constant, then we randomly assign neighbors. This is like the bootstrap approach but instead of grabbing a random observation’s neighborhood we create a totally new one. We do this be assigning the neighbors randomly from all possible locations.\nLet’s look at how we can program this. For each location we need to sample from an index that excludes the observation’s position. Further we need to ensure that there are the same number of neighbors in each location (cardinality).\n\npermute_nb <- function(nb) {\n  # first get the cardinality\n  cards <- st_cardinalties(nb)\n  # instantiate empty list to fill\n  nb_perm <- vector(mode = \"list\", length = length(nb))\n  \n  # instantiate an index\n  index <- seq_along(nb)\n  \n  # iterate through and full nb_perm\n  for (i in index) {\n    # remove i from the index, then sample and assign\n    nb_perm[[i]] <- sample(index[-i], cards[i])\n  }\n  \n  structure(nb_perm, class = \"nb\")\n}\n\n\nnb[1:3]\n\n[[1]]\n[1]  2 13 14\n\n[[2]]\n[1]  1  3 13 14 15\n\n[[3]]\n[1]  2  4 14 15 16\n\npermute_nb(nb)[1:3]\n\n[[1]]\n[1] 68 71 51\n\n[[2]]\n[1]  27  44  26  35 127\n\n[[3]]\n[1]   6 134  48 136  70\n\n\nNow, let’s repeat the same exercise using conditional permutation.\n\nlags2 <- replicate(\n  nsim, {\n    nb_perm <- permute_nb(nb)\n    st_lag(x, nb_perm, st_weights(nb_perm))\n  }\n)\n\nlags2_vec <- as.numeric(lags2)\n  \nlength(unique(lags2_vec))\n\n[1] 71853\n\n\nThere are farrrrr more unique values. In fact, there is a unique value for each simulation - location pair. If we look at the histograms, the difference is even more stark. The conditional permutation approach actually begins to represent a real distribution.\n\npar(mfrow = c(1, 2))\nhist(lags_vec, breaks = 20) \nhist(lags2_vec, breaks = 20)\n\n\n\n\nSo, this is all for me to say that bootstrapping isn’t it for creating simulated distributions for which to calculate your p-values."
  },
  {
    "objectID": "posts/this-is-markdown.html",
    "href": "posts/this-is-markdown.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "Markdown File\nCan Quarto render normal markdown files? If so, I think my move will be to render all of my Rmd files in my og blog as markdown using either hugodown::md_document() or rmarkdown::github_format()."
  },
  {
    "objectID": "posts/2022-05-11-make-your-r-scripts-databricks-notebooks/index.html",
    "href": "posts/2022-05-11-make-your-r-scripts-databricks-notebooks/index.html",
    "title": "Make your R scripts Databricks notebooks",
    "section": "",
    "text": "I’ve never had a good reason to deviate from the canonical .R file extension until today.\nAs you may have seen over the past few month from my numerous rage-tweets and Databricks related threads, I’ve been doing a lot of work getting figuring out Databricks as an R user so we can get onboard with adoption here at NPD.\nOne of my biggest qualms about Databricks is that it’s tailored to their notebooks. The notebooks get magical superpowers that aren’t available anywhere else. Notebooks get root permissions, they have access to dbutils, and are the only thing that can actually be scheduled by Databricks outside of a jar file or SparkSQL code.\nI’ve spent quite a bit of time thinking about how we can schedule R scripts through a notebook. If you’re wondering, have the notebook kickoff the R script with a shell command.\nBut, alas, I’ve learned something today. If you connect your Git repo to Databricks through their \"Repos\", you can have your R scripts be accessible as notebooks with quite literally only two changes.\nFirst, R scripts need to have the less-preferable, though equally functional, file extension .r. Second, the first line of the script should be a comment that says # Databricks notebook source. And that’s it. Then once the git repo has been connected, it will recognize those as notebooks.\nIf you want to create cells in your code write a comment # COMMAND ----------—that’s 10 hyphens at the end.\nIf you create a file main.r which contains the body\n# Databricks notebook source\n\nprint(\"Hello world\")\n\n# COMMAND ---------\n\nprint(\"Databricks! I've figured you out—sorta....\")\n\n# COMMAND ---------\n\nprint(\"I feel powerful.\")\nYou will have an R script that is recognized as a notebook by Databricks that can be scheduled using Databricks’ scheduling wizard."
  },
  {
    "objectID": "posts/2018-11-15-introducing-letters-to-a-layperson.html",
    "href": "posts/2018-11-15-introducing-letters-to-a-layperson.html",
    "title": "Introducing: Letters to a layperson",
    "section": "",
    "text": "I have been in the world of academia for nearly five years now. During this time I’ve read countless scholarly journal articles that I’ve struggled to wrap my head around. The academic language is riddled with obfuscating words like “milieux” and “nexus” which are often used to explain relatively simple concepts in a not so simple language. I’ve had to train myself to understand the academic language and translate it to regular people (layperson) speak.\nThe academic dialect is often associated with the “elitist media” (see Chomsky) which has recently been blamed for creating a strong divide in American politics—as we’ve seen since the beginning of the 2016 presidential primaries. Many words, phrases, and ideas have been shrouded by this language barrier. I have been trying to break down this barrier for myself for years now. I feel like I’ve only made a small dent. I have been trying to educate myself, a layperson, on these phrases and concepts.\nAs an undergraduate student I studied sociology and anthropology, but I found that I was enamored with economics, political science, urban theory, data science, psychology, and other disciplines. Across these fields there are identical concepts represented by different words or phrases—an ever frustrating thing. This is a barrier to understanding these fields. You must know certain ideas, words, and histories to understand the content.\nI have been collecting notes on these ideas and often revisit them to remind myself of what they are, what they mean, and why they exist. These notes were created for a myself, a layperson.\nIn this series of forthcoming posts, I will write about concepts that I wish I knew better in a language that I can understand. I call this collection of posts Letters To a Layperson, inspired by the phenomenal book Letters to a Young Contrarian by Christopher Hitchens."
  },
  {
    "objectID": "posts/2018-11-28-function-methods.html",
    "href": "posts/2018-11-28-function-methods.html",
    "title": "[Not so] generic functions",
    "section": "",
    "text": "Lately I have been doing more of my spatial analysis work in R with the help of the sf package. One shapefile I was working with had some horrendously named columns, and naturally, I tried to clean them using the clean_names() function from the janitor package. But lo, an egregious error occurred. To this end, I officially filed my complaint as an issue. The solution presented was to simply create a method for sf objects.\nYeah, methods, how tough can those be? Apparently the process isn’t at all difficult. But figuring out the process? That was difficult. This post will explain how I went about the process for converting the clean_names() function into a generic (I’ll explain this in a second), and creating a method for sf and tbl_graph objects.\n\nThe Jargon\nOkay, I want to address the jargon. What the hell is a generic function, and what is a method? But first, I want to give a quick tl;dr on what a function is. I define as function as bit of code that takes an input, changes it in some way, and produces an output. Even simpler, a function takes an input and creates an output.\n\nGeneric Functions\nNow, what is a generic function? My favorite definition that I’ve seen so far comes from LispWorks Ltd (their website is a historic landmark, I recommend you give it a look for a reminder of what the internet used to be). They define a generic function as\n\na function whose behavior depends on the classes or identities of the arguments supplied to it.\n\nThis means that we have to create a function that looks at the class of an object and perform an operation based on the object class. That means if there is \"numeric\" or \"list\" object, they will be treated differently. These are called methods. Note: you can find the class of an object by using the class() function on any object.\n\n\nMethods\nTo steal from LispWorks Ltd again, a method is\n\npart of a generic function which provides information about how that generic function should behave [for] certain classes.\n\nThis means that a method is part of a generic function and has to be defined separately. Imagine we have a generic function called f with methods for list and numeric objects. The way that we would denote these methods is by putting a period after the function name and indicating the type of object the function is to be used on. These would look like f.list and f.numeric respectively.\nBut to save time you can always create a default method which will be dispatched (used) on any object that it hasn’t been explicitly told how to operate on (by a specific method).\nNow that the intuition of what generic functions and methods R, we can begin the work of actually creating them. This tutorial will walk through the steps I took in changing the clean_names() from a standard function into a generic function with methods for sf objects and tbl_graph objects from the sf and tidygraph packages respectively.\nA brief overview of the process:\n\nDefine the generic function\nCreate a default method\nCreate additional methods\n\nA quick note: The code that follows is not identical to that of the package. I will be changing it up to make it simpler to read and understand what is happening.\n\n\n\nThe Generic Method\nThe first step, as described above, is to create a generic function. Generic functions are made by creating a new function with the body containing only a call to the UseMethod() function. The only argument to this is the name of your generic function—this should be the same as the name of the function you are making. This tells R that you are creating a generic function. Additionally, you should add any arguments that will be necessary for your function. Here, there are two arguments: dat and case. These indicate the data to be cleaned and the preferred style for them to be cleaned according to.\nI am not setting any default values for dat to make it required, whereas I am setting case to \"snake\".\n\nclean_names <- function(dat, case = \"snake\") {\n  UseMethod(\"clean_names\")\n}\n\nNow we have created a generic function. But this function doesn’t know how to run on any given object types. In other words, there are no methods associated with it. To illustrate this try using the clean_names() function we just defined on objects of different types.\nclean_names(1) # numeric \nclean_names(\"test\") # character \nclean_names(TRUE) # logical \n\n#> [1] \"no applicable method for 'clean_names' applied to an object of class \\\"c('double', 'numeric')\\\"\"\n\n\n#> [1] \"no applicable method for 'clean_names' applied to an object of class \\\"character\\\"\"\n\n\n#> [1] \"no applicable method for 'clean_names' applied to an object of class \\\"logical\\\"\"\n\nThe output of these calls say no applicable method for 'x' applied to an object of [class]. In order to prevent this from happening, we can create a default method. A default method will always be used if the function doesn’t have a method for the provided object type.\n\n\nThe Default Method\nRemember that methods are indicated by writing function.method. It is also important to note that the method should indicate an object class. To figure out what class an object is you can use the class() function. For example class(1) tells you that the number 1 is “numeric”.\nIn this next step I want to create a default method that will be used on every object that there isn’t a method explicitly for. To do this I will create a function called clean_names.default.\nAs background, the clean_names() function takes a data frame and changes column headers to fit a given style. clean_names() in the development version is based on the function make_clean_names() which takes a character vector and makes each value match a given style (the default is snake, and you should only use snake case because everything else is wrong * sarcasm * ).\n\nlibrary(janitor)\n\nNow let’s see how this function works. For this we will use the ugliest character vector I have ever seen from the tests for clean_names() (h/t @sfirke for making this).\n\nugly_names <- c(\n  \"sp ace\", \"repeated\", \"a**^@\", \"%\", \"*\", \"!\",\n  \"d(!)9\", \"REPEATED\", \"can\\\"'t\", \"hi_`there`\", \"  leading spaces\",\n  \"€\", \"ação\", \"Farœ\", \"a b c d e f\", \"testCamelCase\", \"!leadingpunct\",\n  \"average # of days\", \"jan2009sales\", \"jan 2009 sales\"\n)\n\nugly_names\n#>  [1] \"sp ace\"            \"repeated\"          \"a**^@\"             \"%\"                \n#>  [5] \"*\"                 \"!\"                 \"d(!)9\"             \"REPEATED\"         \n#>  [9] \"can\\\"'t\"           \"hi_`there`\"        \"  leading spaces\"  \"€\"                \n#> [13] \"ação\"              \"Farœ\"              \"a b c d e f\"       \"testCamelCase\"    \n#> [17] \"!leadingpunct\"     \"average # of days\" \"jan2009sales\"      \"jan 2009 sales\"\n\nNow to see how this function works:\n\nmake_clean_names(ugly_names)\n#>  [1] \"sp_ace\"                 \"repeated\"               \"a\"                     \n#>  [4] \"percent\"                \"x\"                      \"x_2\"                   \n#>  [7] \"d_9\"                    \"repeated_2\"             \"cant\"                  \n#> [10] \"hi_there\"               \"leading_spaces\"         \"x_3\"                   \n#> [13] \"acao\"                   \"faroe\"                  \"a_b_c_d_e_f\"           \n#> [16] \"test_camel_case\"        \"leadingpunct\"           \"average_number_of_days\"\n#> [19] \"jan2009sales\"           \"jan_2009_sales\"\n\nTrès magnifique!\nThe body of the default method will take column names from a dataframe, clean them, and reassign them. Before we can do this, a dataframe is needed!\n\n# create a data frame with 20 columns\ntest_df <- as_tibble(matrix(sample(100, 20), ncol = 20))\n\n# makes the column names the `ugly_names` vector\nnames(test_df) <- ugly_names\n\n# print the data frame.\ntest_df\n#> # A tibble: 1 × 20\n#>   `sp ace` repeated `a**^@`   `%`   `*`   `!` `d(!)9` REPEATED `can\"'t` hi_\\th…¹   lea…²\n#>      <int>    <int>   <int> <int> <int> <int>   <int>    <int>    <int>    <int>   <int>\n#> 1       78       18      99    28    26    38      43       73       51       33      23\n#> # … with 9 more variables: `€` <int>, ação <int>, Farœ <int>, `a b c d e f` <int>,\n#> #   testCamelCase <int>, `!leadingpunct` <int>, `average # of days` <int>,\n#> #   jan2009sales <int>, `jan 2009 sales` <int>, and abbreviated variable names\n#> #   ¹​`hi_\\`there\\``, ²​`  leading spaces`\n#> # ℹ Use `colnames()` to see all variable names\n\nThe process for writing this function is:\n\ntake a dataframe\ntake the old column names and clean them\nreassign the column names as the new clean names\nreturn the object\n\n\nclean_names.default <- function(dat, case = \"snake\") { \n  # retrieve the old names\n  old_names <- names(dat)\n  # clean the old names\n  new_names <- make_clean_names(old_names, case = case)\n  # assign the column names as the clean names vector\n  names(dat) <- new_names\n  # return the data\n  return(dat)\n  }\n\nNow that the default method has been defined. Try running the function on our test dataframe!\n\nclean_names(test_df)\n#> # A tibble: 1 × 20\n#>   sp_ace repeated     a percent     x   x_2   d_9 repeated_2  cant hi_th…¹ leadi…²   x_3\n#>    <int>    <int> <int>   <int> <int> <int> <int>      <int> <int>   <int>   <int> <int>\n#> 1     78       18    99      28    26    38    43         73    51      33      23    27\n#> # … with 8 more variables: acao <int>, faroe <int>, a_b_c_d_e_f <int>,\n#> #   test_camel_case <int>, leadingpunct <int>, average_number_of_days <int>,\n#> #   jan2009sales <int>, jan_2009_sales <int>, and abbreviated variable names ¹​hi_there,\n#> #   ²​leading_spaces\n#> # ℹ Use `colnames()` to see all variable names\n\nOh, my gorsh. Look at that! We can try replicating this with a named vector to see how the default method dispatched on unknown objects!\n\n# create a vector with 20 elements\ntest_vect <- c(1:20)\n\n# name each element with the ugly_names vector \nnames(test_vect) <- ugly_names\n\n# try cleaning!\nclean_names(test_vect)\n#>                 sp_ace               repeated                      a \n#>                      1                      2                      3 \n#>                percent                      x                    x_2 \n#>                      4                      5                      6 \n#>                    d_9             repeated_2                   cant \n#>                      7                      8                      9 \n#>               hi_there         leading_spaces                    x_3 \n#>                     10                     11                     12 \n#>                   acao                  faroe            a_b_c_d_e_f \n#>                     13                     14                     15 \n#>        test_camel_case           leadingpunct average_number_of_days \n#>                     16                     17                     18 \n#>           jan2009sales         jan_2009_sales \n#>                     19                     20\n\nIt looks like this default function works super well with named objects! Now, we will broach the problem I started with, sf objects.\n\n\nsf method\nThis section will go over the process for creating the sf method. If you have not ever used the sf package, I suggest you give it a try! It makes dataframe objects with spatial data associated with it. This allows you to perform many of the functions from the tidyverse to spatial data.\nBefore getting into it, I want to create a test object to work with. I will take the test_df column, create longitude and latitude columns, and then convert it into an sf object. The details of sf objects is out of the scope of this post.\n\nlibrary(sf)\n\ntest_sf <- test_df %>%\n  # create xy columns\n  mutate(long = -80, \n         lat = 40) %>% \n  # convert to sf object \n  st_as_sf(coords = c(\"long\", \"lat\"))\n\n# converting geometry column name to poor style\nnames(test_sf)[21] <- \"Geometry\"\n\n# telling sf which column is now the geometry\nst_geometry(test_sf) <- \"Geometry\"\n\ntest_sf\n#> Simple feature collection with 1 feature and 20 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: -80 ymin: 40 xmax: -80 ymax: 40\n#> CRS:           NA\n#> # A tibble: 1 × 21\n#>   `sp ace` repeated `a**^@`   `%`   `*`   `!` `d(!)9` REPEATED `can\"'t` hi_\\th…¹   lea…²\n#>      <int>    <int>   <int> <int> <int> <int>   <int>    <int>    <int>    <int>   <int>\n#> 1       78       18      99    28    26    38      43       73       51       33      23\n#> # … with 10 more variables: `€` <int>, ação <int>, Farœ <int>, `a b c d e f` <int>,\n#> #   testCamelCase <int>, `!leadingpunct` <int>, `average # of days` <int>,\n#> #   jan2009sales <int>, `jan 2009 sales` <int>, Geometry <POINT>, and abbreviated\n#> #   variable names ¹​`hi_\\`there\\``, ²​`  leading spaces`\n#> # ℹ Use `colnames()` to see all variable names\n\nThe sf object has been created. But now how does our default method of the clean_names() function work on this object? There is only one way to know, try it.\nclean_names(test_sf)\n\nError in st_geometry.sf(x) : attr(obj, \"sf_column\") does not point to a geometry column. Did you rename it, without setting st_geometry(obj) <- \"newname\"?\nNotice how it fails. sf noticed that I changed the name of the geometry column without explicitly telling it I did so. Since the geometry column is almost always the last column of an sf object, we can use the make_clean_names() function on every column but the last one! To do this we will use the rename_at() function from dplyr. This function allows you rename columns based on their name or position, and a function that renames it (in this case, make_clean_names()).\nFor this example dataset, say I wanted to clean the first column. How would I do that? Note that the first column is called sp ace. How this works can be seen in a simple example. In the below function call we are using the rename_at() function (for more, go here), selecting the first column name, and renaming it using the make_clean_names() function.\n\nrename_at(test_df, .vars = vars(1), .funs = make_clean_names)\n#> # A tibble: 1 × 20\n#>   sp_ace repea…¹ `a**^@`   `%`   `*`   `!` `d(!)9` REPEA…² can\"'…³ hi_\\t…⁴   lea…⁵   `€`\n#>    <int>   <int>   <int> <int> <int> <int>   <int>   <int>   <int>   <int>   <int> <int>\n#> 1     78      18      99    28    26    38      43      73      51      33      23    27\n#> # … with 8 more variables: ação <int>, Farœ <int>, `a b c d e f` <int>,\n#> #   testCamelCase <int>, `!leadingpunct` <int>, `average # of days` <int>,\n#> #   jan2009sales <int>, `jan 2009 sales` <int>, and abbreviated variable names\n#> #   ¹​repeated, ²​REPEATED, ³​`can\"'t`, ⁴​`hi_\\`there\\``, ⁵​`  leading spaces`\n#> # ℹ Use `colnames()` to see all variable names\n\nNotice how only the first column has been cleaned. It went from sp ace to sp_ace. The goal is to replicate this for all columns except the last one.\nTo write the sf method, the above line of code can be adapted to select columns 1 through the number of columns minus 1 (so geometry isn’t selected). In order to make this work, we need to identify the second to last column—this will be supplied as the ending value of our selected variables.\n\nclean_names.sf <- function(dat, case = \"snake\") {\n  # identify last column that is not geometry\n  last_col_to_clean <- ncol(dat) - 1\n  # create a new dat object\n  dat <- rename_at(dat, \n                   # rename the first up until the second to last\n                   .vars = vars(1:last_col_to_clean), \n                   # clean using the make_clean_names\n                   .funs = make_clean_names)\n  return(dat)\n}\n\nVoilà! Our first non-default method has been created. This means that when an sf object is supplied to our generic function clean_names() it looks at the class of the object—class(sf_object)—notices it’s an sf object, then dispatches (uses) the clean_names.sf() method instead of the default.\n\nclean_names(test_sf)\n#> Simple feature collection with 1 feature and 20 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: -80 ymin: 40 xmax: -80 ymax: 40\n#> CRS:           NA\n#> # A tibble: 1 × 21\n#>   sp_ace repeated     a percent     x   x_2   d_9 repeated_2  cant hi_th…¹ leadi…²   x_3\n#>    <int>    <int> <int>   <int> <int> <int> <int>      <int> <int>   <int>   <int> <int>\n#> 1     78       18    99      28    26    38    43         73    51      33      23    27\n#> # … with 9 more variables: acao <int>, faroe <int>, a_b_c_d_e_f <int>,\n#> #   test_camel_case <int>, leadingpunct <int>, average_number_of_days <int>,\n#> #   jan2009sales <int>, jan_2009_sales <int>, Geometry <POINT>, and abbreviated\n#> #   variable names ¹​hi_there, ²​leading_spaces\n#> # ℹ Use `colnames()` to see all variable names\n\nHere we see that it worked exactly as we hoped. Every column but the last has been altered. This allows sf to name it’s geometry columns whatever it would like without disrupting it.\nShortly after this addition was added to the package I became aware of another type of object that had problems using clean_names(). This is the tbl_graph object from the tidygraph package from Thomas Lin Pederson.\n\n\ntbl_graph method\nIn issue #252 @gvdr noted that calling clean_names() on a tbl_graph doesn’t execute. Thankfully @Tazinho noted that you could easily clean the column headers by using the rename_all() function from dplyr.\nHere the solution was even easier than above. As a reminder, in order to make the tbl_graph method, we need to specify the name of the generic followed by the object class.\n\nclean_names.tbl_graph <- function(dat, case = \"snake\") { \n  # rename all columns\n  dat <- rename_all(dat, make_clean_names)\n  return(dat)\n  }\n\nIn order to test the function, we will need a graph to test it on. This example draws on the example used in the issue.\n\nlibrary(tidygraph)\n# create test graph to test clean_names\ntest_graph <- play_erdos_renyi(0, 0.5) %>% \n  # attach test_df as columns \n  bind_nodes(test_df)\n\ntest_graph\n#> # A tbl_graph: 1 nodes and 0 edges\n#> #\n#> # A rooted tree\n#> #\n#> # Node Data: 1 × 20 (active)\n#>   sp ace… repeat… `a**^@`   `%`   `*`   `!` `d(!)9` REPEAT… can\"'t… hi_\\th…   lead…\n#>     <int>   <int>   <int> <int> <int> <int>   <int>   <int>   <int>   <int>   <int>\n#> 1      78      18      99    28    26    38      43      73      51      33      23\n#> # … with 9 more variables: `€` <int>, ação <int>, Farœ <int>, `a b c d e f` <int>,\n#> #   testCamelCase <int>, `!leadingpunct` <int>, `average # of days` <int>,\n#> #   jan2009sales <int>, `jan 2009 sales` <int>\n#> #\n#> # Edge Data: 0 × 2\n#> # … with 2 variables: from <int>, to <int>\n\nHere we see that there is a graph with only 1 node and 0 edges (relations) with bad column headers (for more, visit the GitHub page). Now we can test this as well.\n\nclean_names(test_graph)\n#> # A tbl_graph: 1 nodes and 0 edges\n#> #\n#> # A rooted tree\n#> #\n#> # Node Data: 1 × 20 (active)\n#>   sp_ace repeat…     a percent     x   x_2   d_9 repeat…  cant hi_the… leadin…   x_3\n#>    <int>   <int> <int>   <int> <int> <int> <int>   <int> <int>   <int>   <int> <int>\n#> 1     78      18    99      28    26    38    43      73    51      33      23    27\n#> # … with 8 more variables: acao <int>, faroe <int>, a_b_c_d_e_f <int>,\n#> #   test_camel_case <int>, leadingpunct <int>, average_number_of_days <int>,\n#> #   jan2009sales <int>, jan_2009_sales <int>\n#> #\n#> # Edge Data: 0 × 2\n#> # … with 2 variables: from <int>, to <int>\n\nIt worked as anticipated!\n\n\nReview (tl;dr)\nIn the preceding sections we learned what generic functions and methods are. How to create a generic function, a default method, and methods for objects of different classes.\n\ngeneric function: “A generic function is a function whose behavior depends on the classes or identities of the arguments supplied to it”\ngeneric function method: “part of a generic function and which provides information about how that generic function should behave [for] certain classes”\n\nThe process to create a function with a method is to:\n\nCreate a generic function with:\n\nf_x <- function() { UseMethod(\"f_x\") }\n\nDefine the default method with:\n\nf_x.default <- function() { do something }\n\nDefine object class specific methods with:\n\nf_x.class <- function() { do something else}\n\n\n\nNotes\nIf you have not yet encountered the janitor package it will help you tremendously with various data cleaning processes. Clearly, clean_names() is my favorite function as it helps me enforce my preferred style (and the only). If you are not aware of “proper” R style, I suggest you read the style guide in Advanced R.\nWhile on the subject of Advanced R, I suggest you read the “Creating new methods and generics” section of it. I struggled comprehending it at first because I didn’t even know what a method was. However, if after reading this you feel like you want more, that’s the place to go.\nI’d like to thank @sfirke for being exceptionally helpful in guiding my contributions to the janitor package."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html",
    "title": "The Fallacy of one person, one vote",
    "section": "",
    "text": "On October 6, 2018, the US Senate voted 50–48 in favor of the appointment of Associate Justice Brett Kavanaugh. This led many pundits to point out a “disconnect” between the Senate and the body politic. The 50 senators who voted “yea” represent only 44% of the nation’s population. The year prior, Supreme Court Justice Neil Gorsuch was confirmed by 54 senators representing approximately 45% of the population. This trend of increasing control by a decreasing portion of the constituency has been attributed to a rise in partisanship.\nSince the mid 90’s Dr. Frances E. Lee has been developing a body of literature on Senate apportionment, and her book Sizing Up the Senate has become part of the current political milieu (Vox, CNN, New York Times). The book discusses, among many things, the relevant historical context surrounding the creation and organization of the Senate at the constitutional Convention. In her 1998 paper “The Consequences of Senate Apportionment for the Geographic Distribution of Federal Funds” (Lee, 1998), Dr. Lee describes the “representation index”, a measure to quantify the over- or underrepresentation of a state in the US senate. In the formulation described in the paper, “the index is simply the ratio of the state’s actual population to 1/50th of the nation’s population” (Lee, 1998). In the formulation described in the paper, “the index is simply the ratio of the state’s actual population to 1/50th of the nation’s population” (Lee, 1998). It is written mathematically as \\(\\frac{State \\ Population}{1/50 \\ * \\ US \\ Population}\\). This creates a number between \\((0, \\infty)\\). As it is put in Sizing up the Senate,\nMany of Lee’s analyses utilize this index, and it has proved useful in temporal comparisons and modeling. However, it does not seem immediately capable of effectively evaluating other legislative bodies such as the House of Representatives. Here I will put forth an adaptation of this measure. That measure will then be adjusted to evaluate the House of Representatives. The House model will then be generalized to fit any representative body."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#interpretibility",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#interpretibility",
    "title": "The Fallacy of one person, one vote",
    "section": "Interpretibility",
    "text": "Interpretibility\nThe representation index has three main shortcomings, each of which are simple to address. First, the index produces a counter-intuitive number. An index of greater than 1 indicates an underrepresented state, and vice versa. Second, the interpretation of a middle value of 1 is useful for the “one person, one vote” standard. But the index is a divergent measure where 1 is the middle with the bounds (0,∞). Often, when one thinks of divergence, it is from an origin, or 0. Third, the index has a lower limit of 0 and no upper bound. This inhibits comparisons in both directions.\nTo illustrate the point let’s take the populations of California and Wyoming based on 2010 Census figures.\n\n\n\n\nRegion\nPopulation\n\n\n\n\nWY\n563626\n\n\nCA\n37253956\n\n\nUS\n308745538\n\n\n\n\nIf we calculate the representation index for these places, we get:\n\n\n\n\nRegion\nPopulation\nRepresentation Index\n\n\n\n\nWY\n563626\n0.09\n\n\nCA\n37253956\n6.03\n\n\n\n\nIn this example, California has a representation index of 6. This means that it is vastly underrepresented, whereas Wyoming has an index value of nearly 0 meaning it is vastly overrepresented. To interpret this, we must remember that a larger value actually means less representation.\nBut if we invert our formula, we obtain a more informative number.\n\n\n\n\nRegion\nPopulation\nNew Rep. Index\n\n\n\n\nWY\n563626\n10.96\n\n\nCA\n37253956\n0.17\n\n\n\n\nIn this table it is clear that Wyoming is overrepresented and California is underrepresented. But still, in evaluating these numbers we are required to do the mental math to contextualize the divergence from a middle value. California has a value of 0.83 less than the one person, one vote standard. To handle this, we can center the score around 0 by simply subtracting 1.\n\n\n\n\nRegion\nPopulation\nRepresentation Index\nNew Rep. Index\n\n\n\n\nWY\n563626\n0.09\n9.96\n\n\nCA\n37253956\n6.03\n-0.83\n\n\n\n\nThus the formula for the new representation index is \\(\\dfrac{\\dfrac{1}{50} \\ * \\ US \\ Population}{State \\ Population} - 1\\). When the measures are compared, we see that the initial measure used by Dr. Lee emphasizes underrepresentation of California, whereas the measure I have suggested emphasises the overrepresentation of Wyoming. From here on I will refer to these as the underrepresentation index (URI) and the overrepresentation index (ORI), respectively.\nThe URI and ORI are informative, but both are biased in scale. The bounds of the URI are \\((0,\\infty)\\) and the ORI are \\((-1, \\infty)\\). A value is needed that can simultaneously demonstrate the over- and underrepresentation of a state.\nThe ORI can be altered slightly to create this balanced measure. By taking the natural logarithm of the ratio 1/50th of the US population to a state’s population, a divergent scale naturally occurs. When the ratio is equal to 1 (or adhering to the one person, one vote standard), the value becomes 0. When the denominator is less than the numerator (or when the state has a smaller share of population than its share of votes), the value is positive and vice versa. Thus we arrive at the formula \\(\\ln\\Bigg({\\dfrac{\\dfrac{1}{50} \\ * \\ US \\ Population}{State \\ Population}}\\Bigg)\\). The following table compares these three measures.\n\n\n\n\nRegion\nPopulation\nURI\nORI\nNew Rep. Index\n\n\n\n\nWY\n563626\n0.09\n9.96\n2.39\n\n\nCA\n37253956\n6.03\n-0.83\n-1.80\n\n\n\n\nThe new representation index can be generalized to the House of Representatives or any other representative body. In the following sections, the representation index is adapted to the House of Representatives, the California Assembly and Senate, and the New Hampshire House and Senate."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#representation-index-and-the-house-of-representatives",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#representation-index-and-the-house-of-representatives",
    "title": "The Fallacy of one person, one vote",
    "section": "Representation Index and the House of Representatives",
    "text": "Representation Index and the House of Representatives\nRepresentation in the House of Representative is proportional meaning that a state has a number of legislative representatives proportional to its population. For example, if a state were to have 50% of the nation’s population it should represent 50% of the legislative body. This is the principle that the representation index evaluates.\nIn the above adaptation of the representation index, the nation’s population is divided by 50. This would be the population of a single state if every state had the same number of citizens. Then, that number is scaled (divided) by the state’s actual population, and the logarithm of the result is the representation index. Thus if a state’s population is exactly equal to 1/50th of the nation’s population, its representation in the Senate is proportional.\nTo adapt this measure to the House, we must think about how the relationship between proportional representation and population can be expressed numerically. As mentioned above, proportional representation would mean that a state comprising 50% of the national population would likewise comprise 50% of the House’s representatives. The ratio of these two proportions is 1, which creates a similar comparison to Lee’s ratio of 1/50th of national population to state population. This is the motivation for a formula of a representation index for the House of Representatives. The new formula, then, is \\(\\ln\\Big(\\frac{State \\ share \\ of \\ reps}{State \\ share \\ of \\ pop.}\\Big)\\).\n\n\n\n\n\n\nIn this case, if the share of the population is smaller than the share of representatives, the index is inflated, meaning the state is overrepresented. If the share of population is greater than the share of representatives, the index is deflated, meaning the state is underrepresented. This index ranges from \\((-\\infty, \\infty)\\)."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#generalizing-the-representation-index",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#generalizing-the-representation-index",
    "title": "The Fallacy of one person, one vote",
    "section": "Generalizing the Representation Index",
    "text": "Generalizing the Representation Index\nThe representation index for the House of Representatives is written in such a way that it can be adapted for any representative body. The formula evaluates equality of the share of representatives and the share of the total constituency. In general, the formula can be written as \\(\\ln\\Big(\\frac{\\%\\ share \\ of \\ reps}{\\% \\ share \\ of \\ constituency}\\Big)\\).\nTo illustrate, let’s use this formula to calculate the representation index of the Assembly and Senate of California. In 2011, after the most recent census, California redrew its districts. The data used in this demonstration are from the LA Times. The California Assembly and Senate have 80 and 40 members respectively each representing one district.\nFor this example, I consider a difference of 5% in either direction as adhering to the one person, one vote principle. To illustrate this, if the ratio is \\(\\frac{1%}{0.95}\\) the index score is 0.05. Alternatively, if the ratio is \\(\\frac{1}{1.05}\\), the index score is -0.05.\n\n\n\n\n\n\n\n\n\n\n\n\nThe above example demonstrates the use of the representation index for both houses of the California legislature. This is good news as it demonstrates that the state upholds the Equal Protection Clause of the Fourteenth Amendment and adheres to the Supreme Court decision Reynolds v. Sims, in which the court held that state districts must be proportionally drawn (unlike US Senate districts).\nAs Wikipedia states, “[given California]’s large population and relatively small legislature, the Assembly has the largest population per representative ratio of any lower house legislature in the United States; only the federal U.S. House of Representatives has a larger ratio.” California’s representative body differs greatly from that of, for example, New Hampshire.\nNew Hampshire has arguably the most unique lower house legislature of any state: there are 400 representatives from 204 districts. House districts also include what are called floterial districts, areas that represent multiple municipalities. The legality of such districts has been disputed in the state Supreme Court, but nonetheless they persist, and as a result, New Hampshire has one of the smallest constituent-to-representative ratios in the nation. This results in overrepresentation for almost every municipality.\n\n\n\n\n\n\nThe above chart illustrates this phenomenon. Interestingly, the most populous cities and towns in the region are represented according to the one person, one vote paradigm. When applied to the state senate, the results are much different.\n\n\n\n\n\n\nThe representation index for the New Hampshire Senate trends toward underrepresentation. The median value is shown with a dotted red line. It is apparent that the representation of the Senate of New Hampshire is not as equally representative as that of California. The population distribution across the state is highly unequal with a vast majority residents living close to the Maine and Massachusetts borders plausibly contributing to this inequality."
  },
  {
    "objectID": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#further-directions",
    "href": "posts/2019-03-17-the-fallacy-of-one-person-one-vote.html#further-directions",
    "title": "The Fallacy of one person, one vote",
    "section": "Further Directions",
    "text": "Further Directions\nThe ability to compare representation across governing bodies has large implications for comparative political analysis. Further development of the representation index allows scholars and researchers to compare constituency representation among similar bodies—as demonstrated with the case of California and New Hampshire.\nThe new formulation of the representation index is conducive to inter-governmental body analysis. This is possible by the index’s ability to place bodies of different size on the same scale. A result of this is the ability to perform hypothesis testing among groups. As a motivating example, the representation indexes of states are compared along partisanship lines.\n\n\n\n\n\n\nState Senate representation indexes were calculated using the general representation index formula for all 50 states. A two-sample t-test was performed comparing states with two Republican senators to those with two Democratic senators. In doing so, we fail to reject the null hypothesis \\(( \\ t(39) = 1.117, \\ p = 0.27 \\ )\\) that there is a difference of representation index based solely on partisanship.\n\n\n\n\n\n\n\nParty\nn\nMean\nSD\nSE\n\n\n\n\nDemocrat\n21\n0.22\n0.97\n0.21\n\n\nRepublican\n23\n0.67\n0.99\n0.21\n\n\nSplit / Other\n6\n0.67\n1.28\n0.52\n\n\nTotal\n50\n0.48\n1.02\n0.14\n\n\n\n\nIt has been demonstrated that the representation index is an informative measure that can be utilized to examine over and underrepresentation of a governing body. This new formulation of the representation index is useful in its ability to evaluate both over and under-representation and to compare different political entities. One could imagine, for example, a comparison of constituency representation between the United States and France’s upper and lower legislative houses.\n\nI would like to thank Harley Phleger for his help in editing this piece. The cogency of this writing would be entirely lacking without his superb editing abilities. If you are in need of an editor, give him a message. Also, his poetry is wonderful."
  },
  {
    "objectID": "posts/2018-10-27-read_multiple_csv.html",
    "href": "posts/2018-10-27-read_multiple_csv.html",
    "title": "Reading Multiple csvs as 1 data frame",
    "section": "",
    "text": "In an earlier posting I wrote about having to break a single csv into multiple csvs. In other scenarios one data set maybe provided as multiple a csvs.\nThankfully purrr has a beautiful function called map_df() which will make this into a two liner. This process has essentially 3 steps.\n\nCreate a vector of all .csv files that should be merged together.\nRead each file using readr::read_csv()\nCombine each dataframe into one.\n\nmap_df() maps (applys) a function to each value of an object and produces a dataframe of all outputs.\nFor this example I will use the csvs I created in a previous tutorial utilizing a dataset from the Quantitative Social Science book.\n\n# Get all csv file names \nfile_names <- list.files(\"../../static/data/chunk_data\", pattern = \"\\\\.csv\", full.names = TRUE)\nfile_names\n#>  [1] \"../../static/data/chunk_data/social_chunked_1.csv\" \n#>  [2] \"../../static/data/chunk_data/social_chunked_10.csv\"\n#>  [3] \"../../static/data/chunk_data/social_chunked_11.csv\"\n#>  [4] \"../../static/data/chunk_data/social_chunked_12.csv\"\n#>  [5] \"../../static/data/chunk_data/social_chunked_13.csv\"\n#>  [6] \"../../static/data/chunk_data/social_chunked_2.csv\" \n#>  [7] \"../../static/data/chunk_data/social_chunked_3.csv\" \n#>  [8] \"../../static/data/chunk_data/social_chunked_4.csv\" \n#>  [9] \"../../static/data/chunk_data/social_chunked_5.csv\" \n#> [10] \"../../static/data/chunk_data/social_chunked_6.csv\" \n#> [11] \"../../static/data/chunk_data/social_chunked_7.csv\" \n#> [12] \"../../static/data/chunk_data/social_chunked_8.csv\" \n#> [13] \"../../static/data/chunk_data/social_chunked_9.csv\"\n\n\nlibrary(tidyverse)\n# apply \nall_csvs <- map_df(file_names, read_csv)\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 5866 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#> Rows: 25000 Columns: 6\n#> ── Column specification ────────────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): sex, messages\n#> dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# preview the data\nhead(all_csvs)\n#> # A tibble: 6 × 6\n#>   sex    yearofbirth primary2004 messages   primary2006 hhsize\n#>   <chr>        <dbl>       <dbl> <chr>            <dbl>  <dbl>\n#> 1 male          1941           0 Civic Duty           0      2\n#> 2 female        1947           0 Civic Duty           0      2\n#> 3 male          1951           0 Hawthorne            1      3\n#> 4 female        1950           0 Hawthorne            1      3\n#> 5 female        1982           0 Hawthorne            1      3\n#> 6 male          1981           0 Control              0      3"
  },
  {
    "objectID": "posts/2021-03-28-python-r.html",
    "href": "posts/2021-03-28-python-r.html",
    "title": "Python & R in production — the API way",
    "section": "",
    "text": "In my previous post I discussed how we can alter the R & Python story to be predicated on APIs as a way to bridge the language divide. The R & Python love story feels almost like unrequited love (h/t). Much of the development towards integrating the two languages has been heavily focused on the R user experience. While the developments with respect to reticulate have been enormous and cannot go understated, it might be worthwhile exploring another way in which R & Python and, for that matter, Python & R can be utilized together.\nBy shifting from language based tools that call the other language and translate their objects like reticulate and rpy2, to APIs we can develop robust language agnostic data science pipelines. I want to provide two motivating examples that explore the interplay between R & Python."
  },
  {
    "objectID": "posts/2021-03-28-python-r.html#calling-python-from-r-without-reticulate",
    "href": "posts/2021-03-28-python-r.html#calling-python-from-r-without-reticulate",
    "title": "Python & R in production — the API way",
    "section": "Calling Python from R (without reticulate)",
    "text": "Calling Python from R (without reticulate)\nWhen we talk about R & Python we typically are referring to reticulate, whether that be through python code chunks, the {tensorflow} package, or reticulate itself. However, as discussed in my previous post, another way that we can do this is via API. Flask can be used to create RESTful APIs.\nOn the RStudio Connect demo server there is a Flask app which provides historical stock prices for a few tickers. We can create a simple windowed summary visualization utilizing the Flask app, httr, dplyr, and ggplot2. Let’s break this down. First we use the httr library to send an HTTP request to the Flask app.\nlibrary(httr)\nlibrary(tidyverse)\n\nflask_call <- \"https://colorado.rstudio.com/rsc/flask-stock-service/stocks/AAPL/history\"\n\naapl <- GET(flask_call) %>% \n  content(as = \"text\", encoding = \"utf-8\") %>% \n  jsonlite::fromJSON() %>% \n  as_tibble()\nBy sending this HTTP request, we are then kicking off a Python process which returns our dataset. We can then use dplyr to aggregate our dataset as we normally would.\naapl_yearly <- aapl %>% \n  group_by(year = lubridate::year(date)) %>% \n  summarise(avg_adj = mean(adjusted)) \n\nhead(aapl_yearly)\n#> # A tibble: 6 x 2\n#>    year avg_adj\n#>   <dbl>   <dbl>\n#> 1  2010    24.9\n#> 2  2011    34.8\n#> 3  2012    56.1\n#> 4  2013    53.1\n#> 5  2014    84.3\n#> 6  2015   113.\nFinally we utilize ggplot2 to create the simple visualization.\nggplot(aapl_yearly, aes(year, avg_adj)) +\n  geom_line() + \n  labs(title = \"AAPL stock growth\", x = \"\", y = \"Average Adjusted\") +\n  scale_y_continuous(labels = scales::dollar) + \n  theme_minimal()\n\n\n\nThat was simple, right? In the above code chunks we utilized both R and python while only interacting and writing R code. That’s the brilliance of this approach."
  },
  {
    "objectID": "posts/2021-03-28-python-r.html#calling-r-from-python",
    "href": "posts/2021-03-28-python-r.html#calling-r-from-python",
    "title": "Python & R in production — the API way",
    "section": "Calling R from Python",
    "text": "Calling R from Python\nThe less often discussed part of this love story—hence unrequited love story—is how can Python users utilize R within their own workflows. Often machine learning engineers will use Python in combination with Scikit Learn to create their models. To illustrate how we can let both R and Python users shine I wanted to adapt the wonderful Bike Prediction example from the Solutions Engineering team at RStudio.\n\n\n\nThe Bike Prediction project is an example of orchestrating a number of data science artifacts into a holistic system on RStudio Connect that all work in unity. This example could just as well have been written entirely with Python. It could even be written as a combination of both R and Python. And that is what I’d like to illustrate.\nThe bike prediction app utilizes a custom R package and the power of dbplyr to perform scheduled ETL jobs. It is effective, efficient, and already deployed. Say one has a colleague who would like to create a new machine learning model using the same data how can we enable them to do so? The example works within the context of its own R Markdown that retrains the model. Rather than making a one time export of the data from the ETL process, we can make the data available consistently through a RESTful API hosted here.\n\n\n\nThe training and testing data have been made available through a plumber API that is hosted on RStudio Connect. With the data being available through an API, all that is needed to interact with it is the requests library. Everything else is as one would anticipate!\nimport requests\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nIn the below code chunk we call the Plumber API using an HTTP request which kicks off an R process. That R process utilizes dbplyr and lubridate to extract and partition data for training and testing.\n# Fetch data from Plumber API \ntest_dat_raw = requests.get(\"https://colorado.rstudio.com/rsc/bike-data-api/testing-data\")\ntest_dat = pd.read_json(test_dat_raw.text)\n\ntrain_dat_raw = requests.get(\"https://colorado.rstudio.com/rsc/bike-data-api/training-data\")\ntrain_dat = pd.read_json(train_dat_raw.text)\nNow that the data have been processed by R and loaded as a pandas dataframe the model training can continue as standard.\n# partition data and one hot encode day of week\ntrain_x = pd.concat([train_dat[[\"hour\", \"month\", \"lat\", \"lon\"]], pd.get_dummies(train_dat.dow)], axis = 1)\ntrain_y = train_dat[\"n_bikes\"]\n\n# instantiate xgb model object\nmodel = XGBRegressor()\n \n# fit the model with the training data\nmodel.fit(train_x,train_y)\n\n# predict the target on the test dataset\ntest_dow = pd.get_dummies(pd.Categorical(test_dat.dow, categories = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday']))\ntest_x = pd.concat([test_dat[[\"hour\", \"month\", \"lat\", \"lon\"]], test_dow], axis = 1)\ntest_y = test_dat.n_bikes\n# predict the target on the test dataset\npredict_test = model.predict(test_x)\n \n# MSE on test dataset\nmean_squared_error(test_y,predict_test, squared = False)\n#> 4.502217132673415\nThrough the API both R and Python were able to flourish all the while building extensible infrastructure that can be utilized beyond their own team. The API approach enables the R and Python user to extend their tools beyond their direct team without having to adopt a new toolkit."
  },
  {
    "objectID": "posts/2021-03-28-python-r.html#adopting-apis-for-cross-language-collaboration",
    "href": "posts/2021-03-28-python-r.html#adopting-apis-for-cross-language-collaboration",
    "title": "Python & R in production — the API way",
    "section": "Adopting APIs for cross language collaboration",
    "text": "Adopting APIs for cross language collaboration\nWhile data scientists may usually think of APIs as something that they use to interact with SaaS products or extract data, they are also a tool that can be utilized to build out the data science infrastructure of a team. Through Flask, Plumber, and other libraries that turn code into RESTful APIs, data scientists can bridge language divides with exceptional ease. I think we ought to begin to transition the ways in which we think about language divides. We ought to utilize the universal language of HTTP more thoroughly. By creating these APIs we not only can aid other data scientists, but entirely other teams. A React JS web development can then tap into your API to either serve up predictions, extract data, send files, or whatever else you can dream up. Let’s not limit ourselves to one language. Let’s build out APIs to enable all languages to thrive.\nDisclaimer: This is a personal opinion and not endorsed or published by RStudio. My statements represent no one but myself—sometimes not even that."
  },
  {
    "objectID": "posts/2020-02-19-r-history.html",
    "href": "posts/2020-02-19-r-history.html",
    "title": "Design Paradigms in R",
    "section": "",
    "text": "Lately I have been developing a deep curiosity of the origins of the R language. I have since read a more from the WayBack Machine than a Master’s student probably should. There are four documents that I believe to be extremely foundational and most clearly outline the original philosophies underpinning both R and its predecessor S. These are Evolution of the S Language (Chambers, 1996), A Brief History of S (Becker), Stages in the Evolution of S (Chambers, 200), and R: Past and Future History by Ross Ihaka (1998). The readings have elicited many lines of thought and potential inquiry. What interests me the most at present is the question “how have the design principles of S and R manifested themselves today?”\nThere are a number of design principles that I believe still exist in the R language today. Two of these find their origin in the development of S and the third was most clearly emphasized in the development of R. These are, in no particular order, what I believe to be the design principles that are most prominent in the modern iteration of R. The software should:\n\nbe an interface language;\nbe performant;\nand make users feel like they are performing analyses.\n\nThe first design philosophy I think is quite apparent in R’s continual development as an interface language. I believe part of R’s success is because of the immense community development focusing on interfaces to other tools and languages. Most prominently I would argue is the development of interfaces to SQL, Stan, JavaScript, and Python. Each enables R users to interact with existing infrastructure which vastly increases the breadth of technologies and tools available to useRs.\nIdentifying R’s evolution as an interface language is a rather objective task. Assessing these latter two principles is a subjective task, but one I will endeavor nonetheless.\nThe second principle is stated clearly by Ihaka (1998).\n\n“My own conclusion has been that it is important to pursue efficiency issues, and in particular, speed.”\n\nIn my view of what is the prominent landscape of R today, few packages have paid as much attention to this principle as data.table. data.table’s performance is beyond reproach. The speed at which one can subset, manipulate, order, etc. using data.table is remarkable. Recent speed tests by package author Matt Dowle illustrate this. In performance testing, data.table regularly outperforms other existing tools such as Spark, dplyr, and pandas—spark being an exceptionally notable one.\nThis brings us to the third point which actually finds its origination in the development of S. John Chambers in Stages in the Evolution of S wrote that\n\n“The ambiguity is real and goes to a key objective: we wanted users to be able to begin in an interactive environment, where they did not consciously think of themselves as programming.”\n\nThis is a helpful reminder that S was developed with the intentions of creating a tool for statistical analysis. In the design and implementation of S, the emphasis was doing analysis not programming. From this paradigm is where I think the tidyverse came—perhaps unintentionally, but consequently nonetheless.\nThe development of the tidyverse has, I believe, adhered to this principle. There is what seems to be a constant and conscious consideration to the useR experience. dplyr, for example, has developed a way to perform an analysis that is clear in intent and, to an extent, that can be read in a linguistically cogent manner.\nFrom this view, it clear that the R community has done a great job adhering to these principles. The various odbc packages, numerous javascript packages (particularly in the Shiny space), reticulate, rstan, JuliaCall, and rcpp, among many others are an immense testament to this. Moreover, data.table’s “relentless focus on performance across the entire package” is reason for its success (Wickham, 2019). Similarly, I believe the relentless focus on user experience in the tidyverse is reason for its success. When viewing these two latter toolkits, they should be viewed at two sides to the same coin with each approaching the same end goal from a different perspective."
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html",
    "href": "posts/2018-01-27-introducing-geniusr.html",
    "title": "Introducing geniusR",
    "section": "",
    "text": "knitr::opts_chunk$set(eval=FALSE)"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#install-and-load-the-package",
    "href": "posts/2018-01-27-introducing-geniusr.html#install-and-load-the-package",
    "title": "Introducing geniusR",
    "section": "Install and load the package",
    "text": "Install and load the package\n\ndevtools::install_github(\"josiahparry/geniusR\")\n\nLoad the package:\n\nlibrary(geniusR)\nlibrary(tidyverse) # For manipulation"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#whole-albums",
    "href": "posts/2018-01-27-introducing-geniusr.html#whole-albums",
    "title": "Introducing geniusR",
    "section": "Whole Albums",
    "text": "Whole Albums\ngenius_album() allows you to download the lyrics for an entire album in a tidy format. There are two arguments artists and album. Supply the quoted name of artist and the album (if it gives you issues check that you have the album name and artists as specified on Genius).\nThis returns a tidy data frame with three columns:\n\ntitle: track name\ntrack_n: track number\ntext: lyrics\n\n\nemotions_math <- genius_album(artist = \"Margaret Glaspy\", album = \"Emotions and Math\")\nemotions_math"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#multiple-albums",
    "href": "posts/2018-01-27-introducing-geniusr.html#multiple-albums",
    "title": "Introducing geniusR",
    "section": "Multiple Albums",
    "text": "Multiple Albums\nIf you wish to download multiple albums from multiple artists, try and keep it tidy and avoid binding rows if you can. We can achieve this in a tidy workflow by creating a tibble with two columns: artist and album where each row is an artist and their album. We can then iterate over those columns with purrr:map2().\nIn this example I will extract 3 albums from Kendrick Lamar and Sara Bareilles (two of my favotire musicians). The first step is to create the tibble with artists and album titles.\n\nalbums <-  tibble(\n  artist = c(\n    rep(\"Kendrick Lamar\", 3), \n    rep(\"Sara Bareilles\", 3)\n    ),\n  album = c(\n    \"Section 80\", \"Good Kid, M.A.A.D City\", \"DAMN.\",\n    \"The Blessed Unrest\", \"Kaleidoscope Heart\", \"Little Voice\"\n    )\n)\n\nalbums\n\nNo we can iterate over each row using the map2 function. This allows us to feed each value from the artist and album columns to the genius_album() function. Utilizing a map call within a dplyr::mutate() function creates a list column where each value is a tibble with the data frame from genius_album(). We will later unnest this.\n\n## We will have an additional artist column that will have to be dropped\nalbum_lyrics <- albums %>% \n  mutate(tracks = map2(artist, album, genius_album))\n\nalbum_lyrics\n\nNow when you view this you will see that each value within the tracks column is <tibble>. This means that that value is infact another tibble. We expand this using tidyr::unnest().\n\n# Unnest the lyrics to expand \nlyrics <- album_lyrics %>% \n  unnest(tracks) %>%    # Expanding the lyrics \n  arrange(desc(artist)) # Arranging by artist name \n\nhead(lyrics)"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#song-lyrics",
    "href": "posts/2018-01-27-introducing-geniusr.html#song-lyrics",
    "title": "Introducing geniusR",
    "section": "Song Lyrics",
    "text": "Song Lyrics\n\ngenius_lyrics()\nGetting lyrics to a single song is pretty easy. Let’s get in our ELEMENT. and checkout DNA. by Kendrick Lamar. But first, note that the genius_lyrics() function takes two main arguments, artist and song. Be sure to spell the name of the artist and the song correctly.\n\nDNA <- genius_lyrics(artist = \"Kendrick Lamar\", song = \"DNA.\")\n\nDNA\n\nThis returns a tibble with three columns title, text, and line. However, you can specifiy additional arguments to control the amount of information to be returned using the info argument.\n\ninfo = \"title\" (default): Return the lyrics, line number, and song title.\ninfo = \"simple\": Return just the lyrics and line number.\ninfo = \"artist\": Return the lyrics, line number, and artist.\ninfo = \"all\": Return lyrics, line number, song title, artist."
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#tracklists",
    "href": "posts/2018-01-27-introducing-geniusr.html#tracklists",
    "title": "Introducing geniusR",
    "section": "Tracklists",
    "text": "Tracklists\ngenius_tracklist(), given an artist and an album will return a barebones tibble with the track title, track number, and the url to the lyrics.\n\ngenius_tracklist(artist = \"Basement\", album = \"Colourmeinkindness\")"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#nitty-gritty",
    "href": "posts/2018-01-27-introducing-geniusr.html#nitty-gritty",
    "title": "Introducing geniusR",
    "section": "Nitty Gritty",
    "text": "Nitty Gritty\ngenius_lyrics() generates a url to Genius which is fed to genius_url(), the function that does the heavy lifting of actually fetching lyrics.\nI have not figured out all of the patterns that are used for generating the Genius.com urls, so errors are bound to happen. If genius_lyrics() returns an error. Try utilizing genius_tracklist() and genius_url() together to get the song lyrics.\nFor example, say “(No One Knows Me) Like the Piano” by Sampha wasn’t working in a standard genius_lyrics() call.\n\npiano <- genius_lyrics(\"Sampha\", \"(No One Knows Me) Like the Piano\")\n\nWe could grab the tracklist for the album Process which the song is from. We could then isolate the url for (No One Knows Me) Like the Piano and feed that into `genius_url().\n\n# Get the tracklist for \nprocess <- genius_tracklist(\"Sampha\", \"Process\")\n\n# Filter down to find the individual song\npiano_info <- process %>% \n  filter(title == \"(No One Knows Me) Like the Piano\")\n\n# Filter song using string detection\n# process %>% \n#  filter(stringr::str_detect(title, coll(\"Like the piano\", ignore_case = TRUE)))\n\npiano_url <- piano_info$track_url\n\nNow that we have the url, feed it into genius_url().\n\ngenius_url(piano_url, info = \"simple\")"
  },
  {
    "objectID": "posts/2018-01-27-introducing-geniusr.html#generative-functions",
    "href": "posts/2018-01-27-introducing-geniusr.html#generative-functions",
    "title": "Introducing geniusR",
    "section": "Generative functions",
    "text": "Generative functions\nThis package works almost entirely on pattern detection. The urls from Genius are (mostly) easily reproducible (shout out to Angela Li for pointing this out).\nThe two functions that generate urls are gen_song_url() and gen_album_url(). To see how the functions work, try feeding an artist and song title to gen_song_url() and an artist and album title to gen_album_url().\n\ngen_song_url(\"Laura Marling\", \"Soothing\")\n\n\ngen_album_url(\"Daniel Caesar\", \"Freudian\")\n\ngenius_lyrics() calls gen_song_url() and feeds the output to genius_url() which preforms the scraping.\nGetting lyrics for albums is slightly more involved. It first calls genius_tracklist() which first calls gen_album_url() then using the handy package rvest scrapes the song titles, track numbers, and song lyric urls. Next, the song urls from the output are iterated over and fed to genius_url().\nTo make this more clear, take a look inside of genius_album()\n\ngenius_album <- function(artist = NULL, album = NULL, info = \"simple\") {\n\n  # Obtain tracklist from genius_tracklist\n  album <- genius_tracklist(artist, album) %>%\n\n    # Iterate over the url to the song title\n    mutate(lyrics = map(track_url, genius_url, info)) %>%\n\n    # Unnest the tibble with lyrics\n    unnest(lyrics) %>%\n    \n    # Deselect the track url\n    select(-track_url)\n\n\n  return(album)\n}\n\n\nNotes:\nAs this is my first “package” there will be many issues. Please submit an issue and I will do my best to attend to it.\nThere are already issues of which I am present (the lack of error handling). If you would like to take those on, please go ahead and make a pull request. Please contact me on Twitter."
  },
  {
    "objectID": "posts/2019-08-04-my-parts.html",
    "href": "posts/2019-08-04-my-parts.html",
    "title": "∑ { my parts }",
    "section": "",
    "text": "library(tidyverse)\n\nterrorists <- readr::read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSqNhpFX_69klKgJCVobc3fjHYVE9pNosrDi9h6irLlCtSSLpR704iu9VqI7CxdRi0iKt3p1FDYbu8Y/pub?gid=956062857&single=true&output=csv\")\n\n\n\n\n\nterrorist_by_race\n#> # A tibble: 7 × 6\n#>   race                n fatalities injured total_victims   `%`\n#>   <chr>           <int>      <dbl>   <dbl>         <dbl> <dbl>\n#> 1 white              63        554    1067          1621 69.3 \n#> 2 other               5         90     115           205  8.77\n#> 3 black              19        108      89           197  8.43\n#> 4 asian               8         77      33           110  4.70\n#> 5 unclear             6         40      61           101  4.32\n#> 6 latino             10         44      33            77  3.29\n#> 7 native american     3         19       8            27  1.15\n\n\n\n\n\nterrorist_by_gender\n#> # A tibble: 3 × 6\n#>   gender            n fatalities injured total_victims    `%`\n#>   <chr>         <int>      <dbl>   <dbl>         <dbl>  <dbl>\n#> 1 male            110        903    1380          2283 97.6  \n#> 2 male & female     1         14      21            35  1.50 \n#> 3 female            3         15       5            20  0.855\n\n\nterrorist <- c(\"angry\", \"white\", \"male\")\nmy_parts <- c(\"angry\", \"white\", \"male\")\n\n\nmy_parts == terrorist\n#> [1] TRUE TRUE TRUE\n\n\n`I am` > sum(my_parts)\n\n\n#> [1] TRUE\n\n\n`I am` == sum(terrorist)\n\n\nFALSE\n#> [1] FALSE\n\n\nwhite_males <- filter(terrorists,\n                      race == \"white\",\n                      tolower(gender) == \"male\",\n                      !is.na(name))\n\npull(white_males, name)\n#>  [1] \"Jordan Witmer\"             \"Zephen A. Xaver\"           \"Robert D. Bowers\"         \n#>  [4] \"Jarrod W. Ramos\"           \"Dimitrios Pagourtzis\"      \"Travis Reinking\"          \n#>  [7] \"Nikolas J. Cruz\"           \"Timothy O'Brien Smith\"     \"Kevin Janson Neal\"        \n#> [10] \"Devin Patrick Kelley\"      \"Scott Allen Ostrem\"        \"Stephen Craig Paddock\"    \n#> [13] \"Randy Stair\"               \"Thomas Hartless\"           \"Jason B. Dalton\"          \n#> [16] \"Robert Lewis Dear\"         \"Noah Harpham\"              \"Dylann Storm Roof\"        \n#> [19] \"Elliot Rodger\"             \"John Zawahri\"              \"Kurt Myers\"               \n#> [22] \"Adam Lanza\"                \"Andrew Engeldinger\"        \"Wade Michael Page\"        \n#> [25] \"James Holmes\"              \"Ian Stawicki\"              \"Scott Evans Dekraai\"      \n#> [28] \"Jared Loughner\"            \"Robert Stewart\"            \"Wesley Neal Higdon\"       \n#> [31] \"Steven Kazmierczak\"        \"Robert A. Hawkins\"         \"Tyler Peterson\"           \n#> [34] \"Sulejman Talović\\u0087\"    \"Charles Carl Roberts\"      \"Kyle Aaron Huff\"          \n#> [37] \"Terry Michael Ratzmann\"    \"Nathan Gale\"               \"Douglas Williams\"         \n#> [40] \"Michael McDermott\"         \"Larry Gene Ashbrook\"       \"Day trader Mark O. Barton\"\n#> [43] \"Eric Harris\"               \"Kipland P. Kinkel\"         \"Mitchell Scott Johnson\"   \n#> [46] \"Matthew Beck\"              \"Dean Allen Mellberg\"       \"Kenneth Junior French\"    \n#> [49] \"Gian Luigi Ferri\"          \"John T. Miller\"            \"Eric Houston\"             \n#> [52] \"Thomas McIlvane\"           \"George Hennard\"            \"Joseph T. Wesbecker\"      \n#> [55] \"Patrick Purdy\"             \"Richard Farley\"            \"William Cruse\"            \n#> [58] \"Patrick Sherrill\"          \"James Oliver Huberty\"      \"Abdelkrim Belachheb\"      \n#> [61] \"Carl Robert Brown\"\n\n\nam_i <- function(terrorist) {\n  msg <- paste(\"am i ==\", terrorist)\n  print(msg)\n  print(`am i` == terrorist)\n}\n\n\npull(white_males, name) %>% \n  walk(~am_i(.))\n#> [1] \"`am i` == Jordan Witmer\"\n#> [1] FALSE\n#> [1] \"`am i` == Zephen A. Xaver\"\n#> [1] FALSE\n#> [1] \"`am i` == Robert D. Bowers\"\n#> [1] FALSE\n#> [1] \"`am i` == Jarrod W. Ramos\"\n#> [1] FALSE\n#> [1] \"`am i` == Dimitrios Pagourtzis\"\n#> [1] FALSE\n#> [1] \"`am i` == Travis Reinking\"\n#> [1] FALSE\n#> [1] \"`am i` == Nikolas J. Cruz\"\n#> [1] FALSE\n#> [1] \"`am i` == Timothy O'Brien Smith\"\n#> [1] FALSE\n#> [1] \"`am i` == Kevin Janson Neal\"\n#> [1] FALSE\n#> [1] \"`am i` == Devin Patrick Kelley\"\n#> [1] FALSE\n#> [1] \"`am i` == Scott Allen Ostrem\"\n#> [1] FALSE\n#> [1] \"`am i` == Stephen Craig Paddock\"\n#> [1] FALSE\n#> [1] \"`am i` == Randy Stair\"\n#> [1] FALSE\n#> [1] \"`am i` == Thomas Hartless\"\n#> [1] FALSE\n#> [1] \"`am i` == Jason B. Dalton\"\n#> [1] FALSE\n#> [1] \"`am i` == Robert Lewis Dear\"\n#> [1] FALSE\n#> [1] \"`am i` == Noah Harpham\"\n#> [1] FALSE\n#> [1] \"`am i` == Dylann Storm Roof\"\n#> [1] FALSE\n#> [1] \"`am i` == Elliot Rodger\"\n#> [1] FALSE\n#> [1] \"`am i` == John Zawahri\"\n#> [1] FALSE\n#> [1] \"`am i` == Kurt Myers\"\n#> [1] FALSE\n#> [1] \"`am i` == Adam Lanza\"\n#> [1] FALSE\n#> [1] \"`am i` == Andrew Engeldinger\"\n#> [1] FALSE\n#> [1] \"`am i` == Wade Michael Page\"\n#> [1] FALSE\n#> [1] \"`am i` == James Holmes\"\n#> [1] FALSE\n#> [1] \"`am i` == Ian Stawicki\"\n#> [1] FALSE\n#> [1] \"`am i` == Scott Evans Dekraai\"\n#> [1] FALSE\n#> [1] \"`am i` == Jared Loughner\"\n#> [1] FALSE\n#> [1] \"`am i` == Robert Stewart\"\n#> [1] FALSE\n#> [1] \"`am i` == Wesley Neal Higdon\"\n#> [1] FALSE\n#> [1] \"`am i` == Steven Kazmierczak\"\n#> [1] FALSE\n#> [1] \"`am i` == Robert A. Hawkins\"\n#> [1] FALSE\n#> [1] \"`am i` == Tyler Peterson\"\n#> [1] FALSE\n#> [1] \"`am i` == Sulejman Talović\\u0087\"\n#> [1] FALSE\n#> [1] \"`am i` == Charles Carl Roberts\"\n#> [1] FALSE\n#> [1] \"`am i` == Kyle Aaron Huff\"\n#> [1] FALSE\n#> [1] \"`am i` == Terry Michael Ratzmann\"\n#> [1] FALSE\n#> [1] \"`am i` == Nathan Gale\"\n#> [1] FALSE\n#> [1] \"`am i` == Douglas Williams\"\n#> [1] FALSE\n#> [1] \"`am i` == Michael McDermott\"\n#> [1] FALSE\n#> [1] \"`am i` == Larry Gene Ashbrook\"\n#> [1] FALSE\n#> [1] \"`am i` == Day trader Mark O. Barton\"\n#> [1] FALSE\n#> [1] \"`am i` == Eric Harris\"\n#> [1] FALSE\n#> [1] \"`am i` == Kipland P. Kinkel\"\n#> [1] FALSE\n#> [1] \"`am i` == Mitchell Scott Johnson\"\n#> [1] FALSE\n#> [1] \"`am i` == Matthew Beck\"\n#> [1] FALSE\n#> [1] \"`am i` == Dean Allen Mellberg\"\n#> [1] FALSE\n#> [1] \"`am i` == Kenneth Junior French\"\n#> [1] FALSE\n#> [1] \"`am i` == Gian Luigi Ferri\"\n#> [1] FALSE\n#> [1] \"`am i` == John T. Miller\"\n#> [1] FALSE\n#> [1] \"`am i` == Eric Houston\"\n#> [1] FALSE\n#> [1] \"`am i` == Thomas McIlvane\"\n#> [1] FALSE\n#> [1] \"`am i` == George Hennard\"\n#> [1] FALSE\n#> [1] \"`am i` == Joseph T. Wesbecker\"\n#> [1] FALSE\n#> [1] \"`am i` == Patrick Purdy\"\n#> [1] FALSE\n#> [1] \"`am i` == Richard Farley\"\n#> [1] FALSE\n#> [1] \"`am i` == William Cruse\"\n#> [1] FALSE\n#> [1] \"`am i` == Patrick Sherrill\"\n#> [1] FALSE\n#> [1] \"`am i` == James Oliver Huberty\"\n#> [1] FALSE\n#> [1] \"`am i` == Abdelkrim Belachheb\"\n#> [1] FALSE\n#> [1] \"`am i` == Carl Robert Brown\"\n#> [1] FALSE\n\n\n`I am` > sum(my_parts)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html",
    "title": "genius tutorial",
    "section": "",
    "text": "knitr::opts_chunk$set(eval = FALSE)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#introducing-genius",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#introducing-genius",
    "title": "genius tutorial",
    "section": "Introducing genius",
    "text": "Introducing genius\nYou want to start analysing song lyrics, where do you go? There have been music information retrieval papers written on the topic of programmatically extracting lyrics from the web. Dozens of people have gone through the laborious task of scraping song lyrics from websites. Even a recent winner of the Shiny competition scraped lyrics from Genius.com.\nI too have been there. Scraping websites is not always the best use of your time. genius is an R package that will enable you to programatically download song lyrics in a tidy format ready for analysis. To begin using the package, it first must be installed, and loaded. In addition to genius, we will need our standard data manipulation tools from the tidyverse.\n\ninstall.packages(\"genius\")\n\n\nlibrary(genius)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#single-song-lyrics",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#single-song-lyrics",
    "title": "genius tutorial",
    "section": "Single song lyrics",
    "text": "Single song lyrics\nThe simplest method of extracting song lyrics is to get just a single song at a time. This is done with the genius_lyrics() function. It takes two main arguments: artist and song. These are the quoted name of the artist and song. Additionally there is a third argument info which determines what extra metadata you can get. The possible values are title, simple, artist, features, and all. I recommend trying them all to see how they work.\nIn this example we will work to retrieve the song lyrics for the upcoming musician Renny Conti.\n\nfloating <- genius_lyrics(\"renny conti\", \"people floating\")\nfloating"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#album-lyrics",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#album-lyrics",
    "title": "genius tutorial",
    "section": "Album Lyrics",
    "text": "Album Lyrics\nNow that you have the intuition for obtaining lyrics for a single song, we can now create a larger dataset for the lyrics of an entire album using genius_album(). Similar to genius_lyrics(), the arguments are artist, album, and info.\nIn the exercise below the lyrics for Snail Mail’s album Lush. Try retrieving the lyrics for an album of your own choosing.\n\nlush <- genius_album(\"Snail Mail\", \"Lush\")\nlush"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#adding-lyrics-to-a-data-frame",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#adding-lyrics-to-a-data-frame",
    "title": "genius tutorial",
    "section": "Adding Lyrics to a data frame",
    "text": "Adding Lyrics to a data frame\n\nMultiple songs\nA common use for lyric analysis is to compare the lyrics of one artist to another. In order to do that, you could potentially retrieve the lyrics for multiple songs and albums and then join them together. This has one major issue in my mind, it makes you create multiple object taking up precious memory. For this reason, the function add_genius() was developed. This enables you to create a tibble with a column for an artists name and their album or song title. add_genius() will then go through the entire tibble and add song lyrics for the tracks and albums that are available.\nLet’s try this with a tibble of three songs.\n\nthree_songs <- tribble(\n  ~ artist, ~ title,\n  \"Big Thief\", \"UFOF\",\n  \"Andrew Bird\", \"Imitosis\",\n  \"Sylvan Esso\", \"Slack Jaw\"\n)\n\nsong_lyrics <- three_songs %>% \n  add_genius(artist, title, type = \"lyrics\")\n\nsong_lyrics %>% \n  count(artist)\n\n\n\n\nMultiple albums\nadd_genius() also extends this functionality to albums.\n\nalbums <- tribble(\n  ~ artist, ~ title,\n  \"Andrew Bird\", \"Armchair Apocrypha\",\n  \"Andrew Bird\", \"Things are really great here sort of\"\n)\n\nalbum_lyrics <- albums %>% \n  add_genius(artist, title, type = \"album\")\n\nalbum_lyrics\n\nWhat is important to note here is that the warnings for this function are somewhat informative. When a 404 error occurs, this may be because that the song does not exist in Genius. Or, that the song is actually an instrumental which is the case here with Andrew Bird.\n\n\nAlbums and Songs\nIn the scenario that you want to mix single songs and lyrics, you can supply a column with the type value of each row. The example below illustrates this. First a tibble with artist, track or album title, and type columns are created. Next, the tibble is piped to add_genius() with the unquote column names for the artist, title, and type columns. This will then iterate over each row and fetch the appropriate song lyrics.\n\nsong_album <- tribble(\n  ~ artist, ~ title, ~ type,\n  \"Big Thief\", \"UFOF\", \"lyrics\",\n  \"Andrew Bird\", \"Imitosis\", \"lyrics\",\n  \"Sylvan Esso\", \"Slack Jaw\", \"lyrics\",\n  \"Movements\", \"Feel Something\", \"album\"\n)\n\nmixed_lyrics <- song_album %>% \n  add_genius(artist, title, type)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#self-similarity",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#self-similarity",
    "title": "genius tutorial",
    "section": "Self-similarity",
    "text": "Self-similarity\nAnother feature of genius is the ability to create self-similarity matrices to visualize lyrical patterns within a song. This idea was taken from Colin Morris’ wonderful javascript based Song Sim project. Colin explains the interpretation of a self-similarity matrix in their TEDx talk. An even better description of the interpretation is available in this post.\nTo use Colin’s example we will look at the structure of Ke$ha’s Tik Tok.\nThe function calc_self_sim() will create a self-similarity matrix of a given song. The main arguments for this function are the tibble (df), and the column containing the lyrics (lyric_col). Ideally this is one line per observation as is default from the output of genius_*(). The tidy output compares every ith word with every word in the song. This measures repetition of words and will show us the structure of the lyrics.\n\ntik_tok <- genius_lyrics(\"Ke$ha\", \"Tik Tok\")\n\ntt_self_sim <- calc_self_sim(tik_tok, lyric, output = \"tidy\")\n\ntt_self_sim\n\ntt_self_sim %>% \n  ggplot(aes(x = x_id, y = y_id, fill = identical)) +\n  geom_tile() +\n  scale_fill_manual(values = c(\"white\", \"black\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        axis.text = element_blank()) +\n  scale_y_continuous(trans = \"reverse\") +\n  labs(title = \"Tik Tok\", subtitle = \"Self-similarity matrix\", x = \"\", y = \"\", \n       caption = \"The matrix displays that there are three choruses with a bridge between the last two. The bridge displays internal repetition.\")"
  },
  {
    "objectID": "posts/2019-12-14-spss-haven.html",
    "href": "posts/2019-12-14-spss-haven.html",
    "title": "Finding an SPSS {haven}",
    "section": "",
    "text": "Note (2022-11-14): the dataset that was used can no longer be access from the url I provided in this blog post. I have found it at a zip file in a blog post at https://blog.faradars.org/wp-content/uploads/2020/07/noisedata.zip if you want to try downloading it from there.\nMy education as a social scientist—undergratuate studies in sociology and anthropology—was largely focused on theory and the application of theory to social problems. For the most part, I taught myself how to apply those methods through R. I was fortunate enough to have avoided ever using SPSS. Perhaps that is good. Perhaps it is not. The use of R in the social sciences is increasing and I will go as far as to say that that is great news. However, there are still holdouts.\nVery recently I came across data exported from SPSS in the wild. In the work that I get to engage in at the Boston Area Research Initiative (BARI) we receive data from various sources, whether these be municipal organizations, the police department, or non-profits, and as Tolstoy said:\nIn this post I want to illustrate two of the main pain points I encountered while working with data from SPSS.I also go rather deep into the weeds about R data structures and how we can manipulate them. While this is “about” SPSS data, folks looking for a better understanding of R object may benefit from this as well (specifically Gripe 2)."
  },
  {
    "objectID": "posts/2019-12-14-spss-haven.html#haven",
    "href": "posts/2019-12-14-spss-haven.html#haven",
    "title": "Finding an SPSS {haven}",
    "section": "{haven}",
    "text": "{haven}\nI am not sure where the inspiration for the name “haven” came from. But I am sure that its name does indeed speak for itself. haven enables R programmers that are SPSS—Stata and SAS as well—illiterate to work with data that is not naturally intended for R use. There are two key behaviors of SPSS data that I was unaware of and that plagued me. I break these down into three gripes below."
  },
  {
    "objectID": "posts/2019-12-14-spss-haven.html#gripes",
    "href": "posts/2019-12-14-spss-haven.html#gripes",
    "title": "Finding an SPSS {haven}",
    "section": "Gripes",
    "text": "Gripes\nI maintain three gripes about SPSS data.\n\nFactor values are represented numerically and the values that they represent are stored in metadata.\nColumn names are vague. The values they represent are stored as a label (metadata).\nMissing values can be represented innumerably. Each column can have user defined missing values, i.e. 9999.\n\nNow, I must say that I have found it best practice to try and combat my own gripes.\nIn regards to the former two gripes, this is not unheard of behavior nor is it rare. Associating numeric values with related values—oh, I don’t know…think of a user ID and an email address—is in essence the core of relational database systems (RDBMS). One may even have the gall to argue that RDBMS power many of the tools I use and take for granted. I would most likely be willing to concede that point.\nThe third gripe can be quite easily countered if I am to be frank. Missing data is in of itself data. An NA is an NA is an NA may not be a generalizable statement. Providing values such as 9999 in place of a missing value in some cases may be a relic of antiquity where missingness could not be handled by software. Or, perhaps we can frame missingness other ways. Let’s imagine we are conducting a study and we want to keep track of why there was missing data. This could have been from a non-response, or a withdrawal, or erroneously entered data, or any other number of reasons. Keeping a record of that missing data may useful.\n\nGripe 1: numbers representing characters (or labels)\nSometimes I really would like stringsAsFactors = TRUE. Working with survey data tends to be one of those times. R has a robust method of creating, handling, and manipulating factors1 and because of this, we aren’t required to numerically encode our data. This may be a personal preference, but I really like to be reminded of what I am working with and seeing the factor levels clearly spelled out for me is quite nice.1 Check out forcats for working with factors.\nSince the data I am working with at BARI is confidential, I’ve found some SPSS data hosted by the University of Bath2 to illustrate this with.2 Anthony Horowitz wrote a rather fun murder mystery novel titled Magpie Murders which takes place in a small town outside of bath. I recommend it.\nReading in data is rather straightforward. SPSS data come with a .sav file extension. Following standard readr convention we can read a .sav file with haven::read_sav().\nNote: the syntax above is used for referencing an exported function from a namespace (package name). The syntax is pkgname::function().\nThe below line reads in the sav file from the University of Bath.\nNote: by wrapping an object assignment expression, such as the below, in parentheses the object is then printed (I just recently figured this out).\n\nlibrary(haven)\n\n# read in sav file\n(noise <- haven::read_sav(\"http://staff.bath.ac.uk/pssiw/stats2/noisedata.sav\"))\n\nThe above shows GENDER codes as a numeric value. But if you print out the tibble to your console you will see labels. So in this case, where there is a 1 under GENDER, the printed console shows [male], and the same is true for 2 and [female]. We can get a sense of this by viewing the structure of the tibble.\n\nstr(noise)\n\nAbove we can see that GENDER has an attribute labels which is a named numeric vector. The unique values are 1 and 2 representing “male” and “female” respectively. I struggle to keep this mental association. I’d prefer to have this shown explicitly. Fortunately, haven provides the function haven::as_factor() which will convert these pesky integer columns to their respective factor values. We just need to pass the data frame as the only argument—sure, there are other arguments if you want to get fancy.\n\n# selecting just 2 columns and 2 rows for simlicity.\nsmall_lil_df <- slice(noise, 1, 20) %>% \n  select(1, 2)\n\n# convert coded response to response text\nhaven::as_factor(small_lil_df)\n\nAnd now we can just forget those integers ever existed in the firstplace!\n\n\nGripe 2: uninformative column names\nThere are three key rules to tidy data.\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nEach variable forms a column. Got it. So this means that any thing that can help describe our observation should be a column. Say we have a table of survey respondents. In this case each row should be a respondent and each column should be a variable (or feature, or predictor, or x, or whatever) associate with that respondent. This could be something like age, birth date, or the respondents response to a survey question.\nIn the tidyverse style guide Hadley Wickham writes\n\nGenerally, variable names should be nouns and function names should be verbs. Strive for names that are concise and meaningful (this is not easy!).\n\nI personaly try to extend this to column names as well. Feature names are important so I, as a researcher, can remember what is what. From my encounters with SPSS data, I’ve found that feature names can be rather uninformative e.g. “Q12.” Much like factors, columns may have associated information hidden somewhere within them.\nWe read in the personality data set from the Universit of Bath below.\n\n# read in sav file with column labels\n(personality <- haven::read_sav(\"http://staff.bath.ac.uk/pssiw/stats2/personality.sav\"))\n\nThe first thing that I notice is rather apalling: each column represents a person. oof, untidy. But this issue isn’t what brought me to these data. If you print the data to the console, you see something similar as what is above. If you view (View(df)) the data, the story is different. There is associated information underneath each column header.\n\nstr(personality[,1:5])\n\nYikes. These labels seem to be character judgements! Whatever the labels represent, I want them, and I want them as the column headers.\nFrom looking at the structure of the data frame we can glean that each column has a label.\nWarning: I’m going to walk through a fair bit of the theory and under the hood work to make these labels column names. Scroll to the bottom of this section to find the function definition.\n\nAside: R theory:\n\nEach column of a data frame is actually just a vector. Each vector can have it’s own attributes (as above).\nA data frame is actually just a list of equal length vectors (same number of observations).3\n“All objects can have arbitrary additional attributes, used to store metadata about the object.”4\nWe can fetch list elements using [[ notation, e.g. my_list[[1]]\n\npurrr::pluck() is an alternative to using [[ for grabbing the underlying elements inside of a data structure. This means we can use pluck(my_list, 1) in place of my_list[[1]]\n\n\n3 http://adv-r.had.co.nz/Data-structures.html#data-frames4 http://adv-r.had.co.nz/Data-structures.html#attributesOkay, but how does one actually get the label from the vector? The first step is to actually grab the vector. Below I use purrr::pluck()to pull the first column. Note that slice() is used for grabing specific row indexes. The below code is equivalent to personality[1:10,][[1]]. I prefer using the purrr functions because they are more legible.\n\ncol_1 <- slice(personality, 1:10) %>% \n  pluck(1)\n\nWe can access all of the vectors attributes with, you guessed it, the attributes() function.\n\nattributes(col_1)\n\nThis returns a named list. We can access (or set) specific attributes using the attr() function. The two arguments we must supply are x, the object, and which, which attribute we seek. In this case the values are col_1 and label respectively.\n\nattr(col_1, \"label\")\n\npurrr yet again makes working with list objects easy. purrr exports a function factory5 called purrr::attr_getter(). This function generates a function which accesses specific attributes. We can create a function get_label() using attr_getter() all we have to do is tell it which attribute we would like.5 A function factory is an object which creates other function objects.\n\n# generate a get_label f(x) via purr\nget_label <- purrr::attr_getter(\"label\")\n\nget_label(col_1)\n\nWell, lovely. Let’s just use this on our personality data frame.\n\nget_label(personality)\n\nOpe. Welp. That didn’t work. We should just give up 🤷.\n\nThe reason this didn’t work is because we tried to use get_label() on a tibble which didn’t have the “label” attribute. We can verify this by looking at the list names of the attributes of personality.\n\nnames(attributes(personality))\n\nBut what about the attribtues of each column? We can iterate over each column using map() to look at the attributes. Below I iterate over the first five columns. More on map()6.6 https://adv-r.hadley.nz/functionals.html#map\n\nmap(select(personality, 1:5), ~names(attributes(.)))\n\nWell, now that we’ve iterated over the columns and illustrated that the attributes live there, why not iterate over the columns and use get_label()?\n\n# use get_label to retrieve column labels\nmap_chr(select(personality, 1:5), get_label)\n\nAgain, yikes @ the labels. Let’s store these results into a vector so we can rename the original columns.\n\npers_labels <- map_chr(personality, get_label)\n\nWe can now change the names using setNames() from base R. We will then make the column headers tidy (personal definition of tidy column names) using janitor::clean_names().\n\nsetNames(personality, pers_labels) %>% \n  janitor::clean_names() %>% \n  select(1:5)\n\nIn the case that a column doesn’t have a label, get_label() will return NULL and then setNames() will fail. To work around this, you can use the name of the column rather than the label value. Below is a function definition which handles this for you and, optionally, lets you specify which columns to rename based on a regex pattern. I think we’ve done enough list manipulation for the day. If you have questions about the function definition I’d be happy to work through it one on one with you via twitter DMs.\n\nlabel_to_colname <- function(df, pattern) {\n  get_label <- purrr::attr_getter(\"label\")\n  col_labels <- purrr::map(df, get_label)\n\n  col_labels[unlist(map(col_labels, is.null))]  <- names(col_labels[unlist(purrr::map(col_labels, is.null))])\n\n  if (missing(pattern)) {\n    names_to_replace <- rep(TRUE, ncol(df))\n  } else {\n    names_to_replace <- stringr::str_detect(names(col_labels), pattern)\n  }\n  \n  colnames(df)[names_to_replace] <- janitor::make_clean_names(unlist(col_labels[names_to_replace]))\n\n  haven::zap_label(df)\n}\n\nlabel_to_colname(personality) %>% \n  select(1:5)\n\n# heh.\nlabel_to_colname(personality, \"PERS37\") %>% \n  select(37)\n\n\n\n\nGripe 3: user defined missing values\nI’ll keep this one short. If there are user defined missing values in a .sav file, you can encode these as NA by setting the user_na arugment to TRUE.\n\n# there aren't any missing values but you get the idea\nnoise <- haven::read_sav(\"http://staff.bath.ac.uk/pssiw/stats2/noisedata.sav\",\n                         user_na = TRUE)\n\nAnd if for any reason that did not suffice, you can replace missing values with replace_na()7.7 https://tidyr.tidyverse.org/reference/replace_na.html"
  },
  {
    "objectID": "posts/2019-12-14-spss-haven.html#take-aways",
    "href": "posts/2019-12-14-spss-haven.html#take-aways",
    "title": "Finding an SPSS {haven}",
    "section": "Take aways",
    "text": "Take aways\n\nAll data is messy in it’s own way.\nhaven::read_sav() will read SPSS data.\nhaven::as_factor() will apply column labels in place of the numeric values (if present).\n\nReplace user defined NA values by setting user_na = TRUE i.e. haven::read_sav(\"filepath.sav\", user_na = TRUE)\nAll R objects can have attributes.\nYou can access attributes using attributes() or attr().\nData frames are made of vectors.\nData frames are actually just lists masquerading as rectangles."
  },
  {
    "objectID": "posts/2019-06-11-trendyy-campaigns.html",
    "href": "posts/2019-06-11-trendyy-campaigns.html",
    "title": "Google Trends for Campaigns",
    "section": "",
    "text": "Over the past few years we have seen Google Trends becoming quite ubiquitous in politics. Pundits have used Google seach trends as talking points. It is not uncommon to hear news about a candidates search trends the days following a town hall or significant rally. It seems that Google trends are becoming the go to proxy for a candidate’s salience.\nAs a campaign, you are interested in the popularity of a candidate relative to another one. If candidate A has seen a gain from 50 to 70, that is all well and good. But how does that compare with candidates C and D? There are others potential use cases—that may be less fraught with media interruptions. For example, one can keep track of the popularity of possible policy issues—i.e. healthcare, gun safety, women’s rights.\nThough the usefulness of Google Trends search popularity is still unclear, it may be something that your campaign might like to track. In this chapter we will explore how to acquire and utilize trend data using R. This chapter will describe how one can utilize Google Trends data to compare candidate search popularity and view related search terms. This will be done with the tidyverse, and the package trendyy for accessing this data."
  },
  {
    "objectID": "posts/2019-06-11-trendyy-campaigns.html#google-trends-data",
    "href": "posts/2019-06-11-trendyy-campaigns.html#google-trends-data",
    "title": "Google Trends for Campaigns",
    "section": "Google Trends Data",
    "text": "Google Trends Data\n\nRelative Popularity\nThe key metric that Google Trends provides is the relative popularity of a search term by a given geography. Relative search popularity is scaled from 0 to 100. This number is scaled based on population and geography size (for more information go here). This number may be useful on it’s own, but the strength of Google Trends is it’s ability to compare multiple terms. Using Google Trends we can compare up to 5 search terms—presumably candidates.\n\n\nRelated Queries\nIn addition to popularity, Google Trends provides you with related queries. This can help your media team understand in what context their candidate is being associated online."
  },
  {
    "objectID": "posts/2019-06-11-trendyy-campaigns.html#trendyy",
    "href": "posts/2019-06-11-trendyy-campaigns.html#trendyy",
    "title": "Google Trends for Campaigns",
    "section": "trendyy",
    "text": "trendyy\nNow that we have an intuition of how Google Trends may be utilized, we will look at how actually acquire these data in R. To get started install the package using install.packages(\"trendyy\").\nOnce the package is installed, load the tidyverse and trendyy.\n\nlibrary(trendyy)\nlibrary(tidyverse)\n\nIn this example we will look at the top five polling candidates as of today (6/10/2019). These are, in no particular order, Joe Biden, Kamala Harris, Beto O’Rourke, Bernie Sanders, and Elizabeth Warren. Create a vector with the search terms that you will use (in this case the above candidates).\n\ncandidates <- c(\"Joe Biden\", \"Kamala Harris\", \"Beto O'Rourke\", \"Bernie Sanders\", \"Elizabeth Warren\")\n\nNext we will use the trendyy package to get search popularity. The function trendy() has three main arguments: search_terms, from, and to (in the form of \"yyyy-mm-dd\"). The first argument is the only mandatory one. Provide a vector of length 5 or less as the first argument. Here we will use the candidates vector and look at data from the past two weeks. I will create two variables for the beginning and end dates. This will be to demonstrate how functions can be used to programatically search date ranges.\n\n# to today\nend <- Sys.Date()\n# from 2 weeks ago\nbegin <- Sys.Date() - 14\n\nPass these arguments to trendy() and save them to a variable.\n\ncandidate_trends <- trendy(search_terms = candidates, from = begin, to = end)\n\ncandidate_trends\n#> ~Trendy results~\n#> \n#> Search Terms: Joe Biden, Kamala Harris, Beto O'Rourke, Bernie Sanders, Elizabeth Warren\n#> \n#> (>^.^)> ~~~~~~~~~~~~~~~~~~~~ summary ~~~~~~~~~~~~~~~~~~~~ <(^.^<)\n#> # A tibble: 5 × 5\n#>   keyword          max_hits min_hits from       to        \n#>   <chr>               <int>    <int> <date>     <date>    \n#> 1 Bernie Sanders         12        4 2022-10-31 2022-11-10\n#> 2 Beto O'Rourke           6        1 2022-10-31 2022-11-10\n#> 3 Elizabeth Warren        3        1 2022-10-31 2022-11-10\n#> 4 Joe Biden             100       42 2022-10-31 2022-11-10\n#> 5 Kamala Harris          10        5 2022-10-31 2022-11-10\n\nTrendy creates an object of class trendy see class(candidate_trends) trendy. There are a number of accessor functions. We will use get_interest() and get_related_queries(). See the documentation of the others.\nTo access to relative popularity, we will use get_interest(trendy).\n\npopularity <- get_interest(candidate_trends)\n\npopularity\n#> # A tibble: 55 × 7\n#>    date                 hits keyword   geo   time                  gprop category      \n#>    <dttm>              <int> <chr>     <chr> <chr>                 <chr> <chr>         \n#>  1 2022-10-31 00:00:00    48 Joe Biden world 2022-10-31 2022-11-14 web   All categories\n#>  2 2022-11-01 00:00:00    42 Joe Biden world 2022-10-31 2022-11-14 web   All categories\n#>  3 2022-11-02 00:00:00    51 Joe Biden world 2022-10-31 2022-11-14 web   All categories\n#>  4 2022-11-03 00:00:00    52 Joe Biden world 2022-10-31 2022-11-14 web   All categories\n#>  5 2022-11-04 00:00:00    47 Joe Biden world 2022-10-31 2022-11-14 web   All categories\n#>  6 2022-11-05 00:00:00    45 Joe Biden world 2022-10-31 2022-11-14 web   All categories\n#>  7 2022-11-06 00:00:00    46 Joe Biden world 2022-10-31 2022-11-14 web   All categories\n#>  8 2022-11-07 00:00:00    52 Joe Biden world 2022-10-31 2022-11-14 web   All categories\n#>  9 2022-11-08 00:00:00    67 Joe Biden world 2022-10-31 2022-11-14 web   All categories\n#> 10 2022-11-09 00:00:00   100 Joe Biden world 2022-10-31 2022-11-14 web   All categories\n#> # … with 45 more rows\n#> # ℹ Use `print(n = ...)` to see more rows\n\nFor related queries we will use get_related_queries(trendy). Note that you can either pipe the object or pass it directly.\n\ncandidate_trends %>% \n  get_related_queries() %>% \n  # picking queries for a random candidate\n  filter(keyword == sample(candidates, 1))\n#> # A tibble: 42 × 5\n#>    subject related_queries value                    keyword        category      \n#>    <chr>   <chr>           <chr>                    <chr>          <chr>         \n#>  1 100     top             election                 Bernie Sanders All categories\n#>  2 68      top             big mouth bernie sanders Bernie Sanders All categories\n#>  3 61      top             biden                    Bernie Sanders All categories\n#>  4 56      top             election results         Bernie Sanders All categories\n#>  5 47      top             bernie sanders meme      Bernie Sanders All categories\n#>  6 37      top             bernie sanders party     Bernie Sanders All categories\n#>  7 33      top             bernie sanders age       Bernie Sanders All categories\n#>  8 33      top             joe biden                Bernie Sanders All categories\n#>  9 33      top             trump                    Bernie Sanders All categories\n#> 10 30      top             donald trump             Bernie Sanders All categories\n#> # … with 32 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/2019-06-11-trendyy-campaigns.html#visualizing-trends",
    "href": "posts/2019-06-11-trendyy-campaigns.html#visualizing-trends",
    "title": "Google Trends for Campaigns",
    "section": "Visualizing Trends",
    "text": "Visualizing Trends\nI’m guessing your director enjoys charts—so do I. To make the data more accessible, use the popularity tibble to create a time series chart of popularity over the past two weeks. We will use ggplot2. Remember that time should be displayed on the x axis. We want to have a line for each candidate, so map the color aesthetic to the keyword.\n\nggplot(popularity, \n       aes(x = date, y = hits, color = keyword)) + \n  geom_line() +\n  labs(x = \"\", y = \"Search Popularity\", \n       title = \"Google popularity of top 5 polling candidates\") + \n  theme_minimal() +\n  theme(legend.position = \"bottom\", \n        legend.title = element_blank())"
  },
  {
    "objectID": "posts/2022-05-11-make-your-r-scripts-databricks-notebooks.html",
    "href": "posts/2022-05-11-make-your-r-scripts-databricks-notebooks.html",
    "title": "Make your R scripts Databricks notebooks",
    "section": "",
    "text": "I’ve never had a good reason to deviate from the canonical .R file extension until today.\nAs you may have seen over the past few month from my numerous rage-tweets and Databricks related threads, I’ve been doing a lot of work getting figuring out Databricks as an R user so we can get onboard with adoption here at NPD.\nOne of my biggest qualms about Databricks is that it’s tailored to their notebooks. The notebooks get magical superpowers that aren’t available anywhere else. Notebooks get root permissions, they have access to dbutils, and are the only thing that can actually be scheduled by Databricks outside of a jar file or SparkSQL code.\nI’ve spent quite a bit of time thinking about how we can schedule R scripts through a notebook. If you’re wondering, have the notebook kickoff the R script with a shell command.\nBut, alas, I’ve learned something today. If you connect your Git repo to Databricks through their \"Repos\", you can have your R scripts be accessible as notebooks with quite literally only two changes.\nFirst, R scripts need to have the less-preferable, though equally functional, file extension .r. Second, the first line of the script should be a comment that says # Databricks notebook source. And that’s it. Then once the git repo has been connected, it will recognize those as notebooks.\nIf you want to create cells in your code write a comment # COMMAND ----------—that’s 10 hyphens at the end.\nIf you create a file main.r which contains the body\n# Databricks notebook source\n\nprint(\"Hello world\")\n\n# COMMAND ---------\n\nprint(\"Databricks! I've figured you out—sorta....\")\n\n# COMMAND ---------\n\nprint(\"I feel powerful.\")\nYou will have an R script that is recognized as a notebook by Databricks that can be scheduled using Databricks’ scheduling wizard."
  },
  {
    "objectID": "posts/2022-04-05-an-open-system-requirements-database.html",
    "href": "posts/2022-04-05-an-open-system-requirements-database.html",
    "title": "Actually identifying R package System Requirements",
    "section": "",
    "text": "During my approximately three years at RStudio there were two things that stumped system admins more than anything: proxied authentication and system dependencies for R package (god help anyone trying to install rgdal on RHEL 7). When RStudio Package Manager (RSPM) v1.0.8 was released there was finally an answer. RSPM can help you identify system requirements via the GUI. Also, there’s a restful API that isn’t fully supported but can provide these system requirements programatically if needed. As such, I think it is still a little used feature for most RSPM users and admins.\n{pak} did a great job of providing an interface to the public installation of RSPM. Back in May 2021 I suggested that the API calls to RSPM be made publicly available. Since then pak::pkg_system_requirements() has become an exported function. It is exceptionally useful. I use it in my current work to create a bash script which installs system requirements into a fresh Databricks compute cluster and then install R packages from RSPM.\nOne of my qualms about the RSPM API and thus the output of pak is that it always includes the installation commands for the provided OS–e.g. apt-get install -y which I suppose could be easily stringr::str_remove()d.\nThe second qualm has been that this relies on RSPM. The logic has been semi-hidden behind a closed-source tool. However, RStudio maintains a repository r-system-requirements which is used by RSPM to identify system requirements from a packages DESCRIPTION file.\nAll of the logic for RSPM is in that repository. And that’s why I made https://r-sysreqs.josiahparry.com. It’s a way to provide the REST API functionality from RSPM without having to rely strictly on the public installation.\nUsers can use the functionality from {sysreqs} to make this api available on their own machines.\n\nsysreqs::get_pkgs_sysreqs(c(\"rgdal\", \"igraph\"),\n                          \"ubuntu\", \"18.04\") |> \n  tidyr::unnest(dependencies)\n#> # A tibble: 6 × 5\n#>   pkg    name    dependencies pre_installs post_installs\n#>   <chr>  <chr>   <chr>        <chr>        <chr>        \n#> 1 rgdal  gdal    libgdal-dev  NA           NA           \n#> 2 rgdal  gdal    gdal-bin     NA           NA           \n#> 3 rgdal  proj    libproj-dev  NA           NA           \n#> 4 igraph glpk    libglpk-dev  NA           NA           \n#> 5 igraph gmp     libgmp3-dev  NA           NA           \n#> 6 igraph libxml2 libxml2-dev  NA           NA"
  },
  {
    "objectID": "posts/2021-04-28-user-apr-27-2021.html",
    "href": "posts/2021-04-28-user-apr-27-2021.html",
    "title": "{cpcinema} & associated journey",
    "section": "",
    "text": "Yesterday I had the chance to discuss my R package, {cpcinema} with the Boston useR group. If you’re not familiar with {cpcinema}, please read my previous post. I intended for my talk to be focused on the package’s functionality, how / why I made it, and then briefly on why I didn’t share it widely, my feelings after my seeing the response to my tweet, and why contributing to the open source community in any manner is always appreciated. But as I was preparing my slides on Monday night I encountered some challenges. That became the subject of much of my talk.\nThe talk can be found here and the passcode is D=L?nHQ0.\n\n\n\n\nAfter providing a brief overview of the functionality of {cpcinema} I touched upon how this idea formed in my head. My head is like an unorganized floating abyss with numerous ideas, concepts, and facts just floating. Occasionally I’ll make a connection between two or more ideas. This will happen over a long period of time. In the case of this package I had a number of thoughts regarding data visualization—probably because I had seen Alberto Cairo present at Northeastern towards the end of 2019.\n\nGraphs are informative\nGraphs can be boring\nPeople like pretty things\nPretty graphs are just as informative as normal graphs\nMake graphs pretty and people will enjoy them\n\nAdditionally, I had come to the realization that most data scientists want to make informative charts, but also aren’t going to be exceptionally adept at the design portion of this—nor should they be! Moving beyond the default colors is an important first step in making evocative visualizations.\nSometime in between, I discovered or was shown the instagram page @colorpalette.cinema which provides beautiful color palettes from stills of films. How sweet would it be to use those colors for your plots? Rather sweet.\nIn January at rstudio::conf(2020L) I saw Jesse Sadler present on making custom S3 vector classes with the {vctrs} package (slides). After seeing this I was toying with the idea of making my own custom S3 vector class.\nThen in February it all clicked. How cool would it be to get a colorpalette.cinema picture, extract the colors, and then store them in a custom S3 class which printed the color?! So I did that with hours of struggling and a lot of following along with Jesse’s resources.\nTo start any project of any sort, always **start small*. My workflow consisted of:\n\nDownloading a sample image to work with.\nScour StackOverflow, Google, and Rbloggers for resources on extracting colors.\nExtract the colors from an image manually.\nExtract colors from an image path.\nMake it into a function to replicate.\nTry and get an image from Instagram URL.\n\nWell, apparently you can’t! I tried and I tried to find different ways to extracting these images. But it wasn’t possible. The Instagram API is meant for web developers and businesses, not data scientists trying to get small pieces of data. I left it alone and left it to myself.\nAfter my tweet, I was inspired to find a solution to the Instagram image URL problem. And with a lot of coffee, wifi hotpsotting, the Chrome devtools, I found a workable solution.\nThen Monday night I was making slides and…\n\npal_from_post(\"https://www.instagram.com/p/CMC26FaHbqP/\")\n\nError in download.file(fp, tmp, mode = \"wb\", quiet = TRUE) : \n  cannot open URL 'NA'\nThe function pal_from_post() didn’t work. I did what I always do when a function doesn’t work. I printed out the function body to figure out what the function does.\n\ncpcinema::pal_from_post\n#> function (post_url) \n#> {\n#>     res <- httr::POST(\"https://igram.io/api/\", body = list(url = post_url, \n#>         lang_code = \"en\", vers = \"2\"), encode = \"form\") %>% httr::content()\n#>     imgs <- res %>% rvest::html_nodes(\".py-2 img\") %>% rvest::html_attr(\"src\") %>% \n#>         unique()\n#>     img_paths <- imgs[2:length(imgs)]\n#>     purrr::map(img_paths, extract_cpc_pal)\n#> }\n#> <bytecode: 0x294dac290>\n#> <environment: namespace:cpcinema>\n\nAlways start the beginning. I ran the original API query and I received the following\n\nnonation <- \"https://www.instagram.com/p/CNdEkV4HAXF\"\n\nres <- httr::POST(\"https://igram.io/api/\",\n                  body = list(url = nonation,\n                              lang_code = \"en\",\n                              vers = \"2\"),\n                  encode  = \"form\") \n\nResponse [https://igram.io/api/]\n  Date: 2021-04-26 22:46\n  Status: 403\n  Content-Type: text/html; charset=UTF-8\n  Size: 3.26 kB\n<!DOCTYPE html>\n<!--[if lt IE 7]> <html class=\"no-js ie6 oldie\" lang=\"en-US\"> <![endif]-->\n<!--[if IE 7]>    <html class=\"no-js ie7 oldie\" lang=\"en-US\"> <![endif]-->\n<!--[if IE 8]>    <html class=\"no-js ie8 oldie\" lang=\"en-US\"> <![endif]-->\n<!--[if gt IE 8]><!--> <html class=\"no-js\" lang=\"en-US\"> <!--<![endif]-->\n<head>\n<title>Access denied | igram.io used Cloudflare to restrict access</title>\n<meta charset=\"UTF-8\" />\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge,chrome=1\" />\n...\nMy next step here was to see if it was just me. Next I went to use RStudio Server Pro which has a different IP address. The same query had the different response\nResponse [https://igram.io/api/]\n  Date: 2021-04-26 22:43\n  Status: 503\n  Content-Type: text/html; charset=UTF-8\n  Size: 9.48 kB\n<!DOCTYPE HTML>\n<html lang=\"en-US\">\n<head>\n  <meta charset=\"UTF-8\" />\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge,chrome=1\" />\n  <meta name=\"robots\" content=\"noindex, nofollow\" />\n  <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n  <title>Just a moment...</title>\n  <style type=\"text/css\">\nA different error code. I saw this coming.\nI was web scraping a website that was unquestionably also web scraping. This is a very grey zone of the internet and is questionable at best. These sources are ephemeral at best.\n\n“If it doesn’t work, try the same thing again until it does work” - Me\n\nI went to the same website I was using. I opened opened up the developer tools and watched the requests come in! The request that the browser uses had a change in their url! From https://igram.io/api/ to https://igram.io/i/. Frankly, a super easy fix!\nIt’s now Wednesday and I still haven’t made that change. So, what’s next? (HELP WANTED)\n\nChange the API url in pal_from_post()\nIntegrate with ggplot2 better. Inspo can be taken from {paletteer}\nColor sorting!\nBetter type casting!"
  },
  {
    "objectID": "posts/2021-06-18-sliced-a-brief-reflection.html",
    "href": "posts/2021-06-18-sliced-a-brief-reflection.html",
    "title": "SLICED! a brief reflection",
    "section": "",
    "text": "A few weeks ago I was a contestant on the machine learning game show #SLICED! The format of the challenge is as follows:\nMy stream is uploaded to youtube so you can catch it in all of its glory."
  },
  {
    "objectID": "posts/2021-06-18-sliced-a-brief-reflection.html#how-i-got-roped-in",
    "href": "posts/2021-06-18-sliced-a-brief-reflection.html#how-i-got-roped-in",
    "title": "SLICED! a brief reflection",
    "section": "How I got roped in",
    "text": "How I got roped in\nAbout a month ago I saw the below tweet from Jesse Mostipak. Naturally, it piqued my interest.\n\n\nCome play with meeeeee! I promise to make you look good. https://t.co/U7DRXM8deP\n\n— Jesse Mostipak is making mirepoix for #SLICED (@kierisi) May 8, 2021\n\n\nEveryone’s favorite Tidy-Tuesday-Tom essentially voluntold me. I decided to put my name in the hat and see if I can compete. The challenge, though, is that I have only ever dabbled in machine learning. In May it was something that I had only done a handful of times and with a much older toolset-e.g. caret. If there is one thing I know about myself, it’s that there is nothing like a deadline and a concrete objective to get me to learn something.\nI am a strong believer in Parkinson’s Law—you can thank my father for that—which is characterized by the saying “If you wait until the last minute, it only takes a minute to do.” Or, more formally, “work expands so as to fill the time available for its completion.”\nIn essence, the best way for me to get better at machine learning would be to put myself in a situation—as uncomfortable it may be—where I would have to do machine learning. Alternatively, I could just faily miserably but I don’t like that."
  },
  {
    "objectID": "posts/2021-06-18-sliced-a-brief-reflection.html#getting-a-grip-on-tidymodels",
    "href": "posts/2021-06-18-sliced-a-brief-reflection.html#getting-a-grip-on-tidymodels",
    "title": "SLICED! a brief reflection",
    "section": "Getting a grip on {tidymodels}",
    "text": "Getting a grip on {tidymodels}\nI have been loosely following the tidymodels ecosystem since the beginning. Previously my understanding of tidymodels only included, recipes, rsample, and parsnip. These three packages can get you exceptionally far, but there are so many additional packages that are instrumental to improving the ML workflow for useRs. These are tune, workflows, and workflowsets.\nThe most challenging part of getting started with tidymodels was understanding where each package fits in during the process. The challenging task was to figure out which packages were low level APIs and which were abstractions."
  },
  {
    "objectID": "posts/2021-06-18-sliced-a-brief-reflection.html#understanding-tidymodels-libraries",
    "href": "posts/2021-06-18-sliced-a-brief-reflection.html#understanding-tidymodels-libraries",
    "title": "SLICED! a brief reflection",
    "section": "Understanding tidymodels libraries",
    "text": "Understanding tidymodels libraries\nThe most basic component of a tidymodel ML process is a recipe ({recipes}) and a model specification ({parsnip}). The recipe determines the features used and any preprocessing steps. The model specification determines which model will be trained. Additionally, we often want to include resampling—e.g. bootstrap or cross validation (called a resamples ({rsample}) object in tidymodels). With these three components we can then utilize the {tune} package to train our model on our resamples. We can build a layer of abstraction from these four components which is called a workflow.\nIn the ML process we want to train many models. And rather than just repeating the steps manually for each model, the package workflowsets will create many workflows for you and help you train all of those models quickly. Workflowsets were essential in my approach to sliced."
  },
  {
    "objectID": "posts/2021-06-18-sliced-a-brief-reflection.html#tidymodels-resources",
    "href": "posts/2021-06-18-sliced-a-brief-reflection.html#tidymodels-resources",
    "title": "SLICED! a brief reflection",
    "section": "Tidymodels resources",
    "text": "Tidymodels resources\nGetting up to speed with tidymodels was a bit of a challenge. The packages are still actively under development so building corresponding documentation must be a bit of a challenge for the team! With that said, there are so many resources that you can use to get started. Below are some of the ones that I utilized and found helpful.\n\nTidymodeling with R\nWorkflowsets vignette\nJulia Silge’s blog\nAnd a bunch of the help docs"
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "",
    "text": "Installing python has never been an easy task for me. I remember back in 2016 I wanted to learn how to use pyspark and thus python, I couldn’t figure out how to install python so I gave up. In graduate school I couldn’t install python so I used a docker container my professor created and never changed a thing. When working at RStudio I used the Jupyter Lab instance in RStudio Workbench when I couldn’t install it locally.\nNow, I want to compare pysal results to some functionality I’ve written in R. To do that, I need a python installation. I’ve heard extra horror stories about installing Python on the new Mac M1 chip—which I have."
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#installing-python",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#installing-python",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Installing Python",
    "text": "Installing Python\nThe steps to install python, at least for me, was very simple.\n\nInstall reticulate\nInstall miniconda\n\ninstall.packages(\"reticulate\")\nreticulate::install_miniconda()\nThat’s it. That’s all it took."
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#creating-my-first-conda-environment",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#creating-my-first-conda-environment",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Creating my first conda environment",
    "text": "Creating my first conda environment\nAfter installing python, I restarted R, and began building my first conda environment. I created a conda environment called geo for my geospatial work. I installed libpysal, geopandas, and esda. These installed every other dependency I needed–e.g. pandas, and numpy.\nreticulate::conda_create(\"geo\")\nreticulate::use_condaenv(\"geo\")\nreticulate::conda_install(\"geo\", c(\"libpysal\", \"geopandas\", \"esda\"))"
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#using-my-conda-environment",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#using-my-conda-environment",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Using my conda environment",
    "text": "Using my conda environment\nTo begin using my new conda environment, I opened up a fresh R session and a fresh R Markdown document. In my first code chunk I told reticulate which conda environment to use. Then my following code chunks were python which opened up the python repl. Make sure that you start your code chunk with ```{python}\n\nreticulate::use_condaenv(\"geo\")\n\nIn the following example I utilize esda to calculate a local join count.\n\nimport libpysal\nimport geopandas as gpd\nfrom esda.join_counts_local import Join_Counts_Local\n\nfp = libpysal.examples.root + \"/guerry/\" + \"Guerry.shp\" \n\nguerry_ds = gpd.read_file(fp)\nguerry_ds['SELECTED'] = 0\nguerry_ds.loc[(guerry_ds['Donatns'] > 10997), 'SELECTED'] = 1\n\nw = libpysal.weights.Queen.from_dataframe(guerry_ds)\n\nLJC_uni = Join_Counts_Local(connectivity=w).fit(guerry_ds['SELECTED'])\n\nLJC_uni.p_sim\n\n## array([  nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan, 0.435,   nan, 0.025, 0.025,   nan, 0.328,\n##          nan,   nan,   nan,   nan,   nan,   nan, 0.342,   nan, 0.334,\n##          nan,   nan,   nan,   nan,   nan,   nan, 0.329,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan, 0.481,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan, 0.02 ,   nan,   nan,   nan,   nan,   nan, 0.125,\n##          nan, 0.043,   nan,   nan])"
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html",
    "href": "posts/2022-10-03-spacetime-representations.html",
    "title": "spacetime representations aren’t good—yet",
    "section": "",
    "text": "My beliefs can be summarized somewhat succinctly.\nWe should not limit space-time data to dates or timestamps.\nThe R ecosystem should always utilize a normalized approach as described above. Further, a representation should use friendly R objects. The friendliest object is a data frame. A new representation should allow context switching between geometries and temporal data. That new representation should always use time-long formats and the geometries should never be repeated.\nA spacetime representation should give users complete and total freedom to manipulate their data as they see fit (e.g. dplyr or data.table operations).\nThe only time to be strict in the format of spacetime data is when statstics are going to be derived from the data."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#background",
    "href": "posts/2022-10-03-spacetime-representations.html#background",
    "title": "spacetime representations aren’t good—yet",
    "section": "Background",
    "text": "Background\nWhile implementing emerging hotspot analysis in sfdep I encountered the need for a formalized spacetime class in R. As my focus in sfdep has been tidyverse-centric functionality, I desired a “tidy” data frame that could be used as a spacetime representation. Moreover, space (in the spacetime representation) should be represented as an sf or sfc object. In sfdep I introduced the new S3 class spacetime based on Edzer Pebesma’s 2012 article “spacetime: Spatio-Temporal Data in R” and Thomas Lin Pederson’s tidygraph package."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#representations-of-spatial-data",
    "href": "posts/2022-10-03-spacetime-representations.html#representations-of-spatial-data",
    "title": "spacetime representations aren’t good—yet",
    "section": "Representations of Spatial Data",
    "text": "Representations of Spatial Data\nBefore describing my preferences in a spacetime representation in R, I want to review possible representations of spacetime data.\nPebesma (2012) outlines three tabular representations of spatio-temporal data.\n\n“Time-wide: Where different columns reflect different moments in time.\n\nSpace-wide: Where different columns reflect different measurement locations or areas.\n\nLong formats: Where each record reflects a single time and space combination.\n\nThe “long format” is what we may consider “tidy” per Wickham (2014). In this case, both time and space are variables with unique combinations as rows.\nPebesma further qualifies spatial data representation into a “sparse grid” and a “full grid.” Say we have a variable X. In a spatio temporal full grid we will store all combinations of time (t) and locations (i) . If Xi is missing at any of those location and time combinations (Xit is missing), the value of X is recorded as a missing value. Whereas in a sparse grid, if there is any missing data, the observation is omitted. Necessarily, in a full grid there will be i x t number of rows. In a sparse grid there will be fewer than i x t rows.\nVery recently in an r-spatial blog post, “Vector Data Cubes”, Edzer describes another approach to representing spacetime using a database normalization approach. Database normalization is a process that reduces redundancy by creating a number of smaller tables containing IDs and values. These tables can then be joined only when needed. When we consider spacetime data, we have repeating geometries across time. It is inefficient to to keep multiple copies of the geometry. Instead, we can keep track of the unique ID of a geometry and store the geometry in another table."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#sfdep-spacetime-representation",
    "href": "posts/2022-10-03-spacetime-representations.html#sfdep-spacetime-representation",
    "title": "spacetime representations aren’t good—yet",
    "section": "sfdep spacetime representation",
    "text": "sfdep spacetime representation\nThe spacetime class in sfdep is in essence a database normalization approach (see above blog post). It is implemented with the database normalization approach and the ergonomics of tidygraph in mind.\nThe objective of the spacetime class in sfdep is to\n\nallow complete freedom of data manipulation via data.frame objects,\nprevent duplication of geometries,\nand provide leeway in what “time” can be defined as.\n\nSimilar to tidygraph, spacetime provides access to two contexts: data and geometry. The data context is a data frame and the geometry context. These are linked based on a unqie identifie that is present in both contexts.\nR code\n\nlibrary(dplyr)\n\ntimes <- seq(\n  Sys.time(), \n  Sys.time() + lubridate::hours(5),\n  length.out = 5\n)\n\nlocations <- c(\"001\", \"002\")\n\ndata_context <- tidyr::crossing(\n  location = locations,\n  time = times\n) |> \n  mutate(value = rnorm(n())) |> \n  arrange(location)\n\n\nlibrary(sf)\n\nLinking to GEOS 3.9.1, GDAL 3.2.3, PROJ 7.2.1; sf_use_s2() is TRUE\n\ngeometry_context <- st_sfc(\n  list(st_point(c(0, 1)), st_point(c(1, 1)))\n  ) |> \n  st_as_sf() |> \n  mutate(location = c(\"001\", \"002\"))\n\nUse the spacetime constructor\n\nlibrary(sfdep)\nspt <- spacetime(\n  .data = data_context,\n  .geometry = geometry_context, \n  .loc_col = \"location\", \n  .time_col = \"time\"\n)\n\nSwap contexts with activate\nactivate(spt, \"geometry\")\nspacetime ────\nContext:`geometry`\n2 locations `location`\n5 time periods `time`\n── geometry context ────────────────────────────────────────────────────────────\nSimple feature collection with 2 features and 1 field Geometry type: POINT Dimension: XY Bounding box: xmin: 0 ymin: 1 xmax: 1 ymax: 1 CRS: NA x location 1 POINT (0 1) 001 2 POINT (1 1) 002\nOne of my very strong beliefs is that temporal data does not, and should not, always be represented as a date or a timestamp. This paradigm is too limiting. What about panel data where you’re measuring cohorts along periods 1 - 10? Should these be represented as dates? No, definitely not. Because of this, sfdep allows you to utilize any numeric column that can be sorted.\n\nPerhaps I’ve just spent too much time listening to ecometricians…\n\nexample of using integers\n\nspacetime(\n  mutate(data_context, period = row_number()),\n  geometry_context, \n  .loc_col = \"location\",\n  .time_col = \"period\"\n)\n\nspacetime ────\n\n\nContext:`data`\n\n\n2 locations `location`\n\n\n10 time periods `period`\n\n\n── data context ────────────────────────────────────────────────────────────────\n\n\n# A tibble: 10 × 4\n   location time                 value period\n * <chr>    <dttm>               <dbl>  <int>\n 1 001      2022-11-15 08:23:08 -0.206      1\n 2 001      2022-11-15 09:38:08 -0.289      2\n 3 001      2022-11-15 10:53:08 -0.275      3\n 4 001      2022-11-15 12:08:08  0.136      4\n 5 001      2022-11-15 13:23:08 -1.48       5\n 6 002      2022-11-15 08:23:08  1.01       6\n 7 002      2022-11-15 09:38:08  2.11       7\n 8 002      2022-11-15 10:53:08 -1.68       8\n 9 002      2022-11-15 12:08:08  0.880      9\n10 002      2022-11-15 13:23:08  0.698     10"
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#qualifiers",
    "href": "posts/2022-10-03-spacetime-representations.html#qualifiers",
    "title": "spacetime representations aren’t good—yet",
    "section": "Qualifiers",
    "text": "Qualifiers\nI don’t think my spacetime class is the panacea. I don’t have the technical chops to make a great data format. I also don’t want to have that burden. Additionally, the class is desgned with lattice data in mind. I don’t think it is sufficient for trajectories or point pattern without repeating locations.\nThere’s a new R package called cubble for spatio-temporal data. I’ve not explored it. It may be better suited to your tidy-centric spatio-temporal data."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html",
    "title": "Medium Data and Production API Pipeline",
    "section": "",
    "text": "“[P]arsing huge json strings is difficult and inefficient.”1 If you have an API that needs to receive a large amount of json, sending that over will be slow.1 https://www.opencpu.org/posts/jsonlite-streaming/\nQ: How can we improve that? A: Compression."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#background",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#background",
    "title": "Medium Data and Production API Pipeline",
    "section": "Background",
    "text": "Background\nAn API is an application programming interface. APIs are how machines talk to other machines. APIs are useful because they are language agnostic meaning that the same API request from Python, or R, or JavaScript will work and return the same results. To send data to an API we use a POST request. The data that we send is usually required to be in json format."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#context",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#context",
    "title": "Medium Data and Production API Pipeline",
    "section": "Context",
    "text": "Context\nProblem: With large data API POST requests can become extremely slow and take up a lot of storage space. This can cause a bottleneck.\nSolution: Compress your data and send a file instead of sending plain text json."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#standard-approach",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#standard-approach",
    "title": "Medium Data and Production API Pipeline",
    "section": "Standard approach",
    "text": "Standard approach\nInteracting with an API from R is usually done with the {httr} package. Imagine you want to send a dataframe to an API as json. We can do that by using the httr::POST(), providing a dataframe to the body, and encoding it to json by setting encode = \"json\".\nFirst let’s load our libraries:\n\nlibrary(httr)          # interacts with apis\nlibrary(jsonlite)      # works with json (for later)\nlibrary(nycflights13)  # data for posting \n\nNext, let’s create a sample POST() request to illustrate how posting a dataframe as json works.\n\n\nb_url <- \"http://httpbin.org/post\" # an easy to work with sample API POST endpoint\n\nPOST(url = b_url, \n     body = list(x = cars),\n     encode = \"json\")\n#> Response [http://httpbin.org/post]\n#>   Date: 2022-11-14 22:14\n#>   Status: 200\n#>   Content-Type: application/json\n#>   Size: 4.81 kB\n#> {\n#>   \"args\": {}, \n#>   \"data\": \"{\\\"x\\\":[{\\\"speed\\\":4,\\\"dist\\\":2},{\\\"speed\\\":4,\\\"dist\\\":10},{\\\"speed\\\":7,\\\"...\n#>   \"files\": {}, \n#>   \"form\": {}, \n#>   \"headers\": {\n#>     \"Accept\": \"application/json, text/xml, application/xml, */*\", \n#>     \"Accept-Encoding\": \"deflate, gzip\", \n#>     \"Content-Length\": \"1150\", \n#>     \"Content-Type\": \"application/json\", \n#> ..."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#alternative-approach",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#alternative-approach",
    "title": "Medium Data and Production API Pipeline",
    "section": "Alternative approach",
    "text": "Alternative approach\nAn alternative approach would be to write our dataframe as json to a compressed gzip file. The process will be to:\n\nCreate a temporary file which will store our compressed json.\nCreate a gzip file connection to write the temporary file as a gzip.\nUpload the temporary file to the API.\nRemove the temporary file.\n\nWriting to a temporary gzipped file looks like:\n\n# create the tempfile \ntmp <- tempfile()\n\n# create a gzfile connection (to enable writing gz)\ngz_tmp <- gzfile(tmp)\n\n# write json to the gz file connection\nwrite_json(cars, gz_tmp)\n\n# close the gz file connection\nclose(gz_tmp)\n\nLet’s read the temp file to see what it contains.\n\n# read the temp file for illustration \nreadLines(tmp)\n#> [1] \"[{\\\"speed\\\":4,\\\"dist\\\":2},{\\\"speed\\\":4,\\\"dist\\\":10},{\\\"speed\\\":7,\\\"dist\\\":4},{\\\"speed\\\":7,\\\"dist\\\":22},{\\\"speed\\\":8,\\\"dist\\\":16},{\\\"speed\\\":9,\\\"dist\\\":10},{\\\"speed\\\":10,\\\"dist\\\":18},{\\\"speed\\\":10,\\\"dist\\\":26},{\\\"speed\\\":10,\\\"dist\\\":34},{\\\"speed\\\":11,\\\"dist\\\":17},{\\\"speed\\\":11,\\\"dist\\\":28},{\\\"speed\\\":12,\\\"dist\\\":14},{\\\"speed\\\":12,\\\"dist\\\":20},{\\\"speed\\\":12,\\\"dist\\\":24},{\\\"speed\\\":12,\\\"dist\\\":28},{\\\"speed\\\":13,\\\"dist\\\":26},{\\\"speed\\\":13,\\\"dist\\\":34},{\\\"speed\\\":13,\\\"dist\\\":34},{\\\"speed\\\":13,\\\"dist\\\":46},{\\\"speed\\\":14,\\\"dist\\\":26},{\\\"speed\\\":14,\\\"dist\\\":36},{\\\"speed\\\":14,\\\"dist\\\":60},{\\\"speed\\\":14,\\\"dist\\\":80},{\\\"speed\\\":15,\\\"dist\\\":20},{\\\"speed\\\":15,\\\"dist\\\":26},{\\\"speed\\\":15,\\\"dist\\\":54},{\\\"speed\\\":16,\\\"dist\\\":32},{\\\"speed\\\":16,\\\"dist\\\":40},{\\\"speed\\\":17,\\\"dist\\\":32},{\\\"speed\\\":17,\\\"dist\\\":40},{\\\"speed\\\":17,\\\"dist\\\":50},{\\\"speed\\\":18,\\\"dist\\\":42},{\\\"speed\\\":18,\\\"dist\\\":56},{\\\"speed\\\":18,\\\"dist\\\":76},{\\\"speed\\\":18,\\\"dist\\\":84},{\\\"speed\\\":19,\\\"dist\\\":36},{\\\"speed\\\":19,\\\"dist\\\":46},{\\\"speed\\\":19,\\\"dist\\\":68},{\\\"speed\\\":20,\\\"dist\\\":32},{\\\"speed\\\":20,\\\"dist\\\":48},{\\\"speed\\\":20,\\\"dist\\\":52},{\\\"speed\\\":20,\\\"dist\\\":56},{\\\"speed\\\":20,\\\"dist\\\":64},{\\\"speed\\\":22,\\\"dist\\\":66},{\\\"speed\\\":23,\\\"dist\\\":54},{\\\"speed\\\":24,\\\"dist\\\":70},{\\\"speed\\\":24,\\\"dist\\\":92},{\\\"speed\\\":24,\\\"dist\\\":93},{\\\"speed\\\":24,\\\"dist\\\":120},{\\\"speed\\\":25,\\\"dist\\\":85}]\"\n\n\nPOSTing a file\nTo post a file we use the function httr::upload_file(). The argument we provide is the path, in this case the file path is stored in the tmp object.\n\nPOST(b_url, body = list(x = upload_file(tmp)))\n#> Response [http://httpbin.org/post]\n#>   Date: 2022-11-14 22:14\n#>   Status: 200\n#>   Content-Type: application/json\n#>   Size: 874 B\n#> {\n#>   \"args\": {}, \n#>   \"data\": \"\", \n#>   \"files\": {\n#>     \"x\": \"data:text/plain;base64,H4sIAAAAAAAAA4XSPQ6DMAwF4L3HyMyQ+C8JV6m6wdCtEt0q7t6p...\n#>   }, \n#>   \"form\": {}, \n#>   \"headers\": {\n#>     \"Accept\": \"application/json, text/xml, application/xml, */*\", \n#>     \"Accept-Encoding\": \"deflate, gzip\", \n#> ...\n\n\n\nComparing R object to gzip\nNow, you may be asking, is this really that big of a difference? It actually is. If you’ll notice from the first response where we POSTed the cars dataframe the response size was 4.81kB. This response with the compressed file was only 870B. Thats a whole lot smaller.\nWe can compare the object size to the file size for another look. The below is in bytes.\n\ncat(\" cars: \", object.size(cars), \"\\n\",\n    \"compressed cars: \", file.size(tmp))\n#>  cars:  1648 \n#>  compressed cars:  210"
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#benchmarking",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#benchmarking",
    "title": "Medium Data and Production API Pipeline",
    "section": "Benchmarking",
    "text": "Benchmarking\nLet’s extend this example to some larger datasets as well as benchmark the results. We’ll use data from nycflights13. In this example we’ll send two dataset to an API as the parameters metadata and data. Generally metadata is smaller than the data. So for this example we’ll send 1,000 rows as the metadata and 10,000 rows as the data. We’ll call on the weather and flights datasets from nycflights13.\n\nsmall_weather <- dplyr::sample_n(weather, 1000)\nsmall_flights <- dplyr::sample_n(flights, 10000)\n\n\nMaking it functional\nAs always, I recommend making your repetitive tasks into functions. Here we will create two functions. One for posting the data as gzip files and the second as pure json. These will be labeled post_gz() and post_json() respectively.\nThese functions will take two parameters: metadata and data.\nDefine post_gz()\n\npost_gz <- function(metadata, data) {\n  \n  # write metadata to temp file\n  tmp_meta <- tempfile(\"metadata\")\n  gz_temp_meta <- gzfile(tmp_meta)\n  write_json(metadata, gz_temp_meta)\n  close(gz_temp_meta)\n  \n  # write data to temp file\n  tmp_data <- tempfile(\"data\")\n  gz_temp_data <- gzfile(tmp_data)\n  write_json(data, gz_temp_data)\n  close(gz_temp_data)\n  \n  # post \n  q <- POST(b_url, \n       body = list(\n         metadata = upload_file(tmp_meta),\n         data = upload_file(tmp_data)\n       ))\n  \n  # remove temp files\n  unlink(tmp_meta)\n  unlink(tmp_data)\n  \n  # return a character for purposes of bench marking\n  \"Posted...\"\n}\n\nDefine post_json().\n\npost_json <- function(metadata, data) {\n  q <- POST(b_url, \n       body = list(\n         metadata = metadata,\n         data = data\n       ),\n       encode = \"json\") \n  \n  \"Posted...\"\n}\n\nNow that these functions have been defined, let’s compare their performance using the package bench. We’ll run each function 50 times to get a good understanding of their respective performance.\n\nbm <- bench::mark(\n  post_gz(small_weather, small_flights),\n  post_json(small_weather, small_flights),\n  iterations = 5\n  )\n\nbm\n\n\nggplot2::autoplot(bm)"
  },
  {
    "objectID": "posts/2020-09-07-demographic-change-white-fear-and-social-construction-of-race.html",
    "href": "posts/2020-09-07-demographic-change-white-fear-and-social-construction-of-race.html",
    "title": "Demographic Change, White Fear, and Social Construction of Race",
    "section": "",
    "text": "Two or three weeks ago, somewhere between Carter Dome and Mount Hight in the White Mountains of New Hampshire my friend posed a thought experiment to me. It’s one that I have heard dozens of times whether at a bar top, a fire pit, or an inflatable tube on the Pemigewasset River. It goes something like this.\n\nNote that this is rather extreme example and may not be comfortable for some readers. But thought experiments are supposed to be uncomfortable.\n\n“Take the country Iceland, it has a small population of about 350,000. Say, 100,000 Chinese immigrants move to the country within the period of a year. Is it still Iceland?”\n“Yes, of course.”\n“Okay, say this new population brings a massive baby boom. We know the fertility rate in China is much greater than that of Iceland. This new population has parity with the original 350,000 Icelanders. Making 700,000 total. A massive election is held and there is complete overturn of elected officials and each new official is either from the massive Chinese influx or immediate descendants of the Chinese immigrants. This new government enacts laws that greatly resemble China. Is this country no longer Iceland? What about the Icelandic culture? How can it be preserved? Are you okay with the destruction of a culture?”\nAt this point, for some reason, I’ve always found it tough to provide an argument that can persuade him. Upon reflection, it’s likely because the conversation shifts abruptly from one of pure demographic consideration to one of cultural preservation. The thought experiment feels challenging mainly because the idea of an ethnic and cultural Iceland is portrayed as some static, unshifting, unyielding, monolith. And that is what is at the crux of this.\nThere is an extant fear of racial elimination as a product of demographic growth. Research shows that when white individuals learn about a projected demographic shift from being a majority to minority of the population they show racial preferences for their identified race (source). This has consequences for political party preference as well. White Americans who express concern become more “conservative”—a term I increasingly struggle to use or condone the use of—political views and lead to a great partisan divide (source). Rather prescient, right?\nIceland, while they do not maintain official statistics on race, we do know that approximately 94% of the population are ethnically Icelandic. If we take the complement as entirely people of color (POC) that makes Iceland at most 6% POC. It is likely much less. But what does it mean to be ethnically Icelandic?\nIceland is a discovered land. At the time of its settlement by Norwegians in the 9th century, the land was uninhabited. Icelandic settlers, confirmed by genomic study, are largely from the Scandinavian countries, Ireland, and Scotland. Thus, in the one thousand and change years since its inception, ethnic Icelanders were derived from a melange of Northern Europeans. It would be unreasonable to think that sex would only occur between people of the same homeland indefinitely—that small genetic pool would lead to things like the Hapsburg Jaw. This is illustrative of two points pertinent to the thought experiment.\n\nEthnic Icelanders are descendants of other ethnic groups. Or, put another way, ethnicity is a social construction.\nThe movement of people is a constant in human history.\n\nSay, for the sake of the mental experiment, we give way to the idea that there is an Icelandic culture which can be nailed down and is not in flux. When was it in its purest state? Surely, if we take a snapshot of Icelandic culture of today, it would be unrecognizable to people a century ago, or maybe considerably different than even a few decades ago.\nIf, however, we define culture as an artifact of the history of Iceland—as we rightly should—where the past is important in informing the present, then we must be willing to concede that was is happening presently will become context for understanding Icelandic culture in the future. And that what will happen is soon to be the present and, following, the past (time is a construct I still don’t fully grasp). This is all to say is that culture is a constantly changing (in a state of flux) and that the concept of indefinite cultural preservation is unattainable—and, I’d argue, undesirable. We must accept that populations grow and change; that movement of peoples is a constant in human history; that culture is not a monolith and is constantly shifting; and that ethnicity and race is are myths.\nAt the end of the day, his thought experiment isn’t so much a thought experiment but rather an argument against in-migration. People will move across borders—another social construction, but perhaps with more contemporary utility than that of ethnicity—and the directionality is not only in, but it is also out. For every immigration problem there is an emigration problem. Call me a globalist, but I believe international borders should be more open.\nThere is a scene from Parks and Recreation where Leslie Knope refers to Ann Perkins as racially ambiguous. Today, this might be a problematic statement, but it is a reality of the future.\n\nAll of our descendants within a few generation will be ethnically ambiguous and new ethnic and racial identities will emerge. Fearing change solely based on the color of others’ skin and your preconceptions of them is not a good reason for fearing change. In this thought experiment, the concern ought not be about culture and race. But rather the focus of my argument should have been that of infrastructure. How can we ensure that there is enough housing? Nourishment? Education? Opportunity? You know, the things that truly matter to humanity."
  },
  {
    "objectID": "posts/2022-02-12-the-heck-is-a-statistical-moment.html",
    "href": "posts/2022-02-12-the-heck-is-a-statistical-moment.html",
    "title": "The heck is a statistical moment??",
    "section": "",
    "text": "I wrote myself a short story to help me remember what the moments are.\n\n“The first moment I looked at the distribution I thought only of the average. The second, I thought of the variance. Soon after, I thought then of the skewness. Only then did I think about the kurtosis.”\n\nThis all started when reading Luc Anselin’s “Spatial Regression Analysis in R: A Workbook”, I encountered the following:\n\n“Under the normal assumption for the null, the theoretical moments of Moran’s I only depend on the characteristics of the weights matrix.”\n\nThe moments? The what? Under the normal assumption of my study habits I would skip over this word and continue to the next sentence. However, this was critical for understanding the formula for Moran’s I: \\(E[I] = \\frac{-1}{n - 1}\\).\nWikipedia was likely written by the same gatekeepers. I turned to the article on “Method of moments (statistics)” which writes\n\n“Those expressions are then set equal to the sample moments. The number of such equations is the same as the number of parameters to be estimated. Those equations are then solved for the parameters of interest. The solutions are estimates of those parameters.”\n\nNaturally, I turned to twitter to vent.\n\n\nThe use of the word \"moment\" in statistics is cruel.\n\n— jos (@JosiahParry) October 23, 2021\n\n\nThanks to Nicole Radziwill for a very helpful link from the US Naval Academy.\nThe method of moments is no more than simple summary statistics from a distribution. There are four “moments”.\n\nMean\nVariance\nSkew\nKurtosis\n\nWhy would one use these words? To gatekeep, of course. Academia uses needlessly complex language quite often.\nRemember, friends. Use clear and concise language. Let’s remove “moments” from our statistical lexicon."
  },
  {
    "objectID": "posts/2022-04-06-my-new-ide-theme-xxemocandylandxx.html",
    "href": "posts/2022-04-06-my-new-ide-theme-xxemocandylandxx.html",
    "title": "My new IDE theme: xxEmoCandyLandxx",
    "section": "",
    "text": "In general, I’m not a fan of “Dark Themes.” I find that they have too much contrast and hurt my eyes a bit. However, at night time when the lights are dim, my good ole trusty XCode theme becomes too much and a dark-er theme would be helpful.\nI’ve found that “Material” is nice, but again a bit too dark. I previously had a theme “Keeping Warm” (inspired by the song of the same name) that was a dim maroon based theme. When I returned my machine to RStudio I forgot to save the theme for myself! So for the past four months I’ve been needing something to suit my needs.\nOver the past two weeks or so I’ve worked on developing my own theme. I asked my partner what to call it and she quickly came up with “Emo Candy Land”—genius. Naturally, I renamed it xxEmoCandyLandxx. It was made with a fork of {rsthemes}. I felt very strongly that I’d like my functions to be both italicized and bolded which are not supported today. Though it would be a rather straightforward change I’d think.\n\nYou can find the theme in a Github Gist here.\nFollow these instructions to install the new theme."
  },
  {
    "objectID": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html",
    "href": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html",
    "title": "Exploratory Spatial Data Analysis in R",
    "section": "",
    "text": "Geospatial data is becoming increasingly common across domains and industries. Spatial data is no longer only in the hands of soil scientists, meteorologists, and criminologists, but in marketing, retail, finance, etc. It is common for spatial data to be treated as any other tabular data set. However, there is information to be drawn from our data’s relation to space. The standard exploratory data analysis toolkit will not always suffice. In this talk I introduce the basics of exploratory spatial data analysis (ESDA) and the {sfdep} package. {sfdep} builds on the shoulders of {spdep} for spatial dependence, emphasizes the use of simple features and the {sf} package, and integrates within your tidyverse-centric workflow. By the end of this talk users will understand the basics of ESDA and know how to start incorporating these skills in their own work."
  },
  {
    "objectID": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#recording",
    "href": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#recording",
    "title": "Exploratory Spatial Data Analysis in R",
    "section": "Recording",
    "text": "Recording"
  },
  {
    "objectID": "posts/2018-04-14-the-easyway-first.html",
    "href": "posts/2018-04-14-the-easyway-first.html",
    "title": "Coursera R-Programming: Week 2 Problems",
    "section": "",
    "text": "Over the past several weeks I have been helping students, career professionals, and people of other backgrounds learn R. During this time one this has become apparent, people are teaching the old paradigm of R and avoiding the tidyverse all together.\nI recently had a student reach out to me in need of help with the first programming assignment from the Coursera R-Programming course (part of the Johns Hopkins Data Science Specialization). This particular student was struggling with combining the her new knowledge of R data types, conditional statements, looping, control statements, scoping, and functions to solve the assignment problem set. I provided her with a walk through of each question in base R, the style of the course. I couldn’t help but empathize with her as I too learned the long way first. However I thought that she shouldn’t be learning the hard way first (see David Robinson’s blog post, “Don’t teach students the hard way first”), she should be learning the effective way.\nIn my written response to her, I gave her solutions to her problems in base R and using the tidyverse. Here, I will go over the problems and adress them from a tidy perspective. This will not serve as a full introduction to the tidyverse. For an introduction and a reason why the tidyverse is superior to base R, I leave you with Stat 545: Introduction to dplyr\nThe assignment utilizes a directory of data called specdata which can be downloaded here, and describes it:\n\nThe zip file contains 332 comma-separated-value (CSV) files containing pollution monitoring data for fine particulate matter (PM) air pollution at 332 locations in the United States. Each file contains data from a single monitor and the ID number for each monitor is contained in the file name. For example, data for monitor 200 is contained in the file “200.csv”. Each file contains three variables:\n\n\n\nDate: the date of the observation in YYYY-MM-DD format (year-month-day)\nsulfate: the level of sulfate PM in the air on that date (measured in micrograms per cubic meter)\nnitrate: the level of nitrate PM in the air on that date (measured in micrograms per cubic meter)\n\n\n\nFor this programming assignment you will need to unzip this file and create the directory ‘specdata’. Once you have unzipped the zip file, do not make any modifications to the files in the ‘specdata’ directory. In each file you’ll notice that there are many days where either sulfate or nitrate (or both) are missing (coded as NA). This is common with air pollution monitoring data in the United States.\n\n\n\nPart I\nProblem 1:\n\nWrite a function named ‘pollutantmean’ that calculates the mean of a pollutant (sulfate or nitrate) across a specified list of monitors. The function ‘pollutantmean’ takes three arguments: ‘directory’, ‘pollutant’, and ‘id’. Given a vector monitor ID numbers, ‘pollutantmean’ reads that monitors’ particulate matter data from the directory specified in the ‘directory’ argument and returns the mean of the pollutant across all of the monitors, ignoring any missing values coded as NA. A prototype of the function is as follows\n\n pollutantmean <- function(directory, pollutant, id = 1:332){\n    ## 'directory' is a character vector of length 1 indicating\n    ## the location of the CSV files\n    \n    ## 'pollutant' is a character vector of length 1 indicating\n    ## the name of the pollutant for which we will calculate the \n    ## mean; either \"sulfate\" or \"nitrate\"\n\n    ## 'id' is an integer vector indicating the monitor ID numbers\n    ## to be used\n\n    ## Return the mean of the pollutant across all monitors list\n    ## in the 'id' vector (ignoring NA values)\n    ## NOTE: Do not round the result!\n}\nBefore we tackle the function, I believe the best approach is to first solve the problem in a regular script. This problem has four clear steps:\n\nIdentify files in the directory\nSubset files based on provided ID\nRead the files\nCalculate and return the mean on the desired column\n\nThis problem gives us a directory of files from which we need to read in the data based on the provided IDs. For the sake of this walk through we will randomly sample 10 values within the range designated in the problem statement (332).\nWe will first generate random IDs, then identify all of the files within the specified directory and obtain their file paths using the list.files() function. After this we will subset our file list based on the IDs, then iterate over our file list and read in each file as a csv using purrr:map_df() combined with readr::read_csv(). Fortunately map_df() returns a nice and pretty data frame which lets us avoid having to explicitly bind each unique data frame.\n\nIdentify Files\nHere we create 10 random IDs and store them in the ids variable. Next we use list.files() to look within the specdata folder that we downloaded above. Everyone’s path will most likely be different. Be sure to obtain the correct file path—help for Mac.\nNext we identify the files we need based on the sampled ids and store the subset in the files_filtered variable. We use the values of the ids to locate the file paths positionally. For example, ID number 1 is the first file, number 10 is the tenth, etc.\n\n# Load our handy dandy functions\nlibrary(tidyverse)\n\n# 10 random IDs in ID range\nids <- sample(1:332, 10)\n\n# Identify all files within the directory\nfiles <- list.files(\"../../data/specdata\", full.names = TRUE)\n\n# Subset the data\nfiles_filtered <- files[ids]\n\n# View the files to verify\npaste(ids, files_filtered)\n\n\n\nReading the Files\nNow that we have identified the files that we are going to read in, we can use purrr:map_df() to apply the readr::read_csv() function to each value of files_filtered and return a data frame (hence the _df() suffix). We supply additional arguments to read_csv() to ensure that every column is read in properly.\n\n# Read in the subset of the data. Notice the brackets after files[]\nspecdata <- map_df(files_filtered, read_csv, \n                   col_types = list(\n                     col_date(),\n                     col_double(),\n                     col_double(),\n                     col_integer()\n                   ))\n\nglimpse(specdata)\n\nNext, we get to utilize some dplyr magic. Here we take the specdata object we created from reading in our files, deselct the Date column, then utilize summarise_if() to apply the mean() function to our data. summarise_if() requires that we provide a logical statement as the first argument. If (hence the _if() suffix) the logical statement evaluates to TRUE on a column then it will apply a list of functions to those columns where the statement evaluated to TRUE. We can also specify additional arguments to the functions. Here we specify na.rm = TRUE for handling missing values.\nIn this case, we are checking to see if our columns are of the data type double using the is.double() function. If you’re wondering why we didn’t use is.numeric(), it’s because the ID column is an integer which is considered numeric.\nIf we wanted to take the underlying vector of one of the columns, we can also, use dplyr::pull(col_name). This will be helpful later when we want to obtain the mean of just one column.\n\n# Obtain mean nitrate\nspecdata %>% \n  select(-Date) %>% \n  summarise_if(is.double, mean, na.rm = TRUE) %>% \n  # Pull just the sulfate column\n  pull(sulfate)\n\nspecdata %>% \n  select(-Date) %>% \n  summarise_if(is.double, mean, na.rm = TRUE) %>% \n  # Pull just the nitrate column\n  pull(nitrate)\n\n\nNow that we have all of the tools, we can put this together into a single function, which I will call pollutant_mean() to somewhat adhere—functions should take the name of verbs—to the tidyverse style guide.\nHere we have three arguments:\n\ndirectory: Where to look for the files\npollutant: Which pollutant (nitrate or sulfate) to evaluate\n\nThis needs to be a character value unless we want to get into tidyeval, which frankly I will leave to the professionals. But I will provide an alternative solution at the end that doesn’t require quoted pollutant names.\n\nid: Which monitoring stations we should look at\n\nWithin the function we take everything we did in the above steps but generalize it to a function. We identify the files in the directory provided (specdata), subset the files positionally based on the provided id vector, and then iterate over the file names and read them in with map_df() and read_csv().\nNext we take our data and calculate the mean on both sulfate and nitrate columns. We then pull() the specified column from the pollutant argument and then return() that value.\n\n\npollutant_mean <- function(directory, pollutant, id = 1:332) {\n  files <- list.files(directory, full.names = TRUE)\n  files_filtered <- files[id]\n  specdata <- map_df(files_filtered, read_csv, \n                     col_types = list(\n                       col_date(),\n                       col_double(),\n                       col_double(),\n                       col_integer()\n                     ))\n  \n  specdata %>% \n    select(-Date) %>% \n    summarise_if(is.double, mean, na.rm = TRUE) %>% \n    pull(pollutant) %>% \n    return()\n\n}\n\nHere we can test out the function with both types of pollutants and different id values.\n\npollutant_mean(directory = \"../../data/specdata\", pollutant = \"sulfate\", id = sample(1:332, 20))\n#pollutant_mean(\"../../data/specdata\", \"nitrate\", 2)\n\n\n\n\n\nPart II:\nLet us continue to the second problem in the problem set.\nProblem 2:\n\nWrite a function that reads a directory full of files and reports the number of completely observed cases in each data file. The function should return a data frame where the first column is the name of the file and the second column is the number of complete cases.\n\nThe assignment provides an example function format, but I think it to be a bit misleading. So I will go about this in the way I think is best. We will work on creating a function called complete_spec_cases() which will take only two arguments, directory, and id. directory and id will be used in the the same way as the previous problem.\nFor this problem our goal is to identify how many complete cases there are by provided ID. This should be exceptionally simple. We will have to identify our files, subset them, and read them in the same way as before. Next we can identify complete cases by piping our specdata object to na.omit() which will remove any row with a missing value. Next, we have to group by the ID column and pipe our grouped data frame to count() which will count how many observations there are by group. We will then return this data frame to the user.\n\ncomplete_spec_cases <- function(directory, id = 1:332) {\n\n  files <- list.files(directory, full.names = TRUE)\n  \n  specdata <- map_df(files[id], read_csv,\n                     col_types = list(\n                       col_date(),\n                       col_double(),\n                       col_double(),\n                       col_integer()\n                     ))\n  \n  complete_specdata <- specdata %>% \n    na.omit() %>% \n    group_by(ID) %>% \n    summarise(nobs = n())\n  \n  return(complete_specdata)\n}\n\n\ncomplete_spec_cases(\"../../data/specdata\", id = sample(1:332, 20))\n\n\n\n\nPart III:\nThis final problem is probably the most complicated, but with the method we just used above and with a bit more help from the purrr and dplyr packages, we can do this no problem.\nProblem 3:\n\nWrite a function that takes a directory of data files and a threshold for complete cases and calculates the correlation between sulfate and nitrate for monitor locations where the number of completely observed cases (on all variables) is greater than the threshold. The function should return a vector of correlations for the monitors that meet the threshold requirement. If no monitors meet the threshold requirement, then the function should return a numeric vector of length 0. A prototype of this function follows:\n\nCorrect <- function(directory, threshold = 0){\n    ## 'directory' is a character vector of length 1 indicating\n    ## the location of the CSV files\n\n    ## 'threshold' is a numeric vector of length 1 indicating the\n    ## number of completely observed observations (on all\n    ## variables) required to compute the correlation between\n    ## nitrate and sulfate; the default is 0\n\n    ## Return a numeric vector of correlations\n    ## NOTE: Do not round the result!\n}\nLet keep this simple. The above statement essentially is asking that we find the correlation between nitrate and sulfate for each monitoring station (ID). But there is a catch! Each ID must meet a specified threshold of complete cases, and if none of the monitors meet the requirement the function must return a numeric(0).\nThe way we will structure this function will be to first read in the data—as we have done twice now, except this time there will be no subsetting of IDs. Then we need to identify the number of complete cases by ID—as we did in problem 2—and identify the stations that meet the threshold requirement. At this point we will use an if statement to check if we have at least 1 monitoring station that meets our threshold, if we do not, we return the numeric(0)—there is most likely a more tidy way to do this, but I am not aware. If we have at least 1 monitoring station that meets the specified threshold we will use an inner_join() to make sure that specdata contains only those IDs that meet the requirement.\nFor the sake of this example, we will continue to use the specdata object we created in previous examples, and we will set our threshold to 100. Once we identify the stations with the proper number of counts (> 100), we will store that data frame in an object called id_counts\n\n\nid_counts <- specdata %>% \n    na.omit() %>% \n    group_by(ID) %>% \n    count() %>% \n    filter(n > 100) \n  \n  if (nrow(id_counts) < 1) {\n    return(numeric(0))\n  } else {\n    print(\"All is well.\")\n  }\n\n  specdata <- id_counts %>% \n    inner_join(specdata, by = \"ID\") %>%\n    na.omit() \n  \n  specdata\n\nThis is where it gets kind of funky. Once we have filtered down our data set, we need to calculate the correlations for each ID. The way that we do this is by nesting our data frame on the ID column. Calling nest(-ID) allows us to, for each value of ID, create a data frame for just those rows where the ID is the same. We will then have a new list type column where each value is actually a data frame. Let’s check out what this looks like before we hop into the function.\n\nspecdata %>% \n  nest(-ID)\n\nNow that we know how to nest our data, we need to calculate the correlations for each row (ID value). We will do this by combining mutate() and map(). Here .x references the data that is within each nested tibble. To learn more about purrr I recommend the chapter on iteration from R For Data Science.\nAfter we have done our calculations we undo our nesting using unnest() on the new column we created, and deselect the data column.\n\nspecdata %>% \n  na.omit() %>% \n    nest(-ID) %>% \n    mutate(correlation = map(data, ~cor(.x$sulfate, .x$nitrate))) %>% \n  unnest(correlation) %>% \n  select(-data)\n\nWe can now place these above examples within a new function called pollutant_cor().\n\npollutant_cor <- function(directory, threshold = 0) {\n  files <- list.files(directory, full.names = TRUE)\n  specdata <- map_df(files, read_csv, \n                     col_types = list(\n                       col_date(),\n                       col_double(),\n                       col_double(),\n                       col_integer()\n                     )) %>% na.omit()\n  \n  id_counts <- specdata %>% \n    group_by(ID) %>% \n    count() %>% \n    filter(n > threshold) \n  \n  if (nrow(id_counts) < 1) {\n    return(numeric(0))\n  }\n  \n  correlations <- id_counts %>% \n    inner_join(specdata, by = \"ID\") %>% \n    nest(-ID) %>% \n    mutate(correlation = map(data, ~cor(.x$sulfate, .x$nitrate))) %>% \n    unnest(correlation)\n  \n  return(correlations)\n\n  \n}\n\nWe can now test our function against two different thresholds to see how it reacts.\n\npollutant_cor(\"../../data/specdata\", 100)\n\nIf we set the threshold to 100,000, we should expect a numeric(0).\n\npollutant_cor(\"../../data/specdata\", 100000)\n\nIt all works!"
  },
  {
    "objectID": "posts/2020-02-4-cran-download.html",
    "href": "posts/2020-02-4-cran-download.html",
    "title": "R Security Concerns and 2019 CRAN downloads",
    "section": "",
    "text": "The number of downloads that a package has can tell us a number of things. For example, how much other packages rely on it. A great case of this is rlang. Not many of use rlang, know what it does, or even bother to dive into it. But in reality it is the backbone of so many packages that we know and love and use every day.\nThe other thing that we can infer from the download numbers is how trusted a package may be. Packages that have been downloaded time and time and again have likely been scoped out thoroughly for any bugs. This can be rather comforting for security conscious groups.\nWith a little rvest-web-scraping-magic we can obtain the name of every package published to CRAN.\nRStudio keeps daily logs of packages downloaded from their CRAN mirror. The package cranlogs makes the data available via and API and an R package. Below is a lightweight function which sums the total number of downloads for the entire year. furrr was used to speed up the computation a bit. This takes ~40 minutes to run, as such use the exported data in data/cran-2019-total-downloads.csv.\nWhile this is helpful, these packges also have dependencies, and those dependencies have dependencies. The R core team have built out the tools package which contains, yes, wonderful tools. The function tools::package_dependencies() provides us with the dependencies.\nThe below code identifies the top 500 downloaded packages and their dependencies."
  },
  {
    "objectID": "posts/2020-02-4-cran-download.html#package-checks",
    "href": "posts/2020-02-4-cran-download.html#package-checks",
    "title": "R Security Concerns and 2019 CRAN downloads",
    "section": "Package Checks",
    "text": "Package Checks\nEach package on CRAN goes through a rigorous checking process. The r cmd check is ran on each package for twelve different flavors from a combination of Linux and Windows. If you trust the checks that the R Core team do, I wouldn’t reinvent the wheel.\nThe data are not provided directly from CRAN though an individual has provided these data via an API. I recommend using the API as a check against packages on ingest. I’d also do this process for every time you’re syncing. Again, this doesn’t mean that there are no vulnerabilities. But if there are functions that will literally break the machine, then the checks in general shouldn’t work.\nThe biggest risk is really in the development and publication of applications. The greatest risk you are likely to face are going to be internal or accidental. For example using base R and Shiny, a developer can make an app unintentionally malicious—i.e. permitting system calls or creating a SQL injection. Though this would be rather difficult to build into an app, it is possible. The process here would be to institute a peer review process for the apps developed. Also, you’re going to want to sandbox the applications—which Connect does and will improve with launcher in the future."
  },
  {
    "objectID": "posts/2020-02-4-cran-download.html#instituting-checks",
    "href": "posts/2020-02-4-cran-download.html#instituting-checks",
    "title": "R Security Concerns and 2019 CRAN downloads",
    "section": "Instituting Checks",
    "text": "Instituting Checks\nWe can use the cchecks packge to interact with R-Hub’s CRAN check API. They have done a wonderful job aggregating package check data. The data it returns, however, is in a rather deeply nested list. Below is a function defintion which can tidy up some of the important information produced from the API query.\n\n#devtools::install_github(\"ropenscilabs/cchecks\")\nlibrary(cchecks)\nlibrary(tidyverse)\ntidy_checks <- function(checks) {\n  \n  check_res <- map(checks, pluck, \"data\", \"checks\")\n  check_pkg <- map(checks, pluck, \"data\", \"package\")\n  check_deets <- map(checks, pluck, \"data\", \"check_details\")\n  check_summ <- map(checks, pluck, \"data\", \"summary\")\n  \n  tibble(pkg = unlist(check_pkg),\n         check_results = check_res,\n         check_details = check_deets,\n         check_summary = check_summ) %>% \n    unnest_wider(check_summary)\n  \n}\n\n\n# query the API for packages\nchecks  <- cch_pkgs(c(\"spotifyr\", \"genius\"))\n# tidy up the checks\nclean_checks <- tidy_checks(checks)\n# get check results\nclean_checks %>% \n  unnest(check_results)\n# get check details\nclean_checks %>% \n  unnest_wider(check_details)\n\nYou can pull these checks for the top 500 packages and their dependencies in a rather straightforward manner now. You can iterate through these all. Note that this is an API and you may see some lag time. So go make some tea.\n\ntop_checks <- cch_pkgs(head(top_500_cran$package))\ntidy_checks(top_checks)\n\n\nResources\n\nhttps://environments.rstudio.com/validation.html\nhttps://www.r-bloggers.com/overview-of-the-cran-checks-api/amp\nhttps://blog.r-hub.io/2019/04/25/r-devel-linux-x86-64-debian-clang/"
  },
  {
    "objectID": "posts/2020-06-12-red-queen.html",
    "href": "posts/2020-06-12-red-queen.html",
    "title": "The Red Queen Effect",
    "section": "",
    "text": "The Red Queen and maintenance of state and society\nIt’s Monday morning. You’re back at work after a few days off. Your inbox is a lot more full than you hoped with 70 emails. Time to get reading and sending. It’s been an hour and you’ve read and sent at least 20 emails but your inbox is still at 70. You’ve been working hard and yet it feels like you’ve gone nowhere. This idea of working really hard but feeling like you’ve gone nowhere is at the center of the Red Queen Effect.\nThere are a number of “Red Queen Effect”s in the scientific literature all of which are inspired from the Red Queen’s race from Lewis Carrols’s Through the Looking Glass.\n\n“Well, in our country,” said Alice, still panting a little, “you’d generally get to somewhere else—if you run very fast for a long time, as we’ve been doing.”\n“A slow sort of country!” said the Queen. “Now, here, you see, it takes all the running you can do, to keep in the same place. If you want to get somewhere else, you must run at least twice as fast as that!” \n\nAlice is running and running and getting nowhere. Much like how as you read emails you get new ones. Daron Acemoglu and James A. Robinson adapt this concept to the evolution of states and connect it to their idea of the Narrow Corridor. The path to a “successful” society is a race between the power of the people and the power of the state. States in which the pace of growth in both the peoples’ power and the power / ability of the state are similar often produce more liberal (in the sense of liberty) nations.\n\nThis graphic is meant to illustrate this race. Getting into the corridor is a game of chase between the power of the state and society. Keeping the balance is delicate act—one that no nation has perfected—which requires society to check the power of the state and the state to provide checks to society. They define the Red Queen as\n\n“The process of competition, struggle and cooperation between state and society”"
  },
  {
    "objectID": "projects/sfdep.html",
    "href": "projects/sfdep.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "sfdep is an R package that acts as a tidy interface to the spdep package. In addition it provides some statistics that are not available elsewhere in the R ecosystem such as the neighbor match test.\n\n\n\nEmerging Hot Spot Analysis\nColocation Quotients\nsfnetworks integrations\nLocal Neighbor Match Test\n\n\n\n\n\n GitHub\nRStudio Conf 2022L\nNew York Open Statistical Programming Meetup"
  },
  {
    "objectID": "projects/spdep.html",
    "href": "projects/spdep.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "spdep\nspdep is an absolute powerhouse of an R package. spdep was first released in 2002 and is one of, if not the, first package to implement many of the most important spatial statistics for aerial data.\nMy contributions include:\n\nLocal Geary C (univariate and multivariate)\nLocal bivariate Moran’s I\nGlobal bivariate Moran’s I\nLocal univariate join count\nLocal bivariate join count"
  },
  {
    "objectID": "projects/genius.html",
    "href": "projects/genius.html",
    "title": "genius (retired)",
    "section": "",
    "text": "genius was my first R package and my first digital child. genius provided a way to programatically access song lyrics from genius.com. This included ways to fetch single songs, albums, and track lists. It was, at one point, integrated with the spotifyr package.\nThis R package was the basis of much of my learning of the data science ecosystem. Including APIs and Docker. Creating stacked ensemble machine learning models using LDA outputs as model inputs (see online guide)\n\n GitHub"
  },
  {
    "objectID": "projects/writing/uitk.html",
    "href": "projects/writing/uitk.html",
    "title": "Urban Informatics Toolkit",
    "section": "",
    "text": "The Urban Informatics Toolkit (uitk) is an open text book I wrote with the intention of teaching first year graduate students in Urban Informatics the R programming language.\nWithin it are two years of study in the Urban Informatics Program at Northeastern University, five years of self-directed education in R, two years of teaching R, and innumerable hours learning R."
  },
  {
    "objectID": "projects/writing/mirr.html",
    "href": "projects/writing/mirr.html",
    "title": "mirrr - song genre classification",
    "section": "",
    "text": "Tidy Music Information Retrieval was a bookdown project I wrote back in 2019 that created a stacked ensemble model that predicted musical genre from song audio features and song lyrics.\nI created 3 models. The first utilized LDA text classification outputs from song lyrics as inputs into a classification model. The second used song audio features from spotify. The third model used the outputs of both to create a stacked ensemble model."
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html",
    "href": "posts/2019-03-30-plumber-genius-api.html",
    "title": "genius Plumber API",
    "section": "",
    "text": "get started here\nSince I created genius, I’ve wanted to make a version for python. But frankly, that’s a daunting task for me seeing as my python skills are intermediate at best. But recently I’ve been made aware of the package plumber. To put it plainly, plumber takes your R code and makes it accessible via an API.\nI thought this would be difficult. I was so wrong."
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#using-plumber",
    "href": "posts/2019-03-30-plumber-genius-api.html#using-plumber",
    "title": "genius Plumber API",
    "section": "Using plumber",
    "text": "Using plumber\nPlumber works by using roxygen like comments (#*). Using a single comment, you can define the request type and the end point. Following that you define a function. The arguments to the funciton become the query parameters.\nThe main genius functions only require two main arguments artist and album or song. Making these accessible by API is as simple as:\n#* @get /track\nfunction(artist, song) {\n  genius::genius_lyrics(artist, song)\n}\nWith this line of code I created an endpoint called track to retrieve song lyrics. The two parameters as defined by the anonymous function are artist and song. This means that song lyrics are accessible with a query looking like http://hostname/track?artist=artist_name&song=song_name.\nBut as it stands, this isn’t enough to host the API locally. Save your functions with plumber documentation into a file (I named mine plumber.R).\n\nCreating the API\nCreating the API is probably the easiest part. It takes quite literally, two lines of code. The function plumb() takes two arguments, the file which contains your plumber commented code, and the directory that houses it.\nplumb() creates a router which is “responsible for taking an incoming request, submitting it through the appropriate filters.”\nI created a plumber router which would be used to route income queries.\npr <- plumb(\"plumber.R\")\nThe next step is to actually run the router. Again, this is quite simple by calling the run() method of the pr object. All I need to do is specify the port that the API will listen on, and optionally the host address.\npr$run(port = 80, host = \"0.0.0.0\")\nNow I can construct queries in my browser. An example query is http://localhost/track?artist=andrew%20bird&song=proxy%20war. Sending this request produces a very friendly json output.\n[\n{\"track_title\":\"Proxy War\",\"line\":1,\"lyric\":\"He don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":2,\"lyric\":\"She don't to have get over him\"},\n{\"track_title\":\"Proxy War\",\"line\":3,\"lyric\":\"With all their words preserved forevermore\"},\n{\"track_title\":\"Proxy War\",\"line\":4,\"lyric\":\"You don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":5,\"lyric\":\"She don't have to get over you\"},\n{\"track_title\":\"Proxy War\",\"line\":6,\"lyric\":\"It's true these two have never met before\"},\n{\"track_title\":\"Proxy War\",\"line\":7,\"lyric\":\"At least not in real life\"},\n{\"track_title\":\"Proxy War\",\"line\":8,\"lyric\":\"Where your words cut like a knife\"},\n{\"track_title\":\"Proxy War\",\"line\":9,\"lyric\":\"Conjuring blood, biblical floods\"},\n{\"track_title\":\"Proxy War\",\"line\":10,\"lyric\":\"Looks that stop time\"},\n{\"track_title\":\"Proxy War\",\"line\":11,\"lyric\":\"You don't have to remember\"},\n{\"track_title\":\"Proxy War\",\"line\":12,\"lyric\":\"We forget what memories are for\"},\n{\"track_title\":\"Proxy War\",\"line\":13,\"lyric\":\"Now we store them in the atmosphere\"},\n{\"track_title\":\"Proxy War\",\"line\":14,\"lyric\":\"If you don't want to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":15,\"lyric\":\"You don't have to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":16,\"lyric\":\"It's just what we're calling peer-to-peer\"},\n{\"track_title\":\"Proxy War\",\"line\":17,\"lyric\":\"You don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":18,\"lyric\":\"You don't have to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":19,\"lyric\":\"We store them in the atmosphere\"},\n{\"track_title\":\"Proxy War\",\"line\":20,\"lyric\":\"If you don't want to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":21,\"lyric\":\"She don't have to get over you\"},\n{\"track_title\":\"Proxy War\",\"line\":22,\"lyric\":\"It's true these two have never met before\"},\n{\"track_title\":\"Proxy War\",\"line\":23,\"lyric\":\"At least not in real life\"},\n{\"track_title\":\"Proxy War\",\"line\":24,\"lyric\":\"Where your words cut like a knife\"},\n{\"track_title\":\"Proxy War\",\"line\":25,\"lyric\":\"Conjuring blood, biblical floods\"},\n{\"track_title\":\"Proxy War\",\"line\":26,\"lyric\":\"Looks that stop time\"},\n{\"track_title\":\"Proxy War\",\"line\":27,\"lyric\":\"If you don't want to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":28,\"lyric\":\"You don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":29,\"lyric\":\"If you don't want to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":30,\"lyric\":\"You don't have to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":31,\"lyric\":\"If you want to remember\"},\n{\"track_title\":\"Proxy War\",\"line\":32,\"lyric\":\"If you don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":33,\"lyric\":\"If you don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":34,\"lyric\":\"If you don't want to get over\"}\n]"
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#writing-a-python-wrapper",
    "href": "posts/2019-03-30-plumber-genius-api.html#writing-a-python-wrapper",
    "title": "genius Plumber API",
    "section": "Writing a Python Wrapper",
    "text": "Writing a Python Wrapper\nOne of the appeals of writing an API is that it can be accessed from any language. This was the inspiriation of creating this API. I want to be able to call R using Python. Creating an API is a great intermediary as writing an API wrapper is much easier for me than recreating all of the code that I wrote in R.\nI want to be able to recreate the three main functions of genius. These are genius_lyrics(), genius_album(), and genius_tracklist(). In doing this there are two steps I have to consider. The first is creating query urls, and the second is parsing json.\nTo create the urls, the requests library is used. Next, I created a template for the urls.\nimport requests\nurl_template = \"http://localhost:80/track?artist={}&song={}\"\nThe idea here is that the {} characters will be filled with provided parameters by using the .format() method.\nFor example, if I wanted to get lyrics for Proxy War by Andrew Bird, I would supply \"Andrew Bird\" and \"Proxy War\" as the arguments to format(). It’s important to note that these arguments are taken positionally. The url is created using this method.\nurl = url_template.format(\"andrew bird\", \"proxy war\")\nNow I am at the point where I can ping the server to receive the json. This is accomplished by using the .get() method from requests.\nresponse = requests.get(url)\nThis returns an object that contains the json response. Next, in order to get this into a format that can be analysed, it needs to be parsed. I prefer a Pandas DataFrame, and fortunately Pandas has a lovely read_json function. I will call the .content attribute of the response objectm and feed that into the read_json() function.\nimport pandas as pd\n\nproxy_war = pd.read_json(response.content)\n\nproxy_war.head()\n\n    line    lyric                                   track_title\n0   1   He don't have to get over her               Proxy War\n1   2   She don't to have get over him              Proxy War\n2   3   With all their words preserved forevermore  Proxy War\n3   4   You don't have to get over her              Proxy War\n4   5   She don't have to get over you              Proxy War\nBeautiful. Song lyrics are available through this API and can easily be accessed via python. The next step is to generalize this and the other two functions. The below is the code to create the genius_lyrics() function in python. It works almost identically as in R. However, at this moment it does not have the ability to set the info argument. But this can be changed easily in the original plumber.R file.\n# Define genius_lyrics()\ndef genius_lyrics(artist, song):\n\n    url_template = \"http://localhost:80/track?artist={}&song={}\"\n    \n    url = url_template.format(artist, song)\n\n    response = requests.get(url)\n    \n    song = pd.read_json(response.content)\n    \n    return(song)\nAt this point I’m feeling extremely stoked on the fact that I can use genius with python. Who says R and python practitioners can’t work together?"
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#containerize-with-docker",
    "href": "posts/2019-03-30-plumber-genius-api.html#containerize-with-docker",
    "title": "genius Plumber API",
    "section": "Containerize with Docker",
    "text": "Containerize with Docker\n\nTo make the process of setting up this genius API up easier for those who don’t necessarily interact with R, I created a lightweight-ish Docker container. The idea for this was to be able to pull a Docker image, run a command, and then the API will be available on a local port without having to interact with R at all.\nI’m not the most experience person with creating Docker containers but I can borrow code quite well. Fortunately I came across some wonderful slides from rstudio::conf 2019. Heather Nollis and Jacqueline Nolis presented on “API development with R and TensorFlow at T-Mobile”.\nThis container needs two things: a linux environment and an installation of R with plumber, genius, and its dependencies. An organization called The Rocker Project has created a number of Docker images that are stable and easy to install.\nSince genius relies on many packages from the tidyverse, the rocker/tidyverse image was used. To use their wonderful image, only one line is needed in my Dockerfile.\n# Import existing Docker image\nFROM rocker/tidyverse:3.5.2\nNow, not knowing exactly what I was doing, I copied code from Jacqueline and Heather’s sample Dockerfile in their slides. Their comment says that this is necessary to have the “needed linux libraries for plumber”, I went with it.\n# install needed linux libraries for plumber\nRUN apt-get update -qq && apt-get install -y \\\n  libssl-dev \\\n  libcurl4-gnutls-dev\ngenius and plumber are not part of the tidyverse image and have to be installed manually. The following lines tell Docker to run the listed R commands. For some unknown reason there was an issue with installing genius from CRAN so the repos argument was stated explicitly.\n# Install R packages\nRUN R -e \"install.packages('genius', repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('plumber')\"\nIn addition to the Dockerfile there are two files in my directory which are used to launch the API. The plumber.R and launch_api.R files. These need to be copied into the container. The line COPY / / copies from the location / in my directory to the location / in the container.\nThe Docker image has the libraries and files needed, but it needs to be able to actually launch the API. Since the plumber.R file specifies that the API will be listening on port 80, I need to expose that port in my Docker image using EXPOSE 80.\nThe last part of this is to run the launch_api.R so the API is available. The ENTRYPOINT command tells Docker what to run when the container is launched. In this case ENTRYPOINT [\"Rscript\", \"launch_api.R\"] tells Docker to run the Rscript command with the argument launch_api.R. And with that, the Dockerfile is complete and read to run.\nThe image needs to be built and ran. The simplest way to do this for me was to work from Dockerhub. Thus to run this container only three lines of code are needed!\ndocker pull josiahparry/genius-api:working\n\ndocker build -t josiahparry/genius-api .\n\ndocker run --rm -p 80:80 josiahparry/genius-api\n\nBoom, now you have an API that will be able to use the functionality of genius. If you wish to use Python with the API, I wrote a simple script which creates a nice tidy wrapper around it.\n\nIf anyone is interested in writing a more stable Python library that can call the functionality described above I’d love your help to make genius more readily available to the python community."
  },
  {
    "objectID": "projects/pkgs/sysreqs.html",
    "href": "projects/pkgs/sysreqs.html",
    "title": "R package system requirements",
    "section": "",
    "text": "GitHub repo\nRelated blog post\n\nThe goal of sysreqs is to make it easy to identify R package system dependencies. There are two components to this package: an “API” and a wrapper package.\nThis API and package is based on rstudio/r-system-requirements and the API client for RStudio Package Manager. The functionality is inspired by pak::pkg_system_requirements()."
  },
  {
    "objectID": "projects/writing/r4campaigns.html",
    "href": "projects/writing/r4campaigns.html",
    "title": "R for Progressive Campaigns",
    "section": "",
    "text": "Read the book here.\nIn 2008, the Obama campaign revolutionized the use of data in political campaigns. Since then, data teams have expanded and grown in size, capacity, and complexity.\nThis short bookdown project is intended to illustrate how data can be used in campaigns through the statistical programming language R. This is a collection of small “recipes” that I have created that are intended to aid data teams in leveraging R."
  },
  {
    "objectID": "projects/pkgs/genius.html",
    "href": "projects/pkgs/genius.html",
    "title": "genius (retired)",
    "section": "",
    "text": "genius was my first R package and my first digital child. genius provided a way to programatically access song lyrics from genius.com. This included ways to fetch single songs, albums, and track lists. It was, at one point, integrated with the spotifyr package.\nThis R package was the basis of much of my learning of the data science ecosystem. Including APIs and Docker. Creating stacked ensemble machine learning models using LDA outputs as model inputs (see online guide)\n\n GitHub"
  },
  {
    "objectID": "projects/pkgs/spdep.html",
    "href": "projects/pkgs/spdep.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "spdep (contributor)\nspdep is an absolute powerhouse of an R package. spdep was first released in 2002 and is one of, if not the, first package to implement many of the most important spatial statistics for aerial data.\nMy contributions include:\n\nLocal Geary C (univariate and multivariate)\nLocal bivariate Moran’s I\nGlobal bivariate Moran’s I\nLocal univariate join count\nLocal bivariate join count"
  },
  {
    "objectID": "projects/pkgs/sfdep.html",
    "href": "projects/pkgs/sfdep.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "sfdep is an R package that acts as a tidy interface to the spdep package. In addition it provides some statistics that are not available elsewhere in the R ecosystem such as the neighbor match test.\n\n\n\nEmerging Hot Spot Analysis\nColocation Quotients\nsfnetworks integrations\nLocal Neighbor Match Test\n\n\n\n\n\n GitHub\nRStudio Conf 2022L\nNew York Open Statistical Programming Meetup"
  },
  {
    "objectID": "projects.html#things-ive-written",
    "href": "projects.html#things-ive-written",
    "title": "projects & contributions",
    "section": "Things I’ve written",
    "text": "Things I’ve written\n\n\n\n\n\n\n\nR for Progressive Campaigns\n\n\n\nwriting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban Informatics Toolkit\n\n\n\n\n\n\n\n\n\n\n\n\n\nmirrr - song genre classification\n\n\n\nwriting\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "about",
    "section": "Contact me",
    "text": "Contact me\nIf you want to get in contact with me please send me an email at josiah.parry at gmail dot com.\n\n\ntalks i’ve given\n\n\nExploratory Spatal Data Analysis in the tidyverse\n\nJuly 28th, 2022 rstudio::conf(2022L)\n\nExploratory Spatial Data Analysis in R\n\nRecording\nApril 28th, 2022\n\nAPIs: you’re probably not using them and why you probably should\n\nGovernment Advances in Statistical Programming\nNovember 6th, 2020\n\n“Old Town Road” Rap or Country?: Putting R in Production with Tidymodels, Plumber, and Shiny\n\nBoston useR group\nDecember 10th, 2019\n\nTidy Lyrical Analysis\n\nBoston useR group\nJuly 17th, 2018\n\nNewfound Lake Landscape Value Analysis: Exploring the efficacy of PPGIS, NESTVAL 2016\n\nNew England St. Lawrence River Valley regional American Associations of Geographers Conference\n2016"
  },
  {
    "objectID": "posts/2022-11-17-overlapping-polys/index.html",
    "href": "posts/2022-11-17-overlapping-polys/index.html",
    "title": "Fishnets and overlapping polygons",
    "section": "",
    "text": "Today a question was asked in the geocompr discord. I wanted to share part of the solution as I think it covers 2 helpful things:\n\nmaking a fishnet grid\ncalculating the area of overlap between two polygons\n\nFor this example I’m using data from the Atlanta GIS Open Data Portal. Specifically using the future land use polygons.\nI’ve downloaded a local copy of the data as a geojson. But you can read it using the ArcGIS Feature Server it is hosted on.\nObjective\nCreate a map of Atlanta, visualized as a hexagon grid, that displays the amount of planned mixed use zoning. This will be done in the following sequence:\n\nCreating a fishnet (hexagon) grid over the city\nCreating intersected polygons\nCalculate the area of intersected polygons\nJoin back to the original fishnet grid\nvisualized.\nMixed-use zoning\nStart by loading sf, dplyr, and ggplot2. sf for our spatial work, dplyr for making our lives easier, and ggplot2 for a bad map later.\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nWe read in our data (mine is local). You can use the commented out code to read directly from the ArcGIS feature server.\n\n# read from the ArcGIS feature server\n# st_read(\"https://services5.arcgis.com/5RxyIIJ9boPdptdo/arcgis/rest/services/Land_Use_Future/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\")\n\nfuture_land_use <- read_sf(\"Future_Land_Use_.geojson\") |> \n  mutate(geometry = st_make_valid(geometry))\n\nLet’s look at the different land use descriptions.\n\nfuture_land_use |> \n  st_drop_geometry() |> \n  count(LANDUSEDESC, sort = TRUE) |> \n  reactable::reactable()\n\n\n\n\n\n\nTo see a disgusting map with a bad legend run the following.\n\nfuture_land_use |> \n  ggplot(aes(fill = LANDUSEDESC)) +\n  geom_sf(lwd = 0.15, color = \"black\")\n\nWe can see that there are a bunch of different descriptions for different types of mixed use zoning. Let’s filter down to descriptions that have \"Mixed-Use\" or \"Mixed Use\" and visualize them.\n\n# how much area of mixed use land use?\nmixed_use <- future_land_use |> \n  filter(grepl(\"Mixed-Use|Mixed Use\" , LANDUSEDESC)) \n\nggplot() +\n  geom_sf(data = mixed_use, fill = \"blue\", color = NA) +\n  theme_void()\n\n\n\n\nMaking a fishnet grid\nHaving made a fishnet grid quite a few times, I’ve got this handy function. In essence we create a grid over our target geometry and we keep only those locations from the grid that intersect eachother. If we dont’, we have a square shaped grid.\nIt is important that you create an ID for the grid, otherwise when we intersect later you’ll not know what is being intersected.\n\nmake_fishnet <- function(geometry, n = 10, hex = TRUE) {\n  g <- st_make_grid(geometry, square = !hex, n = n)\n  g[lengths(st_intersects(g, geometry)) != 0] \n}\n\n\ngrd <- make_fishnet(future_land_use, n = 40) |> \n  st_as_sf() |> \n  mutate(hex_id = row_number())\n\nplot(grd)\n\n\n\n\nMan, I love maps of sequential IDs.\nNext, we split our mixed use polygons based on the hexagons.\n\n# how much area in each hexagon\nlu_intersects <- st_intersection(mixed_use, grd)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nThen we calculate the area of each resultant shape.\n\noverlap_area <- lu_intersects |> \n  mutate(area = st_area(geometry)) \n\nplot(overlap_area[, \"area\"])\n\n\n\n\nThe next step here is to take the split polygons, and join the data back to the hexagons. I use a right join because they don’t get enough love. And also because if you try to do a join with two sf objects they’ll scream!!.\n\n# join it back to the grid\nhex_area_overlap <- st_drop_geometry(overlap_area) |> \n  select(hex_id, area) |> \n  right_join(grd, by = \"hex_id\") |> \n  st_as_sf() \n\nhex_area_overlap\n\nSimple feature collection with 1381 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -84.55738 ymin: 33.64417 xmax: -84.28635 ymax: 33.88926\nGeodetic CRS:  WGS 84\n# A tibble: 1,381 × 3\n   hex_id    area                                                              x\n    <int>   [m^2]                                                  <POLYGON [°]>\n 1     72 160485. ((-84.5182 33.65548, -84.52146 33.65737, -84.52146 33.66114, …\n 2     84  44538. ((-84.51493 33.64983, -84.5182 33.65171, -84.5182 33.65548, -…\n 3     85 176134. ((-84.51493 33.66114, -84.5182 33.66302, -84.5182 33.66679, -…\n 4     87   5049. ((-84.51493 33.68376, -84.5182 33.68565, -84.5182 33.68942, -…\n 5     97 380145. ((-84.51167 33.65548, -84.51493 33.65737, -84.51493 33.66114,…\n 6    100 110821. ((-84.51167 33.68942, -84.51493 33.6913, -84.51493 33.69507, …\n 7    106   8232. ((-84.51167 33.75729, -84.51493 33.75917, -84.51493 33.76294,…\n 8    110 109249. ((-84.5084 33.64983, -84.51167 33.65171, -84.51167 33.65548, …\n 9    111 150687. ((-84.5084 33.66114, -84.51167 33.66302, -84.51167 33.66679, …\n10    113 141654. ((-84.5084 33.68376, -84.51167 33.68565, -84.51167 33.68942, …\n# … with 1,371 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow plot it!\n\nggplot(hex_area_overlap, aes(fill = as.numeric(area))) +\n  geom_sf(color = \"black\", lwd = 0.15) +\n  theme_void() +\n  scale_fill_viridis_c(\n    option = \"plasma\", \n    na.value = NA,\n    labels = scales::comma\n    ) +\n  labs(fill = \"Area of mixed-use zoning (m)\")"
  },
  {
    "objectID": "posts/2022-11-17-overlapping-polys/post.html",
    "href": "posts/2022-11-17-overlapping-polys/post.html",
    "title": "Fishnets and overlapping polygons",
    "section": "",
    "text": "Today a question was asked in the geocompr discord. I wanted to share part of the solution as I think it covers 2 helpful things:\n\nmaking a fishnet grid\ncalculating the area of overlap between two polygons\n\nFor this example I’m using data from the Atlanta GIS Open Data Portal. Specifically using the future land use polygons.\nI’ve downloaded a local copy of the data as a geojson. But you can read it using the ArcGIS Feature Server it is hosted on.\nObjective\nCreate a map of Atlanta, visualized as a hexagon grid, that displays the amount of planned mixed use zoning. This will be done in the following sequence:\n\nCreating a fishnet (hexagon) grid over the city\nCreating intersected polygons\nCalculate the area of intersected polygons\nJoin back to the original fishnet grid\nvisualized.\nMixed-use zoning\nStart by loading sf, dplyr, and ggplot2. sf for our spatial work, dplyr for making our lives easier, and ggplot2 for a bad map later.\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nWe read in our data (mine is local). You can use the commented out code to read directly from the ArcGIS feature server.\n\n# read from the ArcGIS feature server\n# st_read(\"https://services5.arcgis.com/5RxyIIJ9boPdptdo/arcgis/rest/services/Land_Use_Future/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\")\n\nfuture_land_use <- read_sf(\"Future_Land_Use_.geojson\") |> \n  mutate(geometry = st_make_valid(geometry))\n\nLet’s look at the different land use descriptions.\n\nfuture_land_use |> \n  st_drop_geometry() |> \n  count(LANDUSEDESC, sort = TRUE) |> \n  reactable::reactable()\n\n\n\n\n\n\nTo see a disgusting map with a bad legend run the following.\n\nfuture_land_use |> \n  ggplot(aes(fill = LANDUSEDESC)) +\n  geom_sf(lwd = 0.15, color = \"black\")\n\nWe can see that there are a bunch of different descriptions for different types of mixed use zoning. Let’s filter down to descriptions that have \"Mixed-Use\" or \"Mixed Use\" and visualize them.\n\n# how much area of mixed use land use?\nmixed_use <- future_land_use |> \n  filter(grepl(\"Mixed-Use|Mixed Use\" , LANDUSEDESC)) \n\nggplot() +\n  geom_sf(data = mixed_use, fill = \"blue\", color = NA) +\n  theme_void()\n\n\n\n\nMaking a fishnet grid\nHaving made a fishnet grid quite a few times, I’ve got this handy function. In essence we create a grid over our target geometry and we keep only those locations from the grid that intersect eachother. If we dont’, we have a square shaped grid.\nIt is important that you create an ID for the grid, otherwise when we intersect later you’ll not know what is being intersected.\n\nmake_fishnet <- function(geometry, n = 10, hex = TRUE) {\n  g <- st_make_grid(geometry, square = !hex, n = n)\n  g[lengths(st_intersects(g, geometry)) != 0] \n}\n\n\ngrd <- make_fishnet(future_land_use, n = 40) |> \n  st_as_sf() |> \n  mutate(hex_id = row_number())\n\nplot(grd)\n\n\n\n\nMan, I love maps of sequential IDs.\nNext, we split our mixed use polygons based on the hexagons.\n\n# how much area in each hexagon\nlu_intersects <- st_intersection(mixed_use, grd)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nThen we calculate the area of each resultant shape.\n\noverlap_area <- lu_intersects |> \n  mutate(area = st_area(geometry)) \n\nplot(overlap_area[, \"area\"])\n\n\n\n\nThe next step here is to take the split polygons, and join the data back to the hexagons. I use a right join because they don’t get enough love. And also because if you try to do a join with two sf objects they’ll scream!!.\n\n# join it back to the grid\nhex_area_overlap <- st_drop_geometry(overlap_area) |> \n  select(hex_id, area) |> \n  right_join(grd, by = \"hex_id\") |> \n  st_as_sf() \n\nhex_area_overlap\n\nSimple feature collection with 1381 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -84.55738 ymin: 33.64417 xmax: -84.28635 ymax: 33.88926\nGeodetic CRS:  WGS 84\n# A tibble: 1,381 × 3\n   hex_id    area                                                              x\n    <int>   [m^2]                                                  <POLYGON [°]>\n 1     72 160485. ((-84.5182 33.65548, -84.52146 33.65737, -84.52146 33.66114, …\n 2     84  44538. ((-84.51493 33.64983, -84.5182 33.65171, -84.5182 33.65548, -…\n 3     85 176134. ((-84.51493 33.66114, -84.5182 33.66302, -84.5182 33.66679, -…\n 4     87   5049. ((-84.51493 33.68376, -84.5182 33.68565, -84.5182 33.68942, -…\n 5     97 380145. ((-84.51167 33.65548, -84.51493 33.65737, -84.51493 33.66114,…\n 6    100 110821. ((-84.51167 33.68942, -84.51493 33.6913, -84.51493 33.69507, …\n 7    106   8232. ((-84.51167 33.75729, -84.51493 33.75917, -84.51493 33.76294,…\n 8    110 109249. ((-84.5084 33.64983, -84.51167 33.65171, -84.51167 33.65548, …\n 9    111 150687. ((-84.5084 33.66114, -84.51167 33.66302, -84.51167 33.66679, …\n10    113 141654. ((-84.5084 33.68376, -84.51167 33.68565, -84.51167 33.68942, …\n# … with 1,371 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow plot it!\n\nggplot(hex_area_overlap, aes(fill = as.numeric(area))) +\n  geom_sf(color = \"black\", lwd = 0.15) +\n  theme_void() +\n  scale_fill_viridis_c(\n    option = \"plasma\", \n    na.value = NA,\n    labels = scales::comma\n    ) +\n  labs(fill = \"Area of mixed-use zoning (m)\")"
  },
  {
    "objectID": "posts/2022-11-23-youtube-videos.html",
    "href": "posts/2022-11-23-youtube-videos.html",
    "title": "YouTube Videos & what not",
    "section": "",
    "text": "Please vote or post a comment in this discuss on what would be helpful for you.\n\nI first made R programming videos when I had the opportunity to teach a remote and asynchronous course called Big Data for Cities. I used the videos as alternative learning material besides a text-book and other required readings.\nI just recently noticed that my video Making your R Markdown Pretty has 16 thousand views. I never thought it would reach that many people! So, if I’ve helped even 0.1% of those people it will have been worth it.\nI’ve started to record some more videos on spatial analaysis in R. I think the spatial anlaysis / statistics videos on youtube are lacking in diversity—with the noteable exception of @GeostatsGuyLectures but he’s more of a environmental scientist :) I will note, though, that the lectures of Luc Anselin are one of the only reason why I am where I am in my knowledge and abilities.\n\n\nThe thing about Anselin’s videos, though, is that they are not about the how of doing it. But they focus more on the theory and the math that sits behind the statistics themselves."
  },
  {
    "objectID": "posts/2022-11-23-youtube-videos.html#i-ask-you",
    "href": "posts/2022-11-23-youtube-videos.html#i-ask-you",
    "title": "YouTube Videos & what not",
    "section": "I ask you!",
    "text": "I ask you!\nWhat do you want to see? What is actually helpful? I’m sure me stumbling and mumbling for 18 minutes on spatial lags can’t be too helpful.\nPlease vote or leave a comment in this discussion."
  },
  {
    "objectID": "posts/2023-01-17-create-formulae.html",
    "href": "posts/2023-01-17-create-formulae.html",
    "title": "Programatically Create Formulas in R",
    "section": "",
    "text": "I’m working on a prototype of a tool using R to create some regressions. I’d like to take in independent and dependent variables as character vectors and use those to create a formula. It took me a minute to figure out how to programatically create a formula. It’s a bit tricky because formula require unquoted objects (symbols).\nThe trick is to use the function reformulate() (thanks StackOverflow).\nThe syntax is reformulate(x_vars, y_var)\n\nform <- reformulate(c(\"x1\", \"x2\"), \"y\")\nform\n\ny ~ x1 + x2\n\n\nNice. Throw it into a function.\n\nmake_lm <- function(.data, y, x) {\n  form <- reformulate(x, y)\n  lm(form, .data)\n}\n\nNow try it out :)\n\nmake_lm(\n  iris, \n  y = \"Petal.Width\",\n  x = c(\"Sepal.Length\", \"Petal.Length\")\n)\n\n\nCall:\nlm(formula = form, data = .data)\n\nCoefficients:\n (Intercept)  Sepal.Length  Petal.Length  \n   -0.008996     -0.082218      0.449376  \n\n\nThis can be a pretty hand pattern in package development. I hope it helps you."
  },
  {
    "objectID": "posts/2023-01-19-raw-strings-in-r.html",
    "href": "posts/2023-01-19-raw-strings-in-r.html",
    "title": "Raw strings in R",
    "section": "",
    "text": "The one thing about Python I actually really like is the ability to use raw strings. Raw strings are super helpful for me because at work I use a windows machine. And windows machines use a silly file path convention. The \\ back slack character is used as the file separator as opposed to the linux / unix / forward slash.\nUsing the backslash is so annoying because it’s also an escape character. In python I can write the following to hard code a file path.\nWhereas in R typically you would have to write:\nSince \\ is an escape character you have to escape it first using itself. So, its annoying. And file.path(\"nav\", \"to\", \"file\", \"path.ext\", fsep = \"\\\\\") is a wee bit cumbersome sometimes."
  },
  {
    "objectID": "posts/2023-01-19-raw-strings-in-r.html#we-have-all-been-sleeping-on-raw-strings-in-r-since-version-4.0.-it-is-r-version",
    "href": "posts/2023-01-19-raw-strings-in-r.html#we-have-all-been-sleeping-on-raw-strings-in-r-since-version-4.0.-it-is-r-version",
    "title": "Raw strings in R",
    "section": "WE HAVE ALL BEEN SLEEPING ON RAW STRINGS IN R SINCE VERSION 4.0. IT IS R VERSION",
    "text": "WE HAVE ALL BEEN SLEEPING ON RAW STRINGS IN R SINCE VERSION 4.0. IT IS R VERSION"
  },
  {
    "objectID": "posts/2023-01-19-raw-strings-in-r.html#aight.",
    "href": "posts/2023-01-19-raw-strings-in-r.html#aight.",
    "title": "Raw strings in R",
    "section": "Aight.",
    "text": "Aight.\nSo like, you can use raw strings today.\nHow can I get the R-devel news? I’m on the mailing list and get it once a week and it’s like “Re: memory leak in png() ` not this stuff. Tips?\nIt was announced in the news for version 4.0.0.\nThey write:\n\nThere is a new syntax for specifying raw character constants similar to the one used in C++: r”(…)” with … any character sequence not containing the sequence ‘⁠)“⁠’. This makes it easier to write strings that contain backslashes or both single and double quotes. For more details see ?Quotes.\n\nYou can write raw strings using the following formats:\n\nr\"( ... )\"\nr\"{ ... }\"\nr\"[ ... ]\"\nR\"( ... )\"\nR\"{ ... }\"\nR\"[ ... ]\"\n\nYou can even trickier by adding dashes between the quote and the delimter. The dashes need to be symmetrical though. So the following is also valid.\n\nr\"-{ ... }\"-\nr\"--{ ... }--\"\nr\"--{ * _ * }--\"\n\nIt kinda looks like a crab\nAlright so back to the example\n\n r\"{nav\\to\\file\\path.ext}\"\n\n[1] \"nav\\\\to\\\\file\\\\path.ext\"\n\n\nHot damn. Thats nice.\nI freaked out at first though because R prints two backslashes. But if you cat the result they go away. So do not worry.\n\n r\"{nav\\to\\file\\path.ext}\" |> \n  cat()\n\nnav\\to\\file\\path.ext"
  },
  {
    "objectID": "posts/2023-02-10-JHU.html#section",
    "href": "posts/2023-02-10-JHU.html#section",
    "title": "JHU",
    "section": "",
    "text": "Hey everyone, my name is Josiah. Thank you all for having me. And thank you René for inviting me here to chat with you all. It excites me to see so many people being exposed to GIS and spatial analysis. I’m going to use my time to tell you all about my experience from undergraduate, graduate school, meeting rene, and where I am today. Then I’ll talk briefly about the spatial analysis field of tech and the direction its moving. My hopes in chatting with you are two fold.\nFirst, I hope that you take away from this that your degree and the classes that you take do not dictate what field you have to go in. That your personal interests and studies are just as important, and if not sometimes more important, than what you learn in the classroom. Meaning, that while college classes are a wonderful introduction to subjects and fields nothing can compare with impassioned self-directed studying.\nSecondly, I want you to want to learn to program. I don’t want you to begrudgingly learn to program, I want you to want to program. Being even conversational in a programming langauge can pay compounding dividends.\n–\nSo who am I and how am I qualified to tell you anything? I can tell you who I am but I don’t know if I’m qualified for anything!\nI work at a company called Esri, we make the software known as ArcGIS. I work as a Sr. Product Engineer on the spatial statistics team. That means that I and a group of about 10 other people think deeply about what tools the “-ists” need to have at their disposal and how to make them intuitive, interpretable, and easy to use. By “ists” I mean the analysts, epidemiologists, biologists, criminologists, and what not.\nBefore that I worked at a company formerly called RStudio which is where I met René.\nI typically avoid biographical slides in my talks but I think in this case it may be important."
  },
  {
    "objectID": "posts/2023-02-10-JHU.html#objectives",
    "href": "posts/2023-02-10-JHU.html#objectives",
    "title": "JHU",
    "section": "objectives",
    "text": "objectives\n\ntell you my story\nencourage you to learn to program\na brief lesson on spatial analysis\na lay of the spatial analysis land\nanything you want this is your time"
  },
  {
    "objectID": "posts/2023-02-10-JHU.html#its-a-me",
    "href": "posts/2023-02-10-JHU.html#its-a-me",
    "title": "JHU",
    "section": "It’s a me",
    "text": "It’s a me\n\nMS in Urban Informatics, Northeastern\nSr. Product Engineer @ Esri\n\nSpatial Statistics\n\nPreviously @ RStudio (now Posit)\nProgramming in R since 2014"
  },
  {
    "objectID": "posts/2023-02-10-JHU.html#humble-beginnings",
    "href": "posts/2023-02-10-JHU.html#humble-beginnings",
    "title": "JHU",
    "section": "humble beginnings",
    "text": "humble beginnings\n\nPlymouth State"
  },
  {
    "objectID": "posts/2023-02-10-JHU.html#section-1",
    "href": "posts/2023-02-10-JHU.html#section-1",
    "title": "JHU",
    "section": "",
    "text": "Hey everyone, my name is Josiah. Thank you all for having me. And thank you René for inviting me here to chat with you all. It excites me to see so many people being exposed to GIS and spatial analysis. I’m going to use my time to tell you all about my experience from undergraduate, graduate school, meeting rene, and where I am today. Then I’ll talk briefly about the spatial analysis field of tech and the direction its moving. My hopes in chatting with you are two fold.\nFirst, I hope that you take away from this that your degree and the classes that you take do not dictate what field you have to go in. That your personal interests and studies are just as important, and if not sometimes more important, than what you learn in the classroom. Meaning, that while college classes are a wonderful introduction to subjects and fields nothing can compare with impassioned self-directed studying.\nSecondly, I want you to want to learn to program. I don’t want you to begrudgingly learn to program, I want you to want to program. Being even conversational in a programming langauge can pay compounding dividends.\n–\nSo who am I and how am I qualified to tell you anything? I can tell you who I am but I don’t know if I’m qualified for anything!\nI work at a company called Esri, we make the software known as ArcGIS. I work as a Sr. Product Engineer on the spatial statistics team. That means that I and a group of about 10 other people think deeply about what tools the “-ists” need to have at their disposal and how to make them intuitive, interpretable, and easy to use. By “ists” I mean the analysts, epidemiologists, biologists, criminologists, and what not.\nBefore that I worked at a company formerly called RStudio which is where I met René.\nI typically avoid biographical slides in my talks but I think in this case it may be important."
  },
  {
    "objectID": "posts/2023-02-10-JHU.html#finding-geography",
    "href": "posts/2023-02-10-JHU.html#finding-geography",
    "title": "JHU",
    "section": "finding geography",
    "text": "finding geography\n\nstudied anthropology and sociology"
  },
  {
    "objectID": "posts/2023-02-10-JHU.html#section-2",
    "href": "posts/2023-02-10-JHU.html#section-2",
    "title": "JHU",
    "section": "",
    "text": "Hey everyone, my name is Josiah. Thank you all for having me. And thank you René for inviting me here to chat with you all. It excites me to see so many people being exposed to GIS and spatial analysis. I’m going to use my time to tell you all about my experience from undergraduate, graduate school, meeting rene, and where I am today. Then I’ll talk briefly about the spatial analysis field of tech and the direction its moving. My hopes in chatting with you are two fold.\nFirst, I hope that you take away from this that your degree and the classes that you take do not dictate what field you have to go in. That your personal interests and studies are just as important, and if not sometimes more important, than what you learn in the classroom. Meaning, that while college classes are a wonderful introduction to subjects and fields nothing can compare with impassioned self-directed studying.\nSecondly, I want you to want to learn to program. I don’t want you to begrudgingly learn to program, I want you to want to program. Being even conversational in a programming langauge can pay compounding dividends.\n–\nSo who am I and how am I qualified to tell you anything? I can tell you who I am but I don’t know if I’m qualified for anything!\nI work at a company called Esri, we make the software known as ArcGIS. I work as a Sr. Product Engineer on the spatial statistics team. That means that I and a group of about 10 other people think deeply about what tools the “-ists” need to have at their disposal and how to make them intuitive, interpretable, and easy to use. By “ists” I mean the analysts, epidemiologists, biologists, criminologists, and what not.\nBefore that I worked at a company formerly called RStudio which is where I met René.\nI typically avoid biographical slides in my talks but I think in this case it may be important."
  },
  {
    "objectID": "posts/2023-02-10-JHU.html#section-3",
    "href": "posts/2023-02-10-JHU.html#section-3",
    "title": "JHU",
    "section": "",
    "text": "Hey everyone, my name is Josiah. Thank you all for having me. And thank you René for inviting me here to chat with you all. It excites me to see so many people being exposed to GIS and spatial analysis. I’m going to use my time to tell you all about my experience from undergraduate, graduate school, meeting rene, and where I am today. Then I’ll talk briefly about the spatial analysis field of tech and the direction its moving. My hopes in chatting with you are two fold.\nFirst, I hope that you take away from this that your degree and the classes that you take do not dictate what field you have to go in. That your personal interests and studies are just as important, and if not sometimes more important, than what you learn in the classroom. Meaning, that while college classes are a wonderful introduction to subjects and fields nothing can compare with impassioned self-directed studying.\nSecondly, I want you to want to learn to program. I don’t want you to begrudgingly learn to program, I want you to want to program. Being even conversational in a programming langauge can pay compounding dividends.\n–\nSo who am I and how am I qualified to tell you anything? I can tell you who I am but I don’t know if I’m qualified for anything!\nI work at a company called Esri, we make the software known as ArcGIS. I work as a Sr. Product Engineer on the spatial statistics team. That means that I and a group of about 10 other people think deeply about what tools the “-ists” need to have at their disposal and how to make them intuitive, interpretable, and easy to use. By “ists” I mean the analysts, epidemiologists, biologists, criminologists, and what not.\nBefore that I worked at a company formerly called RStudio which is where I met René.\nI typically avoid biographical slides in my talks but I think in this case it may be important."
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#objectives",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#objectives",
    "title": "Josiah Parry",
    "section": "objectives",
    "text": "objectives\n\ntell you my story\nmaybe inspire you to program\na brief lesson on spatial analysis\nanything you want this is your time\n\n\nI hope that you take away from this that your degree and the classes that you take do not dictate what field you have to go in. That your personal interests and studies are just as important, and if not sometimes more important, than what you learn in the classroom. Meaning, that while college classes are a wonderful introduction to subjects and fields nothing can compare with impassioned self-directed studying.\nI want you to want to learn to program. I don’t want you to begrudgingly learn to program, I want you to want to program. Being even conversational in a programming langauge can pay compounding dividends.\ni also want to discuss one brief but very important topic in spatial analysis.\nand really this time is yours so you can tell me what you want from me and i can do my best to provide"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#its-a-me",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#its-a-me",
    "title": "Josiah Parry",
    "section": "It’s a me",
    "text": "It’s a me\n\nMS in Urban Informatics, Northeastern\nSr. Product Engineer @ Esri\n\nSpatial Statistics\n\nPreviously @ RStudio (now Posit)\nProgramming in R since 2014\n\n\nSo who am I and how am I qualified to tell you anything? I can tell you who I am but I don’t know if I’m qualified for anything!\nI graduated with a degree in Urban informatics from NEU in April 2020. It was a really bad time but yet also very good time to finish a degree\nI work at a company called Esri, we make the software known as ArcGIS. I work as a Sr. Product Engineer on the spatial statistics team. That means that I and a group of about 10 other people think deeply about what tools the “-ists” need to have at their disposal and how to make them intuitive, interpretable, and easy to use. By “ists” I mean the analysts, epidemiologists, biologists, criminologists, and what not.\nBefore that I worked at a company formerly called RStudio which is where I met René. I worked with government customers ensuring they could use RStudio professional products and bridge open source tools and enterprise products"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#humble-beginnings",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#humble-beginnings",
    "title": "Josiah Parry",
    "section": "humble beginnings",
    "text": "humble beginnings\n\nPlymouth State\nUnlike many of you here I was not, and still am not, very academically inclined. I went to a small school in Northern New Hampshire called Plymouth State University (PSU)."
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#section-3",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#section-3",
    "title": "Josiah Parry",
    "section": "",
    "text": "Hey everyone, my name is Josiah. Thank you all for having me. And thank you René for inviting me here to chat with you all. It excites me to see so many people being exposed to GIS and spatial analysis. I’m going to use my time to tell you all about my experience from undergraduate, graduate school, meeting rene, and where I am today. Then I’ll talk briefly about the spatial analysis field of tech and the direction its moving. My hopes in chatting with you are two fold.\nFirst, I hope that you take away from this that your degree and the classes that you take do not dictate what field you have to go in. That your personal interests and studies are just as important, and if not sometimes more important, than what you learn in the classroom. Meaning, that while college classes are a wonderful introduction to subjects and fields nothing can compare with impassioned self-directed studying.\nSecondly, I want you to want to learn to program. I don’t want you to begrudgingly learn to program, I want you to want to program. Being even conversational in a programming langauge can pay compounding dividends.\n–\nSo who am I and how am I qualified to tell you anything? I can tell you who I am but I don’t know if I’m qualified for anything!\nI work at a company called Esri, we make the software known as ArcGIS. I work as a Sr. Product Engineer on the spatial statistics team. That means that I and a group of about 10 other people think deeply about what tools the “-ists” need to have at their disposal and how to make them intuitive, interpretable, and easy to use. By “ists” I mean the analysts, epidemiologists, biologists, criminologists, and what not.\nBefore that I worked at a company formerly called RStudio which is where I met René.\nI typically avoid biographical slides in my talks but I think in this case it may be important."
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#the-next-few-years",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#the-next-few-years",
    "title": "Josiah Parry",
    "section": "The next few years",
    "text": "The next few years\n\nspend all my free time learning R\nI intern at DataCamp making R courses\nResearch using interactive GIS\nDecide grad school was the right thing for me\n\npromptly rejected from all but one school"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#stats-interest-became-an-r-interest",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#stats-interest-became-an-r-interest",
    "title": "Josiah Parry",
    "section": "stats interest became an R interest",
    "text": "stats interest became an R interest\n\nwanted to do more advanced stats\nlearned 1:1 with my professor\nfinding data meant cleaning data\nexplored new packages and exposed to new domains\n\nnatural language\nsoftware engineering\netc"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#in-urban-studies",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#in-urban-studies",
    "title": "Josiah Parry",
    "section": "In urban studies",
    "text": "In urban studies\n\nthe neighborhood fundamental to sociology\n\nChicago school (Park & Burgess)\n\nused to understand differences inside of the city\ntoo much nuance"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#in-spatial-analysis",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#in-spatial-analysis",
    "title": "Josiah Parry",
    "section": "In spatial analysis",
    "text": "In spatial analysis\n\nare phenomena spatially dependent?\n\ndo similar values occur near each other\n\nstart focal with a location \\(i\\)\nit’s neighbors are \\(j\\)\n\\(X_i\\) is compared to \\(X_j\\)\n\nnot to \\(X\\)"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#contiguities",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#contiguities",
    "title": "Josiah Parry",
    "section": "contiguities",
    "text": "contiguities\nhow do you choose what the neighbors are for a location?"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#rook-contiguity",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#rook-contiguity",
    "title": "Josiah Parry",
    "section": "Rook Contiguity",
    "text": "Rook Contiguity"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#rook-contiguity-1",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#rook-contiguity-1",
    "title": "Josiah Parry",
    "section": "Rook Contiguity",
    "text": "Rook Contiguity"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#queen-contiguity",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#queen-contiguity",
    "title": "Josiah Parry",
    "section": "Queen contiguity",
    "text": "Queen contiguity"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#queen-contiguity-1",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#queen-contiguity-1",
    "title": "Josiah Parry",
    "section": "Queen contiguity",
    "text": "Queen contiguity"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#neighbors-in-practice",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#neighbors-in-practice",
    "title": "Josiah Parry",
    "section": "Neighbors in practice",
    "text": "Neighbors in practice"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#understanding-the-lag",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#understanding-the-lag",
    "title": "Josiah Parry",
    "section": "Understanding the lag",
    "text": "Understanding the lag\n\n“expected value” of the neighborhood\nit is the average value of the neighborhood (excluding \\(i\\))\nsummarizes values of \\(x\\) for an observation \\(i\\)’s neighborhood"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#observed-values",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#observed-values",
    "title": "Josiah Parry",
    "section": "Observed values",
    "text": "Observed values"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#neighborhood-values-at-i",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#neighborhood-values-at-i",
    "title": "Josiah Parry",
    "section": "Neighborhood values at \\(i\\)",
    "text": "Neighborhood values at \\(i\\)\n\n\nthe lag is a neighborhood smoother"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#not-useful-on-its-own",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#not-useful-on-its-own",
    "title": "Josiah Parry",
    "section": "Not useful on it’s own",
    "text": "Not useful on it’s own\n\n\nwhere locations deviate from their neighbors"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#observed-vs-neighborhood",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#observed-vs-neighborhood",
    "title": "Josiah Parry",
    "section": "Observed vs neighborhood",
    "text": "Observed vs neighborhood\n\n\nwhere locations deviate from their neighbors"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#spatial-lag-is-the-basis-of",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#spatial-lag-is-the-basis-of",
    "title": "Josiah Parry",
    "section": "Spatial Lag is the basis of:",
    "text": "Spatial Lag is the basis of:\n\nspatial clustering (autocorrelation)\nhot spot detection (clustering)\nspatial regression\n\n(inference / neighborhood spill over effects)\n\nspatio-temporal hot spot analysis"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#tools-in-the-r-ecosystem",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#tools-in-the-r-ecosystem",
    "title": "Josiah Parry",
    "section": "Tools in the R ecosystem",
    "text": "Tools in the R ecosystem\n\nsf - spatial vector data\nspdep - spatial statistics\nsfdep - a tidy interface to spdep\nrgeoda - R interface to GeoDa"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#tools-in-python-ecosystem",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#tools-in-python-ecosystem",
    "title": "Josiah Parry",
    "section": "Tools in Python ecosystem",
    "text": "Tools in Python ecosystem\n\ngeopandas - sf equivalent\npysal a very robust set of spatial statistics toools\nshapely for geometries"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#northeastern",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#northeastern",
    "title": "Josiah Parry",
    "section": "Northeastern",
    "text": "Northeastern\n\n\nat northeastern is when I really dove into spatial analysis my understanding and utility of it really began when i was a research assistant at the boston area research initiative\nwe looked at indicators of social disorder using administrative data.\na lot of what we looked at were neighbor hood differences but very simple differences using fixed effects in regressions"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#advanced-spatial-analysis",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#advanced-spatial-analysis",
    "title": "Josiah Parry",
    "section": "Advanced Spatial Analysis",
    "text": "Advanced Spatial Analysis"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#advanced-spatial-analysis-1",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#advanced-spatial-analysis-1",
    "title": "Josiah Parry",
    "section": "Advanced Spatial Analysis",
    "text": "Advanced Spatial Analysis\n\none of my last courses in grad school\nlearned about spatial autocorrelation\nspatial regression\nbasics of networks analysis\n\n\ni learned just enough to know where to look. Everything I learned in my graduate program was just enough to know where to look for more if i wanted to"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#a-pandemic-rages",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#a-pandemic-rages",
    "title": "Josiah Parry",
    "section": "a pandemic rages",
    "text": "a pandemic rages"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#example",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#example",
    "title": "Josiah Parry",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#example-1",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#example-1",
    "title": "Josiah Parry",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "posts/2023-02-10-jhu/2023-02-10-JHU.html#what-do-you-want-to-do-next",
    "href": "posts/2023-02-10-jhu/2023-02-10-JHU.html#what-do-you-want-to-do-next",
    "title": "Josiah Parry",
    "section": "What do you want to do next?",
    "text": "What do you want to do next?\n\ni can answer questions\ni can demo code\ni can talk through the spatial lag in more detail\nwe can discuss hot spot analysis\nanything you want :)"
  },
  {
    "objectID": "posts/2023-03-01-learning-rust.html",
    "href": "posts/2023-03-01-learning-rust.html",
    "title": "learning rust",
    "section": "",
    "text": "I have been wanting to learn a low level language for quite some time. Due to the power and prominence of Rcpp I had thought I wanted to learn C++. Every fast package in R uses C++, right? But Rust kept popping up. Rust, is fast. Rust is safe. Linux is going to be rewritten in Rust. Rust is the most loved language for 7 years in a row. Rust is easily multithreaded. Rust. Rust. Rust. I then heard a bit about rextendr a way to incorporate Rust into R packages. With that Rust became a real candidate language to learn.\nThe @hrbrmstr provided some sweet words of encouragement over twitter, an example repo to reference, and provided ideas on how to start learning Rust.\nI spent some time with “The Book” to wrap my head around the basics. I then spent some time writing a chess FEN parser. I wrote a FEN parser. I wrote a whole program in Rust. That was crazy. The reason why I was able to continue learning Rust is because it is so easy to use (in comparison to C++). Once you wrap your head around the basics it rocks.\nRust is a compiled language. Working with a compiled language is a huge paradigm shift. R is an interpreted language. Interpreted languages are super cool because we can run one line of code, do some stuff, and then run another line. Compiled languages have to take everything in at once. So there is no running a line, printing an object, then running another line. But that’s actually okay because of the Rust compiler is the best teacher I’ve ever had.\nWhen you make a mistake in Rust, the compiler will tell you exactly where that mistake is coming from—literally the line and column position. It will also often tell you exactly what code you need to change and how to change it to make your code run. So rather than running line by line, you can compile line by line.\nLearning Rust has made me a better R programmer for many reasons. Here are a few:\n\nI am conscious of type conversions and consistency\nI am conscientious of memory consumption\nI am a glutton for speed now\nI have a better understanding / framework for thinking about inheritance\n\nProgramming in Rust has made me think of ways that R can be improved. Mostly in that scalar classes are missing from R (and from vctrs). We also lack the ability to use 64 bit integers which is a bit of a problem. I also think R packages should be designed to be extended. This would be done by exposing generic s3 functions that can be extended for your class. If the method exists for your class you inherit the functionality. I employed a prototype of this idea in the sdf package."
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers.html",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers.html",
    "title": "Rust traits for R users",
    "section": "",
    "text": "In the few months that I’ve been programming in Rust nothing has so fundamentally shifted the way that I think about programming—and specifically R packages—as Rust traits. I want to talk briefly about Rust traits and why I think they can make R packages better.\nA trait defines a set of behaviors that can be used by objects of different kinds. Each trait is a collection of methods (functions) whose behavior is defined abstractly. The traits can then be implemented for different object types. Any object that implements the trait can then use that method. It’s kind of confusing, isn’t it? Let’s work through the example in The Book™ and how we can implement it in R."
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers.html#implications-for-r-packages",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers.html#implications-for-r-packages",
    "title": "Rust traits for R users",
    "section": "Implications for R packages",
    "text": "Implications for R packages\nThis very simple concept can be transformative for the way that we build R packages. R packages are, for the most part, bespoke. Each one serves their own purpose and works only within its own types or types it’s aware of. But what if an R package could work with any object type? Using this idea we can get from Rust traits, we can do that.\nPackages that want to be extensible can make it easy to do so by doing two fairly simple things. Low-level and critical functions should be exported as generic functions. High level functions that perform some useful functionality should be built upon those generics.\nAn example is the sdf package I’ve prototyped based on this idea. In this case, I have a spatial data frame class that can be implemented on any object that implements methods for the following functions:\n\nis_geometry()\nbounding_box()\ncombine_geometry()\n\nAn example\nThe sdf class is a tibble with a geometry column and a bounding box attribute. The function as_sdf() creates an sdf object that tells us what type of geometry is used and the bounding box of it.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.1; sf_use_s2() is TRUE\n\nlibrary(sdf)\n\n\nAttaching package: 'sdf'\n\n\nThe following object is masked from 'package:sf':\n\n    is_geometry\n\n# get some sample data \ng <- sfdep::guerry[, \"region\"]\n\nas_sdf(g)\n\nGeometry Type: sfc_MULTIPOLYGON\nBounding box: xmin: 47680 ymin: 1703258 xmax: 1031401 ymax: 2677441\n# A tibble: 85 × 2\n   region                                                               geometry\n   <fct>                                                          <MULTIPOLYGON>\n 1 E      (((801150 2092615, 800669 2093190, 800688 2095430, 800780 2095795, 80…\n 2 N      (((729326 2521619, 729320 2521230, 729280 2518544, 728751 2517520, 72…\n 3 C      (((710830 2137350, 711746 2136617, 712430 2135212, 712070 2134132, 71…\n 4 E      (((882701 1920024, 882408 1920733, 881778 1921200, 881526 1922332, 87…\n 5 E      (((886504 1922890, 885733 1922978, 885479 1923276, 883061 1925266, 88…\n 6 S      (((747008 1925789, 746630 1925762, 745723 1925138, 744216 1925236, 74…\n 7 N      (((818893 2514767, 818614 2514515, 817900 2514467, 817327 2514945, 81…\n 8 S      (((509103 1747787, 508820 1747513, 508154 1747093, 505861 1746627, 50…\n 9 E      (((775400 2345600, 775068 2345397, 773587 2345177, 772940 2344780, 77…\n10 S      (((626230 1810121, 626269 1810496, 627494 1811321, 627681 1812424, 62…\n# … with 75 more rows\n\n\nThis is super cool because we can group by and summarize the data just because we have those above functions defined for sfc objects (the geometry column).\n\nlibrary(dplyr)\nas_sdf(g) |> \n  group_by(region) |> \n  summarise(n = n())\n\nGeometry Type: sfc_MULTIPOLYGON\nBounding box: xmin: 47680 ymin: 1703258 xmax: 1031401 ymax: 2677441\n# A tibble: 5 × 3\n  region     n                                                          geometry\n  <fct>  <int>                                                    <MULTIPOLYGON>\n1 C         17 (((710830 2137350, 711746 2136617, 712430 2135212, 712070 213413…\n2 E         17 (((801150 2092615, 800669 2093190, 800688 2095430, 800780 209579…\n3 N         17 (((729326 2521619, 729320 2521230, 729280 2518544, 728751 251752…\n4 S         17 (((747008 1925789, 746630 1925762, 745723 1925138, 744216 192523…\n5 W         17 (((456425 2120055, 456229 2120382, 455943 2121064, 456070 212219…\n\n\nSay we want to create a custom Point class that we want to be usable by an sdf object. We can do this rather simply by creating the proper generics. A Point will be a list of length 2 numeric vectors where the first element is the x coordinate and the second element is the y coordinate.\n\n# create some points\npnt_data <- lapply(1:5, \\(x) runif(2, 0, 90))\n\n# create new vector class\npnts <- vctrs::new_vctr(pnt_data, class = \"Point\")\n\npnts\n\n<Point[5]>\n[1] 23.14043, 87.07455 45.15752, 50.44104 52.56717, 68.01389 87.97772, 18.10026\n[5] 15.29674, 28.89251\n\n\nNow we can start defining our methods. is_geometry() should always return TRUE for our type. We can do this like so:\n\nis_geometry.Point <- function(x) inherits(x, \"Point\")\n\n\n\nThis method will only be dispatched on Points so it will always inherit the Point class. One could just as well always return TRUE\nNext we need to define a method for the bounding box. This is the the maximum and minimum x and y coordinates. Our method should iterate over each point and extract the x and y into their own vector and return the minimum and maxes. These need to be structured in a particular order.\n\nbounding_box.Point <- function(.x) {\n  x <- vapply(.x, `[`, numeric(1), 1)\n  y <- vapply(.x, `[`, numeric(1), 2)\n  \n  c(xmin = min(x), ymin = min(y), xmax = max(x), ymax = max(y))\n}\n\nLastly, we need a way to combine the points together. In this case, we can just “combine” the points by finding the average point. This is not geometrically sound but for the sake of example it suffices. Note that the type it returns always has to be the same! There is not a compiler forcing us, so we must force ourselves!\n\ncombine_geometry.Point <- function(.x) {\n  x <- vapply(.x, `[`, numeric(1), 1)\n  y <- vapply(.x, `[`, numeric(1), 2)\n  \n  vctrs::new_vctr(\n    list(c(mean(x), mean(y))), \n    class = \"Point\"\n    )\n}\n\nWith only those 3 functions we’ve defined enough to create an sdf object where the geometry column is a Point vector. To illustrate this we can use the ggplot2 diamonds data set for example since it has nice x and y coordinates.\nFirst we create a data frame with a Point column.\n\ndata(diamonds, package = \"ggplot2\")\n\ndiamond_pnts <- diamonds |> \n  mutate(\n    pnts = vctrs::new_vctr(\n      purrr::map2(x, y, `c`),\n      class = \"Point\"\n    )\n  ) |> \n  select(cut, pnts)\n\nhead(diamond_pnts)\n\n# A tibble: 6 × 2\n  cut             pnts\n  <ord>        <Point>\n1 Ideal     3.95, 3.98\n2 Premium   3.89, 3.84\n3 Good      4.05, 4.07\n4 Premium   4.20, 4.23\n5 Good      4.34, 4.35\n6 Very Good 3.94, 3.96\n\n\nNext we cast it to an sdf object by using as_sdf().\n\ndiamond_sdf <- as_sdf(diamond_pnts) \ndiamond_sdf\n\nGeometry Type: Point\nBounding box: xmin: 0 ymin: 0 xmax: 10.74 ymax: 58.9\n# A tibble: 53,940 × 2\n   cut             pnts\n   <ord>        <Point>\n 1 Ideal     3.95, 3.98\n 2 Premium   3.89, 3.84\n 3 Good      4.05, 4.07\n 4 Premium   4.20, 4.23\n 5 Good      4.34, 4.35\n 6 Very Good 3.94, 3.96\n 7 Very Good 3.95, 3.98\n 8 Very Good 4.07, 4.11\n 9 Fair      3.87, 3.78\n10 Very Good 4.00, 4.05\n# … with 53,930 more rows\n\n\nNotice that the printing method shows Geometry Type: Point and also has a Bounding box:. That means we have effectively extended the sdf class by implementing our own methods for the exported generic functions from sdf. From that alone the sdf methods for dplyr can be used.\n\ndiamond_sdf |> \n  group_by(cut) |> \n  summarise(n = n())\n\nGeometry Type: Point\nBounding box: xmin: 5.507 ymin: 5.52 xmax: 6.247 ymax: 6.183\n# A tibble: 5 × 3\n  cut           n               pnts\n  <ord>     <int>            <Point>\n1 Fair       1610 6.246894, 6.182652\n2 Good       4906 5.838785, 5.850744\n3 Very Good 12082 5.740696, 5.770026\n4 Premium   13791 5.973887, 5.944879\n5 Ideal     21551 5.507451, 5.520080\n\n\nWhy this works\nThe dplyr sdf methods work like a charm because they use generic functions. Take the summarise() method for example.\n\nsdf:::summarise.sdf\n\nfunction(.data, ...) {\n  geom_col_name <- attr(.data, \"geom_column\")\n  geom_col <- .data[[geom_col_name]]\n  gd <- group_data(.data)\n\n  summarized_geoms <- lapply(gd$.rows, function(ids) combine_geometry(geom_col[ids]))\n\n  res <- NextMethod()\n\n  res[[geom_col_name]] <- rlang::inject(c(!!!summarized_geoms))\n  as_sdf(res)\n\n}\n<bytecode: 0x108f81c88>\n<environment: namespace:sdf>\n\n\nThis method uses the combine_geometry() generic function. combine_geometry() takes a vector of geometries (as determined by is_geometry()) and returns a single element. The summarise method does not care which method is used. It only cares that the output is consistent—in this case that a scalar value is outputted and that multiple of those scalars can be combined using c().\nAnother example\nFor a more detailed example check out the section in the README that implements an sdf class for geos geometry. If you’re interested in the details I recommend looking at the source code it is very simple.\nFirst look at the generic method definitions. Then look at the sf compatibility methods."
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers.html#implementing-a-trait",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers.html#implementing-a-trait",
    "title": "Rust traits for R users",
    "section": "\nimplementing a trait",
    "text": "implementing a trait\nIt’s important that we are able to summarise our newspaper article for the socials ofc—so we need to implement the Summary trait for the NewsArticle struct.\nimpl Summary for NewsArticle {\n    fn summarize(&self) -> String {\n        format!(\"{}, by {} ({})\", self.headline, self.author, self.location)\n    }\n}\n\n\nNotice that the trait is called Summary whereas the method it provides is called summarize(). For the sake of example I’m going to call the R function Summary() throughout the rest of the example. It’s not possible to have a perfect 1:1 relationship between Rust and R ;)\nThis block defines how the summarize() method will work for a NewsArticle struct. It will create a string in the format of \"{title}, by {author} ({location})\". In R, we have to define the NewsArticle method for the Summary function which is done by creating a function object with the name signature {Generic}.{class} <- function(...) { . . . }.\n\n\nBase R\nvctrs\n\n\n\n\nSummary.NewsArticle <- function(x) {\n  sprintf(\n    \"%s, by %s (%s)\",\n    x[[\"headline\"]],\n    x[[\"author\"]],\n    x[[\"location\"]]\n  )\n}\n\n\n\n\nSummary.NewsArticle <- function(x) {\n  sprintf(\n    \"%s, by %s (%s)\",\n    vctrs::field(x, \"headline\"),\n    vctrs::field(x, \"author\"),\n    vctrs::field(x, \"location\")\n  )\n}\n\n\n\n\nSince Musk’s takeover of twitter, tweets are getting out of hand becoming ridiculously long so we need to be able to summarize them too! So if we define a Tweet struct and a corresponding implementation of Summary we’ll be able to easily summarize them exactly the same way as news articles.\n// define the struct\npub struct Tweet {\n    pub username: String,\n    pub content: String,\n    pub reply: bool,\n    pub retweet: bool,\n}\n\n// implement the trait\nimpl Summary for Tweet {\n    fn summarize(&self) -> String {\n        format!(\"{}: {}\", self.username, self.content)\n    }\n}\nCorrespondingly in R, we’re going to be working with both Tweets and News Articles. So we need to define a tweet class to contain our tweets and a Summary() method for the new class.\n\n\nBase R\nvctrs\n\n\n\n\nstructure(\n  list(\n    username = character(),\n    content = character(),\n    reply = logical(),\n    retweet = logical()\n  ),\n  class = \"Tweet\"\n)\n\n$username\ncharacter(0)\n\n$content\ncharacter(0)\n\n$reply\nlogical(0)\n\n$retweet\nlogical(0)\n\nattr(,\"class\")\n[1] \"Tweet\"\n\nSummary.Tweet <- function(x) {\n  sprintf(\"%s: %s\", x[[\"username\"]], x[[\"content\"]])\n}\n\n\n\n\nvctrs::new_rcrd(\n    list(\n    username = character(),\n    content = character(),\n    reply = character(),\n    retweet = logical()\n  ),\n  class = \"Tweet\"\n)\n\n<Tweet[0]>\n\nSummary.Tweet <- function(x) {\n  sprintf(\n    \"%s: %s\", \n    vctrs::field(x, \"username\"), \n    vctrs::field(x, \"content\")\n    )\n}\n\n\n\n\nWe can now define a function that utilizes this trait that will produce consistent String output in the same format for both tweets and new articles.\npub fn notify(item: &impl Summary) {\n    println!(\"Breaking news! {}\", item.summarize());\n}\nThis is huge from the R perspective because we can create a notify() function that calls Summary() and as long as a method if defined for the input class it will work!\n\nnotify <- function(item) {\n  sprintf(\"Breaking news! %s\", Summary(item))\n}\n\nTo test this out lets create a Tweet and a NewsArticle. First we’ll create constructor functions for each.\n\n\nBase R\nvctrs\n\n\n\n\nnew_article <- function(headline, location, author, content) {\n  structure(\n    list(\n      headline = headline,\n      location = location,\n      author = author,\n      content = content\n      ), \n    class = \"NewsArticle\"\n  )\n}\n\nnew_tweet <- function(username, content, reply, retweet) {\n  structure(\n    list(\n      username = username,\n      content = content,\n      reply = reply,\n      retweet = retweet\n    ),\n    class = \"Tweet\"\n  )\n}\n\n\n\n\nnew_article <- function(headline, location, author, content) {\n  vctrs::new_rcrd(\n    list(\n      headline = headline,\n      location = location,\n      author = author,\n      content = content\n      ), \n    class = \"NewsArticle\"\n  )\n}\n\nnew_tweet <- function(username, content, reply, retweet) {\n  vctrs::new_rcrd(\n    list(\n      username = username,\n      content = content,\n      reply = reply,\n      retweet = retweet\n    ),\n    class = \"Tweet\"\n  )\n}\n\n\n\n\nUsing the constructors we can create a tweet and a news article.\n\n# https://www.theonion.com/new-absolut-ad-features-swaying-mom-with-one-eye-closed-1850138855\narticle <- new_article(\n  \"New Absolut Ad Features Swaying Mom With One Eye Closed Telling Camera She Used To Dance\",\n  \"Stockholm\",\n  \"The Onion\", \n  \"The ad concludes abruptly with the mother beginning to cry when, for no particular reason, she suddenly remembers the death of Princess Diana.\"\n)\n\n# https://twitter.com/TheOnion/status/1631104570041552896\ntweet <- new_tweet(\n  \"@TheOnion\",\n  \"Cat Internally Debates Whether Or Not To Rip Head Off Smaller Creature It Just Met https://bit.ly/3J1kNzV\",\n  FALSE,\n  FALSE\n)\n\nWe can see how notify works for both of these.\n\nnotify(tweet)\n\n[1] \"Breaking news! @TheOnion: Cat Internally Debates Whether Or Not To Rip Head Off Smaller Creature It Just Met https://bit.ly/3J1kNzV\"\n\nnotify(article)\n\n[1] \"Breaking news! New Absolut Ad Features Swaying Mom With One Eye Closed Telling Camera She Used To Dance, by The Onion (Stockholm)\"\n\n\nwhat this means\nThis is awesome. This means that any object that we create in the future, as long as it implements the Summary() function for its class we can utilize the notify() function. This comes with a caveat, though—as all good things do.\nThe Rust compiler ensures that any object that implements the Summary trait returns a single string. R is far more laissez faire than Rust with classes and types. One could create a Summary method for an object that returns a vector of strings. That would break notify. Either notify() should have type checking or you should make sure that your method always produces the correct type."
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers.html#implementing-a-trait-1",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers.html#implementing-a-trait-1",
    "title": "Rust traits make for better R packages",
    "section": "\nimplementing a trait",
    "text": "implementing a trait\nIt’s important that we are able to summarise our newspaper article for the socials ofc—so we need to implement the Summary trait for the NewsArticle struct.\nimpl Summary for NewsArticle {\n    fn summarize(&self) -> String {\n        format!(\"{}, by {} ({})\", self.headline, self.author, self.location)\n    }\n}\n\n\nNotice that the trait is called Summary whereas the method it provides is called summarize(). For the sake of example I’m going to call the R function Summary() throughout the rest of the example. It’s not possible to have a perfect 1:1 relationship between Rust and R ;)\nThis block defines how the summarize() method will work for a NewsArticle struct. It will create a string in the format of \"{title}, by {author} ({location})\". In R, we have to define the NewsArticle method for the Summary function which is done by creating a function object with the name signature {Generic}.{class} <- function(...) { . . . }.\n\n\nBase R\nvctrs\n\n\n\n\nSummary.NewsArticle <- function(x) {\n  sprintf(\n    \"%s, by %s (%s)\",\n    x[[\"headline\"]],\n    x[[\"author\"]],\n    x[[\"location\"]]\n  )\n}\n\n\n\n\nSummary.NewsArticle <- function(x) {\n  sprintf(\n    \"%s, by %s (%s)\",\n    vctrs::field(x, \"headline\"),\n    vctrs::field(x, \"author\"),\n    vctrs::field(x, \"location\")\n  )\n}\n\n\n\n\nSince Musk’s takeover of twitter, tweets are getting out of hand becoming ridiculously long so we need to be able to summarize them too! So if we define a Tweet struct and a corresponding implementation of Summary we’ll be able to easily summarize them exactly the same way as news articles.\n// define the struct\npub struct Tweet {\n    pub username: String,\n    pub content: String,\n    pub reply: bool,\n    pub retweet: bool,\n}\n\n// implement the trait\nimpl Summary for Tweet {\n    fn summarize(&self) -> String {\n        format!(\"{}: {}\", self.username, self.content)\n    }\n}\nCorrespondingly in R, we’re going to be working with both Tweets and News Articles. So we need to define a tweet class to contain our tweets and a Summary() method for the new class.\n\n\nBase R\nvctrs\n\n\n\n\nstructure(\n  list(\n    username = character(),\n    content = character(),\n    reply = logical(),\n    retweet = logical()\n  ),\n  class = \"Tweet\"\n)\n\n$username\ncharacter(0)\n\n$content\ncharacter(0)\n\n$reply\nlogical(0)\n\n$retweet\nlogical(0)\n\nattr(,\"class\")\n[1] \"Tweet\"\n\nSummary.Tweet <- function(x) {\n  sprintf(\"%s: %s\", x[[\"username\"]], x[[\"content\"]])\n}\n\n\n\n\nvctrs::new_rcrd(\n    list(\n    username = character(),\n    content = character(),\n    reply = character(),\n    retweet = logical()\n  ),\n  class = \"Tweet\"\n)\n\n<Tweet[0]>\n\nSummary.Tweet <- function(x) {\n  sprintf(\n    \"%s: %s\", \n    vctrs::field(x, \"username\"), \n    vctrs::field(x, \"content\")\n    )\n}\n\n\n\n\nWe can now define a function that utilizes this trait that will produce consistent String output in the same format for both tweets and new articles.\npub fn notify(item: &impl Summary) {\n    println!(\"Breaking news! {}\", item.summarize());\n}\nThis is huge from the R perspective because we can create a notify() function that calls Summary() and as long as a method if defined for the input class it will work!\n\nnotify <- function(item) {\n  sprintf(\"Breaking news! %s\", Summary(item))\n}\n\nTo test this out lets create a Tweet and a NewsArticle. First we’ll create constructor functions for each.\n\n\nBase R\nvctrs\n\n\n\n\nnew_article <- function(headline, location, author, content) {\n  structure(\n    list(\n      headline = headline,\n      location = location,\n      author = author,\n      content = content\n      ), \n    class = \"NewsArticle\"\n  )\n}\n\nnew_tweet <- function(username, content, reply, retweet) {\n  structure(\n    list(\n      username = username,\n      content = content,\n      reply = reply,\n      retweet = retweet\n    ),\n    class = \"Tweet\"\n  )\n}\n\n\n\n\nnew_article <- function(headline, location, author, content) {\n  vctrs::new_rcrd(\n    list(\n      headline = headline,\n      location = location,\n      author = author,\n      content = content\n      ), \n    class = \"NewsArticle\"\n  )\n}\n\nnew_tweet <- function(username, content, reply, retweet) {\n  vctrs::new_rcrd(\n    list(\n      username = username,\n      content = content,\n      reply = reply,\n      retweet = retweet\n    ),\n    class = \"Tweet\"\n  )\n}\n\n\n\n\nUsing the constructors we can create a tweet and a news article.\n\n# https://www.theonion.com/new-absolut-ad-features-swaying-mom-with-one-eye-closed-1850138855\narticle <- new_article(\n  \"New Absolut Ad Features Swaying Mom With One Eye Closed Telling Camera She Used To Dance\",\n  \"Stockholm\",\n  \"The Onion\", \n  \"The ad concludes abruptly with the mother beginning to cry when, for no particular reason, she suddenly remembers the death of Princess Diana.\"\n)\n\n# https://twitter.com/TheOnion/status/1631104570041552896\ntweet <- new_tweet(\n  \"@TheOnion\",\n  \"Cat Internally Debates Whether Or Not To Rip Head Off Smaller Creature It Just Met https://bit.ly/3J1kNzV\",\n  FALSE,\n  FALSE\n)\n\nWe can see how notify works for both of these.\n\nnotify(tweet)\n\n[1] \"Breaking news! @TheOnion: Cat Internally Debates Whether Or Not To Rip Head Off Smaller Creature It Just Met https://bit.ly/3J1kNzV\"\n\nnotify(article)\n\n[1] \"Breaking news! New Absolut Ad Features Swaying Mom With One Eye Closed Telling Camera She Used To Dance, by The Onion (Stockholm)\"\n\n\nwhat this means\nThis is awesome. This means that any object that we create in the future, as long as it implements the Summary() function for its class we can utilize the notify() function. This comes with a caveat, though—as all good things do.\nThe Rust compiler ensures that any object that implements the Summary trait returns a single string. R is far more laissez faire than Rust with classes and types. One could create a Summary method for an object that returns a vector of strings. That would break notify. Either notify() should have type checking or you should make sure that your method always produces the correct type."
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers.html#defining-a-trait",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers.html#defining-a-trait",
    "title": "Rust traits for R users",
    "section": "Defining a Trait",
    "text": "Defining a Trait\nWe start by defining a trait called Summary which as a single method that returns a String. Note how the definition is rather abstract. We know what the function is and what it returns. What happens on the inside doesn’t matter to use.\ntrait Summary {\n    fn summarize(&self) -> String;\n}\nAnalogous in R is the definition of an S3 function generic.\n\n\nIt’s only analogous if the trait implements only one method\n\nSummary <- function(x) UseMethod(\"Summary\")\n\nThe S3 function generic is essentially saying that there is a new function called Summary and it will behave differently based on the class of object passed to it.\n\n\nNote that we can’t specify the output type so we may want to create a validator function later\nNow we want to define a struct called NewsArticle that contains 4 fields related to the news paper itself.\npub struct NewsArticle {\n    pub headline: String,\n    pub location: String,\n    pub author: String,\n    pub content: String,\n}\nThis is similar to creating a new record in vctrs which is rather similar to using base R.\n\n\nBase R\nvctrs\n\n\n\n\nstructure(\n  list(\n    headline = character(),\n    location = character(),\n    author = character(),\n    content = character()\n    ), \n  class = \"NewsArticle\"\n)\n\n$headline\ncharacter(0)\n\n$location\ncharacter(0)\n\n$author\ncharacter(0)\n\n$content\ncharacter(0)\n\nattr(,\"class\")\n[1] \"NewsArticle\"\n\n\n\n\n\nvctrs::new_rcrd(\n  list(\n    headline = character(),\n    location = character(),\n    author = character(),\n    content = character()\n    ), \n  class = \"NewsArticle\"\n)\n\n<NewsArticle[0]>"
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers/2023-03-01-rust-traits-for-r-programmers.html",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers/2023-03-01-rust-traits-for-r-programmers.html",
    "title": "Rust traits for R users",
    "section": "",
    "text": "In the few months that I’ve been programming in Rust nothing has so fundamentally shifted the way that I think about programming—and specifically R packages—as Rust traits. I want to talk briefly about Rust traits and why I think they can make R packages better.\nA trait defines a set of behaviors that can be used by objects of different kinds. Each trait is a collection of methods (functions) whose behavior is defined abstractly. The traits can then be implemented for different object types. Any object that implements the trait can then use that method. It’s kind of confusing, isn’t it? Let’s work through the example in The Book™ and how we can implement it in R."
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers/2023-03-01-rust-traits-for-r-programmers.html#defining-a-trait",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers/2023-03-01-rust-traits-for-r-programmers.html#defining-a-trait",
    "title": "Rust traits for R users",
    "section": "Defining a Trait",
    "text": "Defining a Trait\nWe start by defining a trait called Summary which as a single method that returns a String. Note how the definition is rather abstract. We know what the function is and what it returns. What happens on the inside doesn’t matter to use.\ntrait Summary {\n    fn summarize(&self) -> String;\n}\nAnalogous in R is the definition of an S3 function generic.\n\n\nIt’s only analogous if the trait implements only one method\n\nSummary <- function(x) UseMethod(\"Summary\")\n\nThe S3 function generic is essentially saying that there is a new function called Summary and it will behave differently based on the class of object passed to it.\n\n\nNote that we can’t specify the output type so we may want to create a validator function later\nNow we want to define a struct called NewsArticle that contains 4 fields related to the news paper itself.\npub struct NewsArticle {\n    pub headline: String,\n    pub location: String,\n    pub author: String,\n    pub content: String,\n}\nThis is similar to creating a new record in vctrs which is rather similar to using base R.\n\n\nBase R\nvctrs\n\n\n\n\nstructure(\n  list(\n    headline = character(),\n    location = character(),\n    author = character(),\n    content = character()\n    ), \n  class = \"NewsArticle\"\n)\n\n$headline\ncharacter(0)\n\n$location\ncharacter(0)\n\n$author\ncharacter(0)\n\n$content\ncharacter(0)\n\nattr(,\"class\")\n[1] \"NewsArticle\"\n\n\n\n\n\nvctrs::new_rcrd(\n  list(\n    headline = character(),\n    location = character(),\n    author = character(),\n    content = character()\n    ), \n  class = \"NewsArticle\"\n)\n\n<NewsArticle[0]>"
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers/2023-03-01-rust-traits-for-r-programmers.html#implementing-a-trait",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers/2023-03-01-rust-traits-for-r-programmers.html#implementing-a-trait",
    "title": "Rust traits for R users",
    "section": "\nimplementing a trait",
    "text": "implementing a trait\nIt’s important that we are able to summarise our newspaper article for the socials ofc—so we need to implement the Summary trait for the NewsArticle struct.\nimpl Summary for NewsArticle {\n    fn summarize(&self) -> String {\n        format!(\"{}, by {} ({})\", self.headline, self.author, self.location)\n    }\n}\n\n\nNotice that the trait is called Summary whereas the method it provides is called summarize(). For the sake of example I’m going to call the R function Summary() throughout the rest of the example. It’s not possible to have a perfect 1:1 relationship between Rust and R ;)\nThis block defines how the summarize() method will work for a NewsArticle struct. It will create a string in the format of \"{title}, by {author} ({location})\". In R, we have to define the NewsArticle method for the Summary function which is done by creating a function object with the name signature {Generic}.{class} <- function(...) { . . . }.\n\n\nBase R\nvctrs\n\n\n\n\nSummary.NewsArticle <- function(x) {\n  sprintf(\n    \"%s, by %s (%s)\",\n    x[[\"headline\"]],\n    x[[\"author\"]],\n    x[[\"location\"]]\n  )\n}\n\n\n\n\nSummary.NewsArticle <- function(x) {\n  sprintf(\n    \"%s, by %s (%s)\",\n    vctrs::field(x, \"headline\"),\n    vctrs::field(x, \"author\"),\n    vctrs::field(x, \"location\")\n  )\n}\n\n\n\n\nSince Musk’s takeover of twitter, tweets are getting out of hand becoming ridiculously long so we need to be able to summarize them too! So if we define a Tweet struct and a corresponding implementation of Summary we’ll be able to easily summarize them exactly the same way as news articles.\n// define the struct\npub struct Tweet {\n    pub username: String,\n    pub content: String,\n    pub reply: bool,\n    pub retweet: bool,\n}\n\n// implement the trait\nimpl Summary for Tweet {\n    fn summarize(&self) -> String {\n        format!(\"{}: {}\", self.username, self.content)\n    }\n}\nCorrespondingly in R, we’re going to be working with both Tweets and News Articles. So we need to define a tweet class to contain our tweets and a Summary() method for the new class.\n\n\nBase R\nvctrs\n\n\n\n\nstructure(\n  list(\n    username = character(),\n    content = character(),\n    reply = logical(),\n    retweet = logical()\n  ),\n  class = \"Tweet\"\n)\n\n$username\ncharacter(0)\n\n$content\ncharacter(0)\n\n$reply\nlogical(0)\n\n$retweet\nlogical(0)\n\nattr(,\"class\")\n[1] \"Tweet\"\n\nSummary.Tweet <- function(x) {\n  sprintf(\"%s: %s\", x[[\"username\"]], x[[\"content\"]])\n}\n\n\n\n\nvctrs::new_rcrd(\n    list(\n    username = character(),\n    content = character(),\n    reply = character(),\n    retweet = logical()\n  ),\n  class = \"Tweet\"\n)\n\n<Tweet[0]>\n\nSummary.Tweet <- function(x) {\n  sprintf(\n    \"%s: %s\", \n    vctrs::field(x, \"username\"), \n    vctrs::field(x, \"content\")\n    )\n}\n\n\n\n\nWe can now define a function that utilizes this trait that will produce consistent String output in the same format for both tweets and new articles.\npub fn notify(item: &impl Summary) {\n    println!(\"Breaking news! {}\", item.summarize());\n}\nThis is huge from the R perspective because we can create a notify() function that calls Summary() and as long as a method if defined for the input class it will work!\n\nnotify <- function(item) {\n  sprintf(\"Breaking news! %s\", Summary(item))\n}\n\nTo test this out lets create a Tweet and a NewsArticle. First we’ll create constructor functions for each.\n\n\nBase R\nvctrs\n\n\n\n\nnew_article <- function(headline, location, author, content) {\n  structure(\n    list(\n      headline = headline,\n      location = location,\n      author = author,\n      content = content\n      ), \n    class = \"NewsArticle\"\n  )\n}\n\nnew_tweet <- function(username, content, reply, retweet) {\n  structure(\n    list(\n      username = username,\n      content = content,\n      reply = reply,\n      retweet = retweet\n    ),\n    class = \"Tweet\"\n  )\n}\n\n\n\n\nnew_article <- function(headline, location, author, content) {\n  vctrs::new_rcrd(\n    list(\n      headline = headline,\n      location = location,\n      author = author,\n      content = content\n      ), \n    class = \"NewsArticle\"\n  )\n}\n\nnew_tweet <- function(username, content, reply, retweet) {\n  vctrs::new_rcrd(\n    list(\n      username = username,\n      content = content,\n      reply = reply,\n      retweet = retweet\n    ),\n    class = \"Tweet\"\n  )\n}\n\n\n\n\nUsing the constructors we can create a tweet and a news article.\n\n# https://www.theonion.com/new-absolut-ad-features-swaying-mom-with-one-eye-closed-1850138855\narticle <- new_article(\n  \"New Absolut Ad Features Swaying Mom With One Eye Closed Telling Camera She Used To Dance\",\n  \"Stockholm\",\n  \"The Onion\", \n  \"The ad concludes abruptly with the mother beginning to cry when, for no particular reason, she suddenly remembers the death of Princess Diana.\"\n)\n\n# https://twitter.com/TheOnion/status/1631104570041552896\ntweet <- new_tweet(\n  \"@TheOnion\",\n  \"Cat Internally Debates Whether Or Not To Rip Head Off Smaller Creature It Just Met https://bit.ly/3J1kNzV\",\n  FALSE,\n  FALSE\n)\n\nWe can see how notify works for both of these.\n\nnotify(tweet)\n\n[1] \"Breaking news! @TheOnion: Cat Internally Debates Whether Or Not To Rip Head Off Smaller Creature It Just Met https://bit.ly/3J1kNzV\"\n\nnotify(article)\n\n[1] \"Breaking news! New Absolut Ad Features Swaying Mom With One Eye Closed Telling Camera She Used To Dance, by The Onion (Stockholm)\"\n\n\nwhat this means\nThis is awesome. This means that any object that we create in the future, as long as it implements the Summary() function for its class we can utilize the notify() function. This comes with a caveat, though—as all good things do.\nThe Rust compiler ensures that any object that implements the Summary trait returns a single string. R is far more laissez faire than Rust with classes and types. One could create a Summary method for an object that returns a vector of strings. That would break notify. Either notify() should have type checking or you should make sure that your method always produces the correct type."
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers/2023-03-01-rust-traits-for-r-programmers.html#implications-for-r-packages",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers/2023-03-01-rust-traits-for-r-programmers.html#implications-for-r-packages",
    "title": "Rust traits for R users",
    "section": "Implications for R packages",
    "text": "Implications for R packages\nThis very simple concept can be transformative for the way that we build R packages. R packages are, for the most part, bespoke. Each one serves their own purpose and works only within its own types or types it’s aware of. But what if an R package could work with any object type? Using this idea we can get from Rust traits, we can do that.\nPackages that want to be extensible can make it easy to do so by doing two fairly simple things. Low-level and critical functions should be exported as generic functions. High level functions that perform some useful functionality should be built upon those generics.\nAn example is the sdf package I’ve prototyped based on this idea. In this case, I have a spatial data frame class that can be implemented on any object that implements methods for the following functions:\n\nis_geometry()\nbounding_box()\ncombine_geometry()\n\nAn example\nThe sdf class is a tibble with a geometry column and a bounding box attribute. The function as_sdf() creates an sdf object that tells us what type of geometry is used and the bounding box of it.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.1; sf_use_s2() is TRUE\n\nlibrary(sdf)\n\n\nAttaching package: 'sdf'\n\n\nThe following object is masked from 'package:sf':\n\n    is_geometry\n\n# get some sample data \ng <- sfdep::guerry[, \"region\"]\n\nas_sdf(g)\n\nGeometry Type: sfc_MULTIPOLYGON\nBounding box: xmin: 47680 ymin: 1703258 xmax: 1031401 ymax: 2677441\n# A tibble: 85 × 2\n   region                                                               geometry\n   <fct>                                                          <MULTIPOLYGON>\n 1 E      (((801150 2092615, 800669 2093190, 800688 2095430, 800780 2095795, 80…\n 2 N      (((729326 2521619, 729320 2521230, 729280 2518544, 728751 2517520, 72…\n 3 C      (((710830 2137350, 711746 2136617, 712430 2135212, 712070 2134132, 71…\n 4 E      (((882701 1920024, 882408 1920733, 881778 1921200, 881526 1922332, 87…\n 5 E      (((886504 1922890, 885733 1922978, 885479 1923276, 883061 1925266, 88…\n 6 S      (((747008 1925789, 746630 1925762, 745723 1925138, 744216 1925236, 74…\n 7 N      (((818893 2514767, 818614 2514515, 817900 2514467, 817327 2514945, 81…\n 8 S      (((509103 1747787, 508820 1747513, 508154 1747093, 505861 1746627, 50…\n 9 E      (((775400 2345600, 775068 2345397, 773587 2345177, 772940 2344780, 77…\n10 S      (((626230 1810121, 626269 1810496, 627494 1811321, 627681 1812424, 62…\n# … with 75 more rows\n\n\nThis is super cool because we can group by and summarize the data just because we have those above functions defined for sfc objects (the geometry column).\n\nlibrary(dplyr)\nas_sdf(g) |> \n  group_by(region) |> \n  summarise(n = n())\n\nGeometry Type: sfc_MULTIPOLYGON\nBounding box: xmin: 47680 ymin: 1703258 xmax: 1031401 ymax: 2677441\n# A tibble: 5 × 3\n  region     n                                                          geometry\n  <fct>  <int>                                                    <MULTIPOLYGON>\n1 C         17 (((710830 2137350, 711746 2136617, 712430 2135212, 712070 213413…\n2 E         17 (((801150 2092615, 800669 2093190, 800688 2095430, 800780 209579…\n3 N         17 (((729326 2521619, 729320 2521230, 729280 2518544, 728751 251752…\n4 S         17 (((747008 1925789, 746630 1925762, 745723 1925138, 744216 192523…\n5 W         17 (((456425 2120055, 456229 2120382, 455943 2121064, 456070 212219…\n\n\nSay we want to create a custom Point class that we want to be usable by an sdf object. We can do this rather simply by creating the proper generics. A Point will be a list of length 2 numeric vectors where the first element is the x coordinate and the second element is the y coordinate.\n\n# create some points\npnt_data <- lapply(1:5, \\(x) runif(2, 0, 90))\n\n# create new vector class\npnts <- vctrs::new_vctr(pnt_data, class = \"Point\")\n\npnts\n\n<Point[5]>\n[1] 9.464963, 11.015805 37.10408, 51.10796  10.71916, 32.67404 \n[4] 79.78800, 66.92182  5.658064, 84.300552\n\n\nNow we can start defining our methods. is_geometry() should always return TRUE for our type. We can do this like so:\n\nis_geometry.Point <- function(x) inherits(x, \"Point\")\n\n\n\nThis method will only be dispatched on Points so it will always inherit the Point class. One could just as well always return TRUE\nNext we need to define a method for the bounding box. This is the the maximum and minimum x and y coordinates. Our method should iterate over each point and extract the x and y into their own vector and return the minimum and maxes. These need to be structured in a particular order.\n\nbounding_box.Point <- function(.x) {\n  x <- vapply(.x, `[`, numeric(1), 1)\n  y <- vapply(.x, `[`, numeric(1), 2)\n  \n  c(xmin = min(x), ymin = min(y), xmax = max(x), ymax = max(y))\n}\n\nLastly, we need a way to combine the points together. In this case, we can just “combine” the points by finding the average point. This is not geometrically sound but for the sake of example it suffices. Note that the type it returns always has to be the same! There is not a compiler forcing us, so we must force ourselves!\n\ncombine_geometry.Point <- function(.x) {\n  x <- vapply(.x, `[`, numeric(1), 1)\n  y <- vapply(.x, `[`, numeric(1), 2)\n  \n  vctrs::new_vctr(\n    list(c(mean(x), mean(y))), \n    class = \"Point\"\n    )\n}\n\nWith only those 3 functions we’ve defined enough to create an sdf object where the geometry column is a Point vector. To illustrate this we can use the ggplot2 diamonds data set for example since it has nice x and y coordinates.\nFirst we create a data frame with a Point column.\n\ndata(diamonds, package = \"ggplot2\")\n\ndiamond_pnts <- diamonds |> \n  mutate(\n    pnts = vctrs::new_vctr(\n      purrr::map2(x, y, `c`),\n      class = \"Point\"\n    )\n  ) |> \n  select(cut, pnts)\n\nhead(diamond_pnts)\n\n# A tibble: 6 × 2\n  cut             pnts\n  <ord>        <Point>\n1 Ideal     3.95, 3.98\n2 Premium   3.89, 3.84\n3 Good      4.05, 4.07\n4 Premium   4.20, 4.23\n5 Good      4.34, 4.35\n6 Very Good 3.94, 3.96\n\n\nNext we cast it to an sdf object by using as_sdf().\n\ndiamond_sdf <- as_sdf(diamond_pnts) \ndiamond_sdf\n\nGeometry Type: Point\nBounding box: xmin: 0 ymin: 0 xmax: 10.74 ymax: 58.9\n# A tibble: 53,940 × 2\n   cut             pnts\n   <ord>        <Point>\n 1 Ideal     3.95, 3.98\n 2 Premium   3.89, 3.84\n 3 Good      4.05, 4.07\n 4 Premium   4.20, 4.23\n 5 Good      4.34, 4.35\n 6 Very Good 3.94, 3.96\n 7 Very Good 3.95, 3.98\n 8 Very Good 4.07, 4.11\n 9 Fair      3.87, 3.78\n10 Very Good 4.00, 4.05\n# … with 53,930 more rows\n\n\nNotice that the printing method shows Geometry Type: Point and also has a Bounding box:. That means we have effectively extended the sdf class by implementing our own methods for the exported generic functions from sdf. From that alone the sdf methods for dplyr can be used.\n\ndiamond_sdf |> \n  group_by(cut) |> \n  summarise(n = n())\n\nGeometry Type: Point\nBounding box: xmin: 5.507 ymin: 5.52 xmax: 6.247 ymax: 6.183\n# A tibble: 5 × 3\n  cut           n               pnts\n  <ord>     <int>            <Point>\n1 Fair       1610 6.246894, 6.182652\n2 Good       4906 5.838785, 5.850744\n3 Very Good 12082 5.740696, 5.770026\n4 Premium   13791 5.973887, 5.944879\n5 Ideal     21551 5.507451, 5.520080\n\n\nWhy this works\nThe dplyr sdf methods work like a charm because they use generic functions. Take the summarise() method for example.\n\nsdf:::summarise.sdf\n\nfunction(.data, ...) {\n  geom_col_name <- attr(.data, \"geom_column\")\n  geom_col <- .data[[geom_col_name]]\n  gd <- group_data(.data)\n\n  summarized_geoms <- lapply(gd$.rows, function(ids) combine_geometry(geom_col[ids]))\n\n  res <- NextMethod()\n\n  res[[geom_col_name]] <- rlang::inject(c(!!!summarized_geoms))\n  as_sdf(res)\n\n}\n<bytecode: 0x105eba2c8>\n<environment: namespace:sdf>\n\n\nThis method uses the combine_geometry() generic function. combine_geometry() takes a vector of geometries (as determined by is_geometry()) and returns a single element. The summarise method does not care which method is used. It only cares that the output is consistent—in this case that a scalar value is outputted and that multiple of those scalars can be combined using c().\nAnother example\nFor a more detailed example check out the section in the README that implements an sdf class for geos geometry. If you’re interested in the details I recommend looking at the source code it is very simple.\nFirst look at the generic method definitions. Then look at the sf compatibility methods."
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers/index.html",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers/index.html",
    "title": "Rust traits for R users",
    "section": "",
    "text": "In the few months that I’ve been programming in Rust nothing has so fundamentally shifted the way that I think about programming—and specifically R packages—as Rust traits. I want to talk briefly about Rust traits and why I think they can make R packages better.\nA trait defines a set of behaviors that can be used by objects of different kinds. Each trait is a collection of methods (functions) whose behavior is defined abstractly. The traits can then be implemented for different object types. Any object that implements the trait can then use that method. It’s kind of confusing, isn’t it? Let’s work through the example in The Book™ and how we can implement it in R."
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers/index.html#defining-a-trait",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers/index.html#defining-a-trait",
    "title": "Rust traits for R users",
    "section": "Defining a Trait",
    "text": "Defining a Trait\nWe start by defining a trait called Summary which as a single method that returns a String. Note how the definition is rather abstract. We know what the function is and what it returns. What happens on the inside doesn’t matter to use.\ntrait Summary {\n    fn summarize(&self) -> String;\n}\nAnalogous in R is the definition of an S3 function generic.\n\n\nIt’s only analogous if the trait implements only one method\n\nSummary <- function(x) UseMethod(\"Summary\")\n\nThe S3 function generic is essentially saying that there is a new function called Summary and it will behave differently based on the class of object passed to it.\n\n\nNote that we can’t specify the output type so we may want to create a validator function later\nNow we want to define a struct called NewsArticle that contains 4 fields related to the news paper itself.\npub struct NewsArticle {\n    pub headline: String,\n    pub location: String,\n    pub author: String,\n    pub content: String,\n}\nThis is similar to creating a new record in vctrs which is rather similar to using base R.\n\n\nBase R\nvctrs\n\n\n\n\nstructure(\n  list(\n    headline = character(),\n    location = character(),\n    author = character(),\n    content = character()\n    ), \n  class = \"NewsArticle\"\n)\n\n$headline\ncharacter(0)\n\n$location\ncharacter(0)\n\n$author\ncharacter(0)\n\n$content\ncharacter(0)\n\nattr(,\"class\")\n[1] \"NewsArticle\"\n\n\n\n\n\nvctrs::new_rcrd(\n  list(\n    headline = character(),\n    location = character(),\n    author = character(),\n    content = character()\n    ), \n  class = \"NewsArticle\"\n)\n\n<NewsArticle[0]>"
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers/index.html#implementing-a-trait",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers/index.html#implementing-a-trait",
    "title": "Rust traits for R users",
    "section": "\nimplementing a trait",
    "text": "implementing a trait\nIt’s important that we are able to summarise our newspaper article for the socials ofc—so we need to implement the Summary trait for the NewsArticle struct.\nimpl Summary for NewsArticle {\n    fn summarize(&self) -> String {\n        format!(\"{}, by {} ({})\", self.headline, self.author, self.location)\n    }\n}\n\n\nNotice that the trait is called Summary whereas the method it provides is called summarize(). For the sake of example I’m going to call the R function Summary() throughout the rest of the example. It’s not possible to have a perfect 1:1 relationship between Rust and R ;)\nThis block defines how the summarize() method will work for a NewsArticle struct. It will create a string in the format of \"{title}, by {author} ({location})\". In R, we have to define the NewsArticle method for the Summary function which is done by creating a function object with the name signature {Generic}.{class} <- function(...) { . . . }.\n\n\nBase R\nvctrs\n\n\n\n\nSummary.NewsArticle <- function(x) {\n  sprintf(\n    \"%s, by %s (%s)\",\n    x[[\"headline\"]],\n    x[[\"author\"]],\n    x[[\"location\"]]\n  )\n}\n\n\n\n\nSummary.NewsArticle <- function(x) {\n  sprintf(\n    \"%s, by %s (%s)\",\n    vctrs::field(x, \"headline\"),\n    vctrs::field(x, \"author\"),\n    vctrs::field(x, \"location\")\n  )\n}\n\n\n\n\nSince Musk’s takeover of twitter, tweets are getting out of hand becoming ridiculously long so we need to be able to summarize them too! So if we define a Tweet struct and a corresponding implementation of Summary we’ll be able to easily summarize them exactly the same way as news articles.\n// define the struct\npub struct Tweet {\n    pub username: String,\n    pub content: String,\n    pub reply: bool,\n    pub retweet: bool,\n}\n\n// implement the trait\nimpl Summary for Tweet {\n    fn summarize(&self) -> String {\n        format!(\"{}: {}\", self.username, self.content)\n    }\n}\nCorrespondingly in R, we’re going to be working with both Tweets and News Articles. So we need to define a tweet class to contain our tweets and a Summary() method for the new class.\n\n\nBase R\nvctrs\n\n\n\n\nstructure(\n  list(\n    username = character(),\n    content = character(),\n    reply = logical(),\n    retweet = logical()\n  ),\n  class = \"Tweet\"\n)\n\n$username\ncharacter(0)\n\n$content\ncharacter(0)\n\n$reply\nlogical(0)\n\n$retweet\nlogical(0)\n\nattr(,\"class\")\n[1] \"Tweet\"\n\nSummary.Tweet <- function(x) {\n  sprintf(\"%s: %s\", x[[\"username\"]], x[[\"content\"]])\n}\n\n\n\n\nvctrs::new_rcrd(\n    list(\n    username = character(),\n    content = character(),\n    reply = character(),\n    retweet = logical()\n  ),\n  class = \"Tweet\"\n)\n\n<Tweet[0]>\n\nSummary.Tweet <- function(x) {\n  sprintf(\n    \"%s: %s\", \n    vctrs::field(x, \"username\"), \n    vctrs::field(x, \"content\")\n    )\n}\n\n\n\n\nWe can now define a function that utilizes this trait that will produce consistent String output in the same format for both tweets and new articles.\npub fn notify(item: &impl Summary) {\n    println!(\"Breaking news! {}\", item.summarize());\n}\nThis is huge from the R perspective because we can create a notify() function that calls Summary() and as long as a method if defined for the input class it will work!\n\nnotify <- function(item) {\n  sprintf(\"Breaking news! %s\", Summary(item))\n}\n\nTo test this out lets create a Tweet and a NewsArticle. First we’ll create constructor functions for each.\n\n\nBase R\nvctrs\n\n\n\n\nnew_article <- function(headline, location, author, content) {\n  structure(\n    list(\n      headline = headline,\n      location = location,\n      author = author,\n      content = content\n      ), \n    class = \"NewsArticle\"\n  )\n}\n\nnew_tweet <- function(username, content, reply, retweet) {\n  structure(\n    list(\n      username = username,\n      content = content,\n      reply = reply,\n      retweet = retweet\n    ),\n    class = \"Tweet\"\n  )\n}\n\n\n\n\nnew_article <- function(headline, location, author, content) {\n  vctrs::new_rcrd(\n    list(\n      headline = headline,\n      location = location,\n      author = author,\n      content = content\n      ), \n    class = \"NewsArticle\"\n  )\n}\n\nnew_tweet <- function(username, content, reply, retweet) {\n  vctrs::new_rcrd(\n    list(\n      username = username,\n      content = content,\n      reply = reply,\n      retweet = retweet\n    ),\n    class = \"Tweet\"\n  )\n}\n\n\n\n\nUsing the constructors we can create a tweet and a news article.\n\n# https://www.theonion.com/new-absolut-ad-features-swaying-mom-with-one-eye-closed-1850138855\narticle <- new_article(\n  \"New Absolut Ad Features Swaying Mom With One Eye Closed Telling Camera She Used To Dance\",\n  \"Stockholm\",\n  \"The Onion\", \n  \"The ad concludes abruptly with the mother beginning to cry when, for no particular reason, she suddenly remembers the death of Princess Diana.\"\n)\n\n# https://twitter.com/TheOnion/status/1631104570041552896\ntweet <- new_tweet(\n  \"@TheOnion\",\n  \"Cat Internally Debates Whether Or Not To Rip Head Off Smaller Creature It Just Met https://bit.ly/3J1kNzV\",\n  FALSE,\n  FALSE\n)\n\nWe can see how notify works for both of these.\n\nnotify(tweet)\n\n[1] \"Breaking news! @TheOnion: Cat Internally Debates Whether Or Not To Rip Head Off Smaller Creature It Just Met https://bit.ly/3J1kNzV\"\n\nnotify(article)\n\n[1] \"Breaking news! New Absolut Ad Features Swaying Mom With One Eye Closed Telling Camera She Used To Dance, by The Onion (Stockholm)\"\n\n\nwhat this means\nThis is awesome. This means that any object that we create in the future, as long as it implements the Summary() function for its class we can utilize the notify() function. This comes with a caveat, though—as all good things do.\nThe Rust compiler ensures that any object that implements the Summary trait returns a single string. R is far more laissez faire than Rust with classes and types. One could create a Summary method for an object that returns a vector of strings. That would break notify. Either notify() should have type checking or you should make sure that your method always produces the correct type."
  },
  {
    "objectID": "posts/2023-03-01-rust-traits-for-r-programmers/index.html#implications-for-r-packages",
    "href": "posts/2023-03-01-rust-traits-for-r-programmers/index.html#implications-for-r-packages",
    "title": "Rust traits for R users",
    "section": "Implications for R packages",
    "text": "Implications for R packages\nThis very simple concept can be transformative for the way that we build R packages. R packages are, for the most part, bespoke. Each one serves their own purpose and works only within its own types or types it’s aware of. But what if an R package could work with any object type? Using this idea we can get from Rust traits, we can do that.\nPackages that want to be extensible can make it easy to do so by doing two fairly simple things. Low-level and critical functions should be exported as generic functions. High level functions that perform some useful functionality should be built upon those generics.\nAn example is the sdf package I’ve prototyped based on this idea. In this case, I have a spatial data frame class that can be implemented on any object that implements methods for the following functions:\n\nis_geometry()\nbounding_box()\ncombine_geometry()\n\nAn example\nThe sdf class is a tibble with a geometry column and a bounding box attribute. The function as_sdf() creates an sdf object that tells us what type of geometry is used and the bounding box of it.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.1; sf_use_s2() is TRUE\n\nlibrary(sdf)\n\n\nAttaching package: 'sdf'\n\n\nThe following object is masked from 'package:sf':\n\n    is_geometry\n\n# get some sample data \ng <- sfdep::guerry[, \"region\"]\n\nas_sdf(g)\n\nGeometry Type: sfc_MULTIPOLYGON\nBounding box: xmin: 47680 ymin: 1703258 xmax: 1031401 ymax: 2677441\n# A tibble: 85 × 2\n   region                                                               geometry\n   <fct>                                                          <MULTIPOLYGON>\n 1 E      (((801150 2092615, 800669 2093190, 800688 2095430, 800780 2095795, 80…\n 2 N      (((729326 2521619, 729320 2521230, 729280 2518544, 728751 2517520, 72…\n 3 C      (((710830 2137350, 711746 2136617, 712430 2135212, 712070 2134132, 71…\n 4 E      (((882701 1920024, 882408 1920733, 881778 1921200, 881526 1922332, 87…\n 5 E      (((886504 1922890, 885733 1922978, 885479 1923276, 883061 1925266, 88…\n 6 S      (((747008 1925789, 746630 1925762, 745723 1925138, 744216 1925236, 74…\n 7 N      (((818893 2514767, 818614 2514515, 817900 2514467, 817327 2514945, 81…\n 8 S      (((509103 1747787, 508820 1747513, 508154 1747093, 505861 1746627, 50…\n 9 E      (((775400 2345600, 775068 2345397, 773587 2345177, 772940 2344780, 77…\n10 S      (((626230 1810121, 626269 1810496, 627494 1811321, 627681 1812424, 62…\n# … with 75 more rows\n\n\nThis is super cool because we can group by and summarize the data just because we have those above functions defined for sfc objects (the geometry column).\n\nlibrary(dplyr)\nas_sdf(g) |> \n  group_by(region) |> \n  summarise(n = n())\n\nGeometry Type: sfc_MULTIPOLYGON\nBounding box: xmin: 47680 ymin: 1703258 xmax: 1031401 ymax: 2677441\n# A tibble: 5 × 3\n  region     n                                                          geometry\n  <fct>  <int>                                                    <MULTIPOLYGON>\n1 C         17 (((710830 2137350, 711746 2136617, 712430 2135212, 712070 213413…\n2 E         17 (((801150 2092615, 800669 2093190, 800688 2095430, 800780 209579…\n3 N         17 (((729326 2521619, 729320 2521230, 729280 2518544, 728751 251752…\n4 S         17 (((747008 1925789, 746630 1925762, 745723 1925138, 744216 192523…\n5 W         17 (((456425 2120055, 456229 2120382, 455943 2121064, 456070 212219…\n\n\nSay we want to create a custom Point class that we want to be usable by an sdf object. We can do this rather simply by creating the proper generics. A Point will be a list of length 2 numeric vectors where the first element is the x coordinate and the second element is the y coordinate.\n\n# create some points\npnt_data <- lapply(1:5, \\(x) runif(2, 0, 90))\n\n# create new vector class\npnts <- vctrs::new_vctr(pnt_data, class = \"Point\")\n\npnts\n\n<Point[5]>\n[1] 53.59747, 75.12560  25.34989, 51.07764  8.542277, 19.890635\n[4] 52.60644, 68.45127  16.40016, 75.16829 \n\n\nNow we can start defining our methods. is_geometry() should always return TRUE for our type. We can do this like so:\n\nis_geometry.Point <- function(x) inherits(x, \"Point\")\n\n\n\nThis method will only be dispatched on Points so it will always inherit the Point class. One could just as well always return TRUE\nNext we need to define a method for the bounding box. This is the the maximum and minimum x and y coordinates. Our method should iterate over each point and extract the x and y into their own vector and return the minimum and maxes. These need to be structured in a particular order.\n\nbounding_box.Point <- function(.x) {\n  x <- vapply(.x, `[`, numeric(1), 1)\n  y <- vapply(.x, `[`, numeric(1), 2)\n  \n  c(xmin = min(x), ymin = min(y), xmax = max(x), ymax = max(y))\n}\n\nLastly, we need a way to combine the points together. In this case, we can just “combine” the points by finding the average point. This is not geometrically sound but for the sake of example it suffices. Note that the type it returns always has to be the same! There is not a compiler forcing us, so we must force ourselves!\n\ncombine_geometry.Point <- function(.x) {\n  x <- vapply(.x, `[`, numeric(1), 1)\n  y <- vapply(.x, `[`, numeric(1), 2)\n  \n  vctrs::new_vctr(\n    list(c(mean(x), mean(y))), \n    class = \"Point\"\n    )\n}\n\nWith only those 3 functions we’ve defined enough to create an sdf object where the geometry column is a Point vector. To illustrate this we can use the ggplot2 diamonds data set for example since it has nice x and y coordinates.\nFirst we create a data frame with a Point column.\n\ndata(diamonds, package = \"ggplot2\")\n\ndiamond_pnts <- diamonds |> \n  mutate(\n    pnts = vctrs::new_vctr(\n      purrr::map2(x, y, `c`),\n      class = \"Point\"\n    )\n  ) |> \n  select(cut, pnts)\n\nhead(diamond_pnts)\n\n# A tibble: 6 × 2\n  cut             pnts\n  <ord>        <Point>\n1 Ideal     3.95, 3.98\n2 Premium   3.89, 3.84\n3 Good      4.05, 4.07\n4 Premium   4.20, 4.23\n5 Good      4.34, 4.35\n6 Very Good 3.94, 3.96\n\n\nNext we cast it to an sdf object by using as_sdf().\n\ndiamond_sdf <- as_sdf(diamond_pnts) \ndiamond_sdf\n\nGeometry Type: Point\nBounding box: xmin: 0 ymin: 0 xmax: 10.74 ymax: 58.9\n# A tibble: 53,940 × 2\n   cut             pnts\n   <ord>        <Point>\n 1 Ideal     3.95, 3.98\n 2 Premium   3.89, 3.84\n 3 Good      4.05, 4.07\n 4 Premium   4.20, 4.23\n 5 Good      4.34, 4.35\n 6 Very Good 3.94, 3.96\n 7 Very Good 3.95, 3.98\n 8 Very Good 4.07, 4.11\n 9 Fair      3.87, 3.78\n10 Very Good 4.00, 4.05\n# … with 53,930 more rows\n\n\nNotice that the printing method shows Geometry Type: Point and also has a Bounding box:. That means we have effectively extended the sdf class by implementing our own methods for the exported generic functions from sdf. From that alone the sdf methods for dplyr can be used.\n\ndiamond_sdf |> \n  group_by(cut) |> \n  summarise(n = n())\n\nGeometry Type: Point\nBounding box: xmin: 5.507 ymin: 5.52 xmax: 6.247 ymax: 6.183\n# A tibble: 5 × 3\n  cut           n               pnts\n  <ord>     <int>            <Point>\n1 Fair       1610 6.246894, 6.182652\n2 Good       4906 5.838785, 5.850744\n3 Very Good 12082 5.740696, 5.770026\n4 Premium   13791 5.973887, 5.944879\n5 Ideal     21551 5.507451, 5.520080\n\n\nWhy this works\nThe dplyr sdf methods work like a charm because they use generic functions. Take the summarise() method for example.\n\nsdf:::summarise.sdf\n\nfunction(.data, ...) {\n  geom_col_name <- attr(.data, \"geom_column\")\n  geom_col <- .data[[geom_col_name]]\n  gd <- group_data(.data)\n\n  summarized_geoms <- lapply(gd$.rows, function(ids) combine_geometry(geom_col[ids]))\n\n  res <- NextMethod()\n\n  res[[geom_col_name]] <- rlang::inject(c(!!!summarized_geoms))\n  as_sdf(res)\n\n}\n<bytecode: 0x1066d0b00>\n<environment: namespace:sdf>\n\n\nThis method uses the combine_geometry() generic function. combine_geometry() takes a vector of geometries (as determined by is_geometry()) and returns a single element. The summarise method does not care which method is used. It only cares that the output is consistent—in this case that a scalar value is outputted and that multiple of those scalars can be combined using c().\nAnother example\nFor a more detailed example check out the section in the README that implements an sdf class for geos geometry. If you’re interested in the details I recommend looking at the source code it is very simple.\nFirst look at the generic method definitions. Then look at the sf compatibility methods."
  },
  {
    "objectID": "projects/pkgs/h3o.html",
    "href": "projects/pkgs/h3o.html",
    "title": "h3o for H3 indexing",
    "section": "",
    "text": "{h3o} is an R package that offers high-performance geospatial indexing using the H3 grid system. The package is built using {extendr} and provides bindings to the Rust library of the same name.\nThe Rust community built h3o which is a pure rust implementation of Uber’s H3 hierarchical hexagon grid system. Since h3o is a pure rust library it is typically safer to use, just as fast, and dependency free."
  },
  {
    "objectID": "projects/pkgs/h3o.html#benefits-of-h3o",
    "href": "projects/pkgs/h3o.html#benefits-of-h3o",
    "title": "h3o for H3 indexing",
    "section": "Benefits of h3o",
    "text": "Benefits of h3o\nSince h3o is built purely in Rust and R it is system dependency free and can be compiled for multiple platforms including Linux, MacOS, and Windows, making it easy to use across different OS.\nh3o benefits greatly from the type safety of Rust and provides robust error handling often returning 0 length vectors or NA values when appropriate where errors would typically occur using another H3 library.\nAnd moreover, it is very fast!"
  },
  {
    "objectID": "projects/pkgs/h3o.html#features",
    "href": "projects/pkgs/h3o.html#features",
    "title": "h3o for H3 indexing",
    "section": "Features",
    "text": "Features\nh3o supports all of the functionality that is provided by the C library and the Rust library h3o.\n\n\nIf there are any features missing, please make an issue on GitHub and I’ll be sure to address it!\nh3o was built with sf objects and the tidyverse in mind. h3o objects can be created from sf objects and vice versa. Compatibility with the tidyverse is accomplished via the vctrs package.\n\n\nsf::st_as_sfc() methods for H3 and H3Edge vectors\nautomatic nesting by creating lists of H3 and H3Edge vectors\n\nvectorized output will never return more objects than inputs\n\n\n\nExample\nCreate some points in the bounding box of Wyoming.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.1; sf_use_s2() is TRUE\n\nlibrary(h3o)\n\n# create a bounding box\nbbox_raw <- setNames(\n  c(-111.056888, 40.994746, -104.05216, 45.005904),\n  c(\"xmin\", \"ymin\", \"xmax\", \"ymax\")\n)\n\n# create some points\npnts <- st_bbox(bbox_raw) |> \n  st_as_sfc() |> \n  st_set_crs(4326) |> \n  st_sample(25)\n\n# convert to H3 index\nhexs <- h3_from_points(pnts, 4) \nhexs\n\n<H3[25]>\n [1] 8426b45ffffffff 8426a05ffffffff 8426a5dffffffff 8426b4dffffffff\n [5] 8426b15ffffffff 842896dffffffff 8426b43ffffffff 8426a05ffffffff\n [9] 84279bbffffffff 8426843ffffffff 8426a15ffffffff 8426ae5ffffffff\n[13] 8426a63ffffffff 8426a3bffffffff 84278b5ffffffff 8426b11ffffffff\n[17] 8426a4bffffffff 842685bffffffff 8426b19ffffffff 8426a35ffffffff\n[21] 8426b15ffffffff 8426845ffffffff 8426b69ffffffff 84278b1ffffffff\n[25] 84278b5ffffffff\n\n\nThe H3 vectors can be easily visualized by converting to sf objects. The st_as_sfc() method is defined for H3 vectors. While you may be familair with st_as_sf() the _sfc variant is used for creating columns and should be used on a vector not a dataframe. This way you can use it in a dplyr pipe.\n\npolys <- st_as_sfc(hexs)\npolys\n\nGeometry set for 25 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -110.8472 ymin: 40.83137 xmax: -103.6023 ymax: 44.86262\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOLYGON ((-109.9082 43.59055, -109.6438 43.7344...\n\n\nPOLYGON ((-105.9715 42.89294, -105.7036 43.0287...\n\n\nPOLYGON ((-104.3536 43.6947, -104.0817 43.82518...\n\n\nPOLYGON ((-109.6918 43.96129, -109.426 44.10402...\n\n\nPOLYGON ((-108.7011 41.9535, -108.4393 42.09726...\n\n\nThis can be plotted.\n\nplot(polys)\n\n\n\n\nTo illustrate tidyverse compatibility lets create an sf object and create a column of H3 indexes.\n\nlibrary(dplyr, warn.conflicts = FALSE)\n\nhexs <- tibble(geometry = pnts) |> \n  st_as_sf() |> \n  mutate(h3 = h3_from_points(geometry, 4))\n\nhexs\n\nSimple feature collection with 25 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -110.4469 ymin: 41.09064 xmax: -104.1631 ymax: 44.70057\nGeodetic CRS:  WGS 84\n# A tibble: 25 × 2\n               geometry              h3\n *          <POINT [°]>            <H3>\n 1 (-110.1512 43.82525) 8426b45ffffffff\n 2 (-106.1227 43.14504) 8426a05ffffffff\n 3 (-104.2611 43.84146) 8426a5dffffffff\n 4  (-109.9657 44.0744) 8426b4dffffffff\n 5 (-108.7754 42.18515) 8426b15ffffffff\n 6 (-110.3885 44.12357) 842896dffffffff\n 7 (-109.2388 43.58793) 8426b43ffffffff\n 8 (-106.0339 42.94859) 8426a05ffffffff\n 9 (-107.3991 44.60339) 84279bbffffffff\n10 (-106.7301 41.27761) 8426843ffffffff\n# … with 15 more rows\n\n\nAfterwards, lets create a K = 3 disk around each grid cell, create a compact disk by compacting the cells, then unnest into a longer data frame, and update our geometries.\n\ncompact_hexs <- hexs |> \n  mutate(\n    disks = grid_disk(h3, 3),\n    compact_disks = purrr::map(disks, compact_cells)\n  ) |> \n  tidyr::unnest_longer(compact_disks) |> \n  mutate(geometry = st_as_sfc(compact_disks)) |> \n  st_as_sf() \n\nUse ggplot2 to make a simple visualization.\n\nlibrary(ggplot2)\n\nggplot(compact_hexs) +\n  geom_sf(fill = NA) +\n  theme_void()"
  },
  {
    "objectID": "posts/2023-04-13-counting-chars/index.html",
    "href": "posts/2023-04-13-counting-chars/index.html",
    "title": "Feeling rusty: counting characters",
    "section": "",
    "text": "I’m working on a new video for you all on GeoHashes. As I keep working on my slides and script I keep finding new things I want to explore or that I need to write. Recently, that was to compare multiple geohashes to a known geohash. The goal of that is to count how many of the first characters matched a reference. If there are matching leading characters between a geohashes that means that they are in the same geohash at some level of precision. Knowing that number of shared characters tells us at what level of precision they coexist. The challenge is that there isn’t any easy way to do that in base R or packages I could find. So, what do we do when we can’t find something to do a task for us? We make it.\nFor these small tasks that require a lot of iteration and counting, I’ve been leaning on Rust a lot more. I find it actually easier for the more computer sciency type tasks.\nHere’s how I solved it.\nDefine two geohashes to compare:\nNext we want to iterate over each of these string slices (represented as &str). Typically we’d use the .iter() or .into_iter() methods to iterate over objects but these are slices and not a vector or array.\nWe iterate through the characters of a slice using .chars(). We’ll want to iterate through both of them at the same time. Then, for each iteration, we check to see if they’re the same.\nThis will instantiate an iterator over each of the strings where each element is a char\nThese iterators will only be able to be iterated over one at a time using .map() and the like. We can combine them into one iterator using the .zip() method which zips them together into one iterator.\nThis is good! Now we have a single iterator to work through. Each element in the resultant iterator will be a tuple with the first element .0 being the first character of x and .1 being the first character of y.\nThe approach I took here is to use the .take_while() method which takes a closure that returns a bool (true or false). It’ll return another iterator that contains only the elements where that statement was true.\nHere, the closure has the argument |a| which is the tuple from x and y. It checks to see if the characters are equal. The resultant iterator now only has elements for matching characters. We don’t really need to iterate over it, but rather we just need to count how many items are in the iterator.\nWe can use the .count() method for that. Shouts out to the Rust discord for helping me with this one.\nLet’s wrap this into a function:\nWe can make it available in R using rextendr::rust_function().\nrextendr::rust_function(\"\nfn count_seq_chars(x: &str, y: &str) -&gt; usize {\n  x.chars().zip(y.chars())\n      .take_while(|a| a.0 == a.1)\n      .count()\n}\")\n\ncount_seq_chars(\"drt2yyy1cxwy\", \"drt2yywg71qc\")\n\n[1] 6\nBut this isn’t vectorized yet. It only works on two scalars. We can improve it by changing the x argument to take a vector of strings Vec&lt;String&gt;.\nEssentially, what we do next is take this vector of strings, iterate over it, convert the string to a &str then just do what we did before!\nWe use .map() to apply an expression over each element of x. The closure takes a single argument xi which represents the ith element of x. We convert it to a slice, then iterate over it’s characters and the rest should be similar in there!\nLastly, we collect the resultant usize objects into a vector of them Vec&lt;usize&gt;.\nAgain, we can use rextendr to wrap this into a single R function that we can use.\nrextendr::rust_function(\"\n  fn count_seq_chars_to_ref(x: Vec&lt;String&gt;, y: &str) -&gt; Vec&lt;usize&gt; {\n    x.into_iter()\n        .map(|xi| \n            xi.as_str().chars().zip(y.chars())\n            .take_while(|a| a.0 == a.1)\n            .count()\n        )\n        .collect()\n  }\n\")\n\ncount_seq_chars_to_ref(\"drt2yyy1cxwy\", \"drt2yywg71qc\")\n\n[1] 6\nLet’s test this and see how it works with a larger dataset of 100,000 strings. We create a bunch of sample strings that sample a-e and 1-5, are sorted, then pasted together. We then can compare them to the reference string \"abcd123\".\nsample_strings &lt;- replicate(100000, paste0(\n  paste0(sort(sample(letters[1:5], 4)), collapse = \"\"),\n  paste0(sample(1:5, 3), collapse = \"\"),\n  collapse = \"\"\n))\n\n\nhead(sample_strings)\n\n[1] \"bcde123\" \"bcde532\" \"acde413\" \"bcde351\" \"abcd453\" \"bcde413\"\n\ncount_seq_chars_to_ref(head(sample_strings), \"abcd123\")\n\n[1] 0 0 1 0 4 0\nPhilippe Massicotte was kind enough to provide an R only example in a reply to a tweet of mine. We can compare the speed of the two implementations. A pure Rust implementation and an R native implementation.\n{{ tweet philmassicotte 1646191363728240642 }}\nHere we wrap his implementation into a function count_seq_lapply(). I’ve modified this implementation to handle the scenario where the first element is not true so we don’t get a run length of FALSE elements.\ncount_seq_lapply &lt;- function(x, ref) {\n  res &lt;- lapply(x, \\(x) {\n    a &lt;- unlist(strsplit(x, \"\"))\n    x &lt;- unlist(strsplit(ref, \"\"))\n    \n    comparison &lt;- a == x\n    \n    if (!comparison[1]) return(0)\n    \n    rle(comparison)$lengths[1]\n  })\n  \n  unlist(res)\n}\n\ncount_seq_lapply(head(sample_strings), \"abcd123\")\n\n[1] 0 0 1 0 4 0\nAs you can see his works just as well and frankly, better. That’s because he inherits the NA handling of the base R functions he is using. If any NA are introduced into a pure Rust implementation without using extendr types and proper handling you’ll get a panic! which will cause the R function to error.\nbench::mark(\n  lapply = count_seq_lapply(sample_strings, \"abcd123\"), \n  rust = count_seq_chars_to_ref(sample_strings, \"abcd123\")\n  )\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 lapply      925.9ms  925.9ms      1.08    1.58MB     69.1\n2 rust         56.8ms   57.6ms     17.2    781.3KB      0\nThe R implementation is still super fast. It’s just that Rust is also super super fast!"
  },
  {
    "objectID": "index.html#letters-to-a-layperson-myself",
    "href": "index.html#letters-to-a-layperson-myself",
    "title": "Josiah Parry",
    "section": "",
    "text": "Welcome to my personal website. I always aspire to keep pumping out content, but I don’t.\nHere you will find my “recent” blog posts, possibly outdated biographical information, my socials, and links to projects I have made or worked on.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-04-13-counting-chars/index.html#addendum-chatgpt-rules-apparently",
    "href": "posts/2023-04-13-counting-chars/index.html#addendum-chatgpt-rules-apparently",
    "title": "Feeling rusty: counting characters",
    "section": "Addendum: ChatGPT rules apparently",
    "text": "Addendum: ChatGPT rules apparently\nSo I asked Chat GPT to rewrite my above function but using C++ and the results are absolutely startling!\nstd::vector&lt;size_t&gt; count_seq_chars_to_ref_cpp(std::vector&lt;std::string&gt; x, const std::string& y) {\n  std::vector&lt;size_t&gt; result;\n  for (const auto& xi : x) {\n    size_t count = 0;\n    auto it_x = xi.begin();\n    auto it_y = y.begin();\n    while (it_x != xi.end() && it_y != y.end() && *it_x == *it_y) {\n      ++count;\n      ++it_x;\n      ++it_y;\n    }\n    result.push_back(count);\n  }\n  return result;\n}\nThis is the code it wrote after only one prompt. I didn’t correct it. It worked right off the rip. I did, however, provide ChatGPT with my above rust code.\nLet’s bench mark this.\n\nRcpp::cppFunction(\"std::vector&lt;size_t&gt; count_seq_chars_to_ref_cpp(std::vector&lt;std::string&gt; x, const std::string& y) {\n  std::vector&lt;size_t&gt; result;\n  for (const auto& xi : x) {\n    size_t count = 0;\n    auto it_x = xi.begin();\n    auto it_y = y.begin();\n    while (it_x != xi.end() && it_y != y.end() && *it_x == *it_y) {\n      ++count;\n      ++it_x;\n      ++it_y;\n    }\n    result.push_back(count);\n  }\n  return result;\n}\")\n\nbench::mark(\n  GPT_cpp = count_seq_chars_to_ref_cpp(sample_strings, \"abcd123\"),\n  rust = count_seq_chars_to_ref(sample_strings, \"abcd123\")\n)\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 GPT_cpp      1.93ms   2.04ms     481.      784KB     6.17\n2 rust        58.18ms  58.56ms      17.0     781KB     2.13\n\n\nAbsolutely friggin’ bonkers!! If I was better at concurrency and threading I’d try to compare that but alas. I’m stopping here :)"
  }
]