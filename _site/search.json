[
  {
    "objectID": "posts/2023-11-24-dfusionrdr/index.html",
    "href": "posts/2023-11-24-dfusionrdr/index.html",
    "title": "arrow-extendr: extending R with Arrow and Rust",
    "section": "",
    "text": "For this tutorial we’re going to create a very simple Rust-based R package using extendr and arrow-extendr. The package will use the new and very powerful DataFusion crate to create a csv reader.\nWe’ll learn a little bit about how extendr, Rust, and arrow-extendr works throughout the process."
  },
  {
    "objectID": "posts/2023-11-24-dfusionrdr/index.html#create-a-new-r-package",
    "href": "posts/2023-11-24-dfusionrdr/index.html#create-a-new-r-package",
    "title": "arrow-extendr: extending R with Arrow and Rust",
    "section": "Create a new R package",
    "text": "Create a new R package\nWe will use {usethis} to create a new R package called dfusionrdr (prnounced d-fusion reader).\n\n\nThe following section is the standard process for creating a new Rust based R package. It’s a pretty simple process once you get used to it!\nusethis::create_package(\"dfusionrdr\")\nThis will open a new R project with the scaffolding of an R package. From here, we need to make the R package into an extendr R package. To do so we userextendr::use_extendr().\n\n\n\n\n\n\nTip\n\n\n\n\n\nuse_extendr() creates the directory src/ a rust crate in src/rust/ as wll as a few Makevars files in src/ that are used to define how to compile the Rust library. Rust is a compiled language unlike R and Python which are interpreted. Meaning that instead of being able to run code line by line, we have to run it all at once.\nCompiled code can be turned into something called a static library. R can call functions and objects from these libraries using the .Call() function. You do not need to worry about this function. It’s just for context. :)\n\n\n\nrextendr::use_extendr()\n\n\nBefore running this, make sure you have a compatible Rust installation by running rextendr::rust_sitrep(). If you do not, it will tell you need to do. If you’re on windows, you’re likely missing a target."
  },
  {
    "objectID": "posts/2023-11-24-dfusionrdr/index.html#building-your-package",
    "href": "posts/2023-11-24-dfusionrdr/index.html#building-your-package",
    "title": "arrow-extendr: extending R with Arrow and Rust",
    "section": "Building your package",
    "text": "Building your package\nOnce you’ve initialized extendr in your package, we can check to see if everything worked by running the hello_world() function that is included. To do so, we can build our package, then document it.\n\n\n\n\n\n\nTip\n\n\n\nI use the RStudio shortcut to build my package which is cmd + shift + b or if on Windows it’s (probably) ctrl + shift + b. If neither of those work for you, run devtools::build().\n\n\nTo make R functions from Rust available into R, we run rextendr::document().\n\n\nrextendr::document() will also compile your R package for you if need be. Personally, I prefer to build it then document it. For some reason—and it may just be me—I find that compilation from the console can freeze? The cargo file lock is wonky and I probably mess it up a bunch.\nRun devtools::load_all() to bring the documented functions into scope and run the function!\ndevtools::load_all()\nhello_world()\n#&gt; [1] \"Hello world!\"\nWe’ve now ran Rust code directly from R. Pretty simple Rust, but Rust nonetheless."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "recent posts",
    "section": "",
    "text": "arrow-extendr: a very brief tutorial\n\n\n\n\n\n\n\n\n\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nEnums in R: towards type safe R\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhere am I in the sky?\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhere am I in the sky?\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExport Python functions in R packages\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExport Python functions in R packages\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Science Across Languages\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Science Across Languages\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nValve: putting R in production\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nR is still fast: a salty reaction to a salty blog post\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s so special about arrays?\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFeeling rusty: counting characters\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFeeling rusty: counting characters\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nRust traits for R users\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nlearning rust\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nJHU talk (slides)\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nRaw strings in R\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically Create Formulas in R\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nYouTube Videos & what not\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFishnets and overlapping polygons\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nComplete spatial randomness\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nspacetime representations aren’t good—yet\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMake your R scripts Databricks notebooks\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Spatial Data Analysis in R\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMy new IDE theme: xxEmoCandyLandxx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nActually identifying R package System Requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe heck is a statistical moment??\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Python on my M1 in under 10 minutes\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSLICED! a brief reflection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n{cpcinema} & associated journey\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nOSINT in 7 minutes\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nAPIs: the language agnostic love story\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nPython & R in production — the API way\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nColor Palette Cinema\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSecure R Package Environments\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is critical race theory, anyways?\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nDemographic Change, White Fear, and Social Construction of Race\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMedium Data and Production API Pipeline\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Red Queen Effect\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExcel in pRod\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nDesign Paradigms in R\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nR Security Concerns and 2019 CRAN downloads\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nNon-interactive user tokens with googlesheets4\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFinding an SPSS {haven}\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Tidy Modeling\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWater Quality Analysis\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n∑ { my parts }\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Trends for Campaigns\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWeb-scraping for Campaigns\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing trendyy\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\ngenius tutorial\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\ngenius Plumber API\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Fallacy of one person, one vote\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cost of Gridlock\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nxgboost feature importance\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n[Not so] generic functions\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nUS Representation: Part I\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing: Letters to a layperson\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nChunking your csv\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nReading Multiple csvs as 1 data frame\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nCoursera R-Programming: Week 2 Problems\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing geniusR\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "posts"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "Packages & contributions\n\n\n\n\n\n\n\nR package system requirements\n\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenius (retired)\n\n\n\npackage\n\n\nmusic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nh3o for H3 indexing\n\n\n\nspatial\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspdep (contributor)\n\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nThings I’ve written\n\n\n\n\n\n\n\nR for Progressive Campaigns\n\n\n\nwriting\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban Informatics Toolkit\n\n\n\nwriting\n\n\nurban-informatics\n\n\ntextbook\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmirrr - song genre classification\n\n\n\nwriting\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "projects"
    ]
  },
  {
    "objectID": "projects/pkgs/sfdep.html",
    "href": "projects/pkgs/sfdep.html",
    "title": "sfdep",
    "section": "",
    "text": "sfdep is an R package that acts as a tidy interface to the spdep package. In addition it provides some statistics that are not available elsewhere in the R ecosystem such as the neighbor match test.\n\n\n\nEmerging Hot Spot Analysis\nColocation Quotients\nsfnetworks integrations\nLocal Neighbor Match Test\n\n\n\n\n\n GitHub\nRStudio Conf 2022L\nNew York Open Statistical Programming Meetup"
  },
  {
    "objectID": "projects/pkgs/sfdep.html#package-highlights",
    "href": "projects/pkgs/sfdep.html#package-highlights",
    "title": "sfdep",
    "section": "",
    "text": "Emerging Hot Spot Analysis\nColocation Quotients\nsfnetworks integrations\nLocal Neighbor Match Test"
  },
  {
    "objectID": "projects/pkgs/sfdep.html#resources",
    "href": "projects/pkgs/sfdep.html#resources",
    "title": "sfdep",
    "section": "",
    "text": "GitHub\nRStudio Conf 2022L\nNew York Open Statistical Programming Meetup"
  },
  {
    "objectID": "projects/pkgs/h3o.html",
    "href": "projects/pkgs/h3o.html",
    "title": "h3o for H3 indexing",
    "section": "",
    "text": "{h3o} is an R package that offers high-performance geospatial indexing using the H3 grid system. The package is built using {extendr} and provides bindings to the Rust library of the same name.\nThe Rust community built h3o which is a pure rust implementation of Uber’s H3 hierarchical hexagon grid system. Since h3o is a pure rust library it is typically safer to use, just as fast, and dependency free."
  },
  {
    "objectID": "projects/pkgs/h3o.html#benefits-of-h3o",
    "href": "projects/pkgs/h3o.html#benefits-of-h3o",
    "title": "h3o for H3 indexing",
    "section": "Benefits of h3o",
    "text": "Benefits of h3o\nSince h3o is built purely in Rust and R it is system dependency free and can be compiled for multiple platforms including Linux, MacOS, and Windows, making it easy to use across different OS.\nh3o benefits greatly from the type safety of Rust and provides robust error handling often returning 0 length vectors or NA values when appropriate where errors would typically occur using another H3 library.\nAnd moreover, it is very fast!"
  },
  {
    "objectID": "projects/pkgs/h3o.html#features",
    "href": "projects/pkgs/h3o.html#features",
    "title": "h3o for H3 indexing",
    "section": "Features",
    "text": "Features\nh3o supports all of the functionality that is provided by the C library and the Rust library h3o.\n\n\nIf there are any features missing, please make an issue on GitHub and I’ll be sure to address it!\nh3o was built with sf objects and the tidyverse in mind. h3o objects can be created from sf objects and vice versa. Compatibility with the tidyverse is accomplished via the vctrs package.\n\n\nsf::st_as_sfc() methods for H3 and H3Edge vectors\nautomatic nesting by creating lists of H3 and H3Edge vectors\n\nvectorized output will never return more objects than inputs\n\n\n\nExample\nCreate some points in the bounding box of Wyoming.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(h3o)\n\n# create a bounding box\nbbox_raw &lt;- setNames(\n  c(-111.056888, 40.994746, -104.05216, 45.005904),\n  c(\"xmin\", \"ymin\", \"xmax\", \"ymax\")\n)\n\n# create some points\npnts &lt;- st_bbox(bbox_raw) |&gt; \n  st_as_sfc() |&gt; \n  st_set_crs(4326) |&gt; \n  st_sample(25)\n\n# convert to H3 index\nhexs &lt;- h3_from_points(pnts, 4) \nhexs\n\n&lt;H3[25]&gt;\n [1] 8428965ffffffff 84279b3ffffffff 8426b07ffffffff 8426a0bffffffff\n [5] 8426861ffffffff 8427999ffffffff 8426b07ffffffff 8426b45ffffffff\n [9] 8426a45ffffffff 842896dffffffff 84279a7ffffffff 8426b05ffffffff\n[13] 8428963ffffffff 8426a37ffffffff 8428965ffffffff 8426b5dffffffff\n[17] 84278bdffffffff 8426b37ffffffff 8426aedffffffff 8428963ffffffff\n[21] 842686bffffffff 8426b4bffffffff 8426a21ffffffff 8426b01ffffffff\n[25] 8426b19ffffffff\n\n\nThe H3 vectors can be easily visualized by converting to sf objects. The st_as_sfc() method is defined for H3 vectors. While you may be familair with st_as_sf() the _sfc variant is used for creating columns and should be used on a vector not a dataframe. This way you can use it in a dplyr pipe.\n\npolys &lt;- st_as_sfc(hexs)\npolys\n\nGeometry set for 25 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -111.3692 ymin: 40.92904 xmax: -103.9451 ymax: 45.39309\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOLYGON ((-110.0544 44.27004, -109.7884 44.4130...\n\n\nPOLYGON ((-107.5966 43.81128, -107.328 43.94941...\n\n\nPOLYGON ((-109.6219 42.21408, -109.3608 42.3595...\n\n\nPOLYGON ((-104.8236 42.97064, -104.5542 43.1035...\n\n\nPOLYGON ((-108.2209 40.92904, -107.9611 41.0732...\n\n\nThis can be plotted.\n\nplot(polys)\n\n\n\n\nTo illustrate tidyverse compatibility lets create an sf object and create a column of H3 indexes.\n\nlibrary(dplyr, warn.conflicts = FALSE)\n\nhexs &lt;- tibble(geometry = pnts) |&gt; \n  st_as_sf() |&gt; \n  mutate(h3 = h3_from_points(geometry, 4))\n\nhexs\n\nSimple feature collection with 25 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -111.0513 ymin: 41.06984 xmax: -104.1015 ymax: 44.9823\nGeodetic CRS:  WGS 84\n# A tibble: 25 × 2\n               geometry              h3\n *          &lt;POINT [°]&gt;            &lt;H3&gt;\n 1 (-110.1877 44.44031) 8428965ffffffff\n 2 (-107.6884 43.92492) 84279b3ffffffff\n 3 (-109.9335 42.48766) 8426b07ffffffff\n 4  (-104.607 43.28044) 8426a0bffffffff\n 5 (-108.3843 41.06984) 8426861ffffffff\n 6  (-106.3236 44.9823) 8427999ffffffff\n 7 (-109.4325 42.36607) 8426b07ffffffff\n 8 (-109.7946 43.75016) 8426b45ffffffff\n 9   (-105.5976 44.114) 8426a45ffffffff\n10  (-110.2683 44.2698) 842896dffffffff\n# ℹ 15 more rows\n\n\nAfterwards, lets create a K = 3 disk around each grid cell, create a compact disk by compacting the cells, then unnest into a longer data frame, and update our geometries.\n\ncompact_hexs &lt;- hexs |&gt; \n  mutate(\n    disks = grid_disk(h3, 3),\n    compact_disks = purrr::map(disks, compact_cells)\n  ) |&gt; \n  tidyr::unnest_longer(compact_disks) |&gt; \n  mutate(geometry = st_as_sfc(compact_disks)) |&gt; \n  st_as_sf() \n\nUse ggplot2 to make a simple visualization.\n\nlibrary(ggplot2)\n\nggplot(compact_hexs) +\n  geom_sf(fill = NA) +\n  theme_void()"
  },
  {
    "objectID": "projects/pkgs/spdep.html",
    "href": "projects/pkgs/spdep.html",
    "title": "spdep (contributor)",
    "section": "",
    "text": "spdep (contributor)\nspdep is an absolute powerhouse of an R package. spdep was first released in 2002 and is one of, if not the, first package to implement many of the most important spatial statistics for aerial data.\nMy contributions include:\n\nLocal Geary C (univariate and multivariate)\nLocal bivariate Moran’s I\nGlobal bivariate Moran’s I\nLocal univariate join count\nLocal bivariate join count"
  },
  {
    "objectID": "projects/writing/r4campaigns.html",
    "href": "projects/writing/r4campaigns.html",
    "title": "R for Progressive Campaigns",
    "section": "",
    "text": "Read the book here.\nIn 2008, the Obama campaign revolutionized the use of data in political campaigns. Since then, data teams have expanded and grown in size, capacity, and complexity.\nThis short bookdown project is intended to illustrate how data can be used in campaigns through the statistical programming language R. This is a collection of small “recipes” that I have created that are intended to aid data teams in leveraging R."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "employment: Senior Product Engineer @ Esri\neducation:\n\nMS Urban Informatic, Northeastern University (2020)\nBA Sociology, Plymouth State University\n\nMinor, General Mathematics\nProfessional Certificate GIS\nI am a Senior Product Engineer on the Spatial Analysis team at Esri. Previously, I was at The NPD Group as a Research Analyst where I worked to modernize our data science infrastructure to use Databricks, Docker, and Spark. Before that, I was at RStudio, PBC on the customer success team enabling public sector adoption of data science tools. In 2020 I received my master’s degree in Urban Informatics from Northeastern University following my bachelor’s degree in sociology with focuses in geographic information systems and general mathematics from Plymouth State University in 2018.",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "about",
    "section": "Contact me",
    "text": "Contact me\nIf you want to get in contact with me please send me an email at josiah.parry at gmail dot com.\n\n\ntalks i’ve given\n\n\nExploratory Spatal Data Analysis in the tidyverse\n\nJuly 28th, 2022 rstudio::conf(2022L)\n\nExploratory Spatial Data Analysis in R\n\nRecording\nApril 28th, 2022\n\nAPIs: you’re probably not using them and why you probably should\n\nGovernment Advances in Statistical Programming\nNovember 6th, 2020\n\n“Old Town Road” Rap or Country?: Putting R in Production with Tidymodels, Plumber, and Shiny\n\nBoston useR group\nDecember 10th, 2019\n\nTidy Lyrical Analysis\n\nBoston useR group\nJuly 17th, 2018\n\nNewfound Lake Landscape Value Analysis: Exploring the efficacy of PPGIS, NESTVAL 2016\n\nNew England St. Lawrence River Valley regional American Associations of Geographers Conference\n2016",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "Welcome to my personal website. I always aspire to keep pumping out content, but I don’t.\nHere you will find my “recent” blog posts, possibly outdated biographical information, my socials, and links to projects I have made or worked on.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\n\n\n\n\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\n\n\n\n\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\n\n\n\n\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\n\n\n\n\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\n\n\n\n\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\n\n\n\n\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\n\n\n\n\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\n\n\n\n\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\n\n\n\n\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "index.html#letters-to-a-layperson-myself",
    "href": "index.html#letters-to-a-layperson-myself",
    "title": "Josiah Parry",
    "section": "",
    "text": "Welcome to my personal website. I always aspire to keep pumping out content, but I don’t.\nHere you will find my “recent” blog posts, possibly outdated biographical information, my socials, and links to projects I have made or worked on.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\n\n\n\n\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\n\n\n\n\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\n\n\n\n\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\n\n\n\n\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\n\n\n\n\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\n\n\n\n\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\n\n\n\n\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\n\n\n\n\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\n\n\n\n\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html",
    "title": "Medium Data and Production API Pipeline",
    "section": "",
    "text": "“[P]arsing huge json strings is difficult and inefficient.”1 If you have an API that needs to receive a large amount of json, sending that over will be slow.\nQ: How can we improve that? A: Compression."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#background",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#background",
    "title": "Medium Data and Production API Pipeline",
    "section": "Background",
    "text": "Background\nAn API is an application programming interface. APIs are how machines talk to other machines. APIs are useful because they are language agnostic meaning that the same API request from Python, or R, or JavaScript will work and return the same results. To send data to an API we use a POST request. The data that we send is usually required to be in json format."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#context",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#context",
    "title": "Medium Data and Production API Pipeline",
    "section": "Context",
    "text": "Context\nProblem: With large data API POST requests can become extremely slow and take up a lot of storage space. This can cause a bottleneck.\nSolution: Compress your data and send a file instead of sending plain text json."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#standard-approach",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#standard-approach",
    "title": "Medium Data and Production API Pipeline",
    "section": "Standard approach",
    "text": "Standard approach\nInteracting with an API from R is usually done with the {httr} package. Imagine you want to send a dataframe to an API as json. We can do that by using the httr::POST(), providing a dataframe to the body, and encoding it to json by setting encode = \"json\".\nFirst let’s load our libraries:\n\nlibrary(httr)          # interacts with apis\nlibrary(jsonlite)      # works with json (for later)\nlibrary(nycflights13)  # data for posting \n\nNext, let’s create a sample POST() request to illustrate how posting a dataframe as json works.\n\n\nb_url &lt;- \"http://httpbin.org/post\" # an easy to work with sample API POST endpoint\n\nPOST(url = b_url, \n     body = list(x = cars),\n     encode = \"json\")\n#&gt; Response [http://httpbin.org/post]\n#&gt;   Date: 2022-11-14 22:14\n#&gt;   Status: 200\n#&gt;   Content-Type: application/json\n#&gt;   Size: 4.81 kB\n#&gt; {\n#&gt;   \"args\": {}, \n#&gt;   \"data\": \"{\\\"x\\\":[{\\\"speed\\\":4,\\\"dist\\\":2},{\\\"speed\\\":4,\\\"dist\\\":10},{\\\"speed\\\":7,\\\"...\n#&gt;   \"files\": {}, \n#&gt;   \"form\": {}, \n#&gt;   \"headers\": {\n#&gt;     \"Accept\": \"application/json, text/xml, application/xml, */*\", \n#&gt;     \"Accept-Encoding\": \"deflate, gzip\", \n#&gt;     \"Content-Length\": \"1150\", \n#&gt;     \"Content-Type\": \"application/json\", \n#&gt; ..."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#alternative-approach",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#alternative-approach",
    "title": "Medium Data and Production API Pipeline",
    "section": "Alternative approach",
    "text": "Alternative approach\nAn alternative approach would be to write our dataframe as json to a compressed gzip file. The process will be to:\n\nCreate a temporary file which will store our compressed json.\nCreate a gzip file connection to write the temporary file as a gzip.\nUpload the temporary file to the API.\nRemove the temporary file.\n\nWriting to a temporary gzipped file looks like:\n\n# create the tempfile \ntmp &lt;- tempfile()\n\n# create a gzfile connection (to enable writing gz)\ngz_tmp &lt;- gzfile(tmp)\n\n# write json to the gz file connection\nwrite_json(cars, gz_tmp)\n\n# close the gz file connection\nclose(gz_tmp)\n\nLet’s read the temp file to see what it contains.\n\n# read the temp file for illustration \nreadLines(tmp)\n#&gt; [1] \"[{\\\"speed\\\":4,\\\"dist\\\":2},{\\\"speed\\\":4,\\\"dist\\\":10},{\\\"speed\\\":7,\\\"dist\\\":4},{\\\"speed\\\":7,\\\"dist\\\":22},{\\\"speed\\\":8,\\\"dist\\\":16},{\\\"speed\\\":9,\\\"dist\\\":10},{\\\"speed\\\":10,\\\"dist\\\":18},{\\\"speed\\\":10,\\\"dist\\\":26},{\\\"speed\\\":10,\\\"dist\\\":34},{\\\"speed\\\":11,\\\"dist\\\":17},{\\\"speed\\\":11,\\\"dist\\\":28},{\\\"speed\\\":12,\\\"dist\\\":14},{\\\"speed\\\":12,\\\"dist\\\":20},{\\\"speed\\\":12,\\\"dist\\\":24},{\\\"speed\\\":12,\\\"dist\\\":28},{\\\"speed\\\":13,\\\"dist\\\":26},{\\\"speed\\\":13,\\\"dist\\\":34},{\\\"speed\\\":13,\\\"dist\\\":34},{\\\"speed\\\":13,\\\"dist\\\":46},{\\\"speed\\\":14,\\\"dist\\\":26},{\\\"speed\\\":14,\\\"dist\\\":36},{\\\"speed\\\":14,\\\"dist\\\":60},{\\\"speed\\\":14,\\\"dist\\\":80},{\\\"speed\\\":15,\\\"dist\\\":20},{\\\"speed\\\":15,\\\"dist\\\":26},{\\\"speed\\\":15,\\\"dist\\\":54},{\\\"speed\\\":16,\\\"dist\\\":32},{\\\"speed\\\":16,\\\"dist\\\":40},{\\\"speed\\\":17,\\\"dist\\\":32},{\\\"speed\\\":17,\\\"dist\\\":40},{\\\"speed\\\":17,\\\"dist\\\":50},{\\\"speed\\\":18,\\\"dist\\\":42},{\\\"speed\\\":18,\\\"dist\\\":56},{\\\"speed\\\":18,\\\"dist\\\":76},{\\\"speed\\\":18,\\\"dist\\\":84},{\\\"speed\\\":19,\\\"dist\\\":36},{\\\"speed\\\":19,\\\"dist\\\":46},{\\\"speed\\\":19,\\\"dist\\\":68},{\\\"speed\\\":20,\\\"dist\\\":32},{\\\"speed\\\":20,\\\"dist\\\":48},{\\\"speed\\\":20,\\\"dist\\\":52},{\\\"speed\\\":20,\\\"dist\\\":56},{\\\"speed\\\":20,\\\"dist\\\":64},{\\\"speed\\\":22,\\\"dist\\\":66},{\\\"speed\\\":23,\\\"dist\\\":54},{\\\"speed\\\":24,\\\"dist\\\":70},{\\\"speed\\\":24,\\\"dist\\\":92},{\\\"speed\\\":24,\\\"dist\\\":93},{\\\"speed\\\":24,\\\"dist\\\":120},{\\\"speed\\\":25,\\\"dist\\\":85}]\"\n\n\nPOSTing a file\nTo post a file we use the function httr::upload_file(). The argument we provide is the path, in this case the file path is stored in the tmp object.\n\nPOST(b_url, body = list(x = upload_file(tmp)))\n#&gt; Response [http://httpbin.org/post]\n#&gt;   Date: 2022-11-14 22:14\n#&gt;   Status: 200\n#&gt;   Content-Type: application/json\n#&gt;   Size: 874 B\n#&gt; {\n#&gt;   \"args\": {}, \n#&gt;   \"data\": \"\", \n#&gt;   \"files\": {\n#&gt;     \"x\": \"data:text/plain;base64,H4sIAAAAAAAAA4XSPQ6DMAwF4L3HyMyQ+C8JV6m6wdCtEt0q7t6p...\n#&gt;   }, \n#&gt;   \"form\": {}, \n#&gt;   \"headers\": {\n#&gt;     \"Accept\": \"application/json, text/xml, application/xml, */*\", \n#&gt;     \"Accept-Encoding\": \"deflate, gzip\", \n#&gt; ...\n\n\n\nComparing R object to gzip\nNow, you may be asking, is this really that big of a difference? It actually is. If you’ll notice from the first response where we POSTed the cars dataframe the response size was 4.81kB. This response with the compressed file was only 870B. Thats a whole lot smaller.\nWe can compare the object size to the file size for another look. The below is in bytes.\n\ncat(\" cars: \", object.size(cars), \"\\n\",\n    \"compressed cars: \", file.size(tmp))\n#&gt;  cars:  1648 \n#&gt;  compressed cars:  210"
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#benchmarking",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#benchmarking",
    "title": "Medium Data and Production API Pipeline",
    "section": "Benchmarking",
    "text": "Benchmarking\nLet’s extend this example to some larger datasets as well as benchmark the results. We’ll use data from nycflights13. In this example we’ll send two dataset to an API as the parameters metadata and data. Generally metadata is smaller than the data. So for this example we’ll send 1,000 rows as the metadata and 10,000 rows as the data. We’ll call on the weather and flights datasets from nycflights13.\n\nsmall_weather &lt;- dplyr::sample_n(weather, 1000)\nsmall_flights &lt;- dplyr::sample_n(flights, 10000)\n\n\nMaking it functional\nAs always, I recommend making your repetitive tasks into functions. Here we will create two functions. One for posting the data as gzip files and the second as pure json. These will be labeled post_gz() and post_json() respectively.\nThese functions will take two parameters: metadata and data.\nDefine post_gz()\n\npost_gz &lt;- function(metadata, data) {\n  \n  # write metadata to temp file\n  tmp_meta &lt;- tempfile(\"metadata\")\n  gz_temp_meta &lt;- gzfile(tmp_meta)\n  write_json(metadata, gz_temp_meta)\n  close(gz_temp_meta)\n  \n  # write data to temp file\n  tmp_data &lt;- tempfile(\"data\")\n  gz_temp_data &lt;- gzfile(tmp_data)\n  write_json(data, gz_temp_data)\n  close(gz_temp_data)\n  \n  # post \n  q &lt;- POST(b_url, \n       body = list(\n         metadata = upload_file(tmp_meta),\n         data = upload_file(tmp_data)\n       ))\n  \n  # remove temp files\n  unlink(tmp_meta)\n  unlink(tmp_data)\n  \n  # return a character for purposes of bench marking\n  \"Posted...\"\n}\n\nDefine post_json().\n\npost_json &lt;- function(metadata, data) {\n  q &lt;- POST(b_url, \n       body = list(\n         metadata = metadata,\n         data = data\n       ),\n       encode = \"json\") \n  \n  \"Posted...\"\n}\n\nNow that these functions have been defined, let’s compare their performance using the package bench. We’ll run each function 50 times to get a good understanding of their respective performance.\n\nbm &lt;- bench::mark(\n  post_gz(small_weather, small_flights),\n  post_json(small_weather, small_flights),\n  iterations = 5\n  )\n\nbm\n\n\nggplot2::autoplot(bm)"
  },
  {
    "objectID": "posts/2020-06-12-red-queen.html",
    "href": "posts/2020-06-12-red-queen.html",
    "title": "The Red Queen Effect",
    "section": "",
    "text": "The Red Queen and maintenance of state and society\nIt’s Monday morning. You’re back at work after a few days off. Your inbox is a lot more full than you hoped with 70 emails. Time to get reading and sending. It’s been an hour and you’ve read and sent at least 20 emails but your inbox is still at 70. You’ve been working hard and yet it feels like you’ve gone nowhere. This idea of working really hard but feeling like you’ve gone nowhere is at the center of the Red Queen Effect.\nThere are a number of “Red Queen Effect”s in the scientific literature all of which are inspired from the Red Queen’s race from Lewis Carrols’s Through the Looking Glass.\n\n“Well, in our country,” said Alice, still panting a little, “you’d generally get to somewhere else—if you run very fast for a long time, as we’ve been doing.”\n“A slow sort of country!” said the Queen. “Now, here, you see, it takes all the running you can do, to keep in the same place. If you want to get somewhere else, you must run at least twice as fast as that!” \n\nAlice is running and running and getting nowhere. Much like how as you read emails you get new ones. Daron Acemoglu and James A. Robinson adapt this concept to the evolution of states and connect it to their idea of the Narrow Corridor. The path to a “successful” society is a race between the power of the people and the power of the state. States in which the pace of growth in both the peoples’ power and the power / ability of the state are similar often produce more liberal (in the sense of liberty) nations.\n\nThis graphic is meant to illustrate this race. Getting into the corridor is a game of chase between the power of the state and society. Keeping the balance is delicate act—one that no nation has perfected—which requires society to check the power of the state and the state to provide checks to society. They define the Red Queen as\n\n“The process of competition, struggle and cooperation between state and society”"
  },
  {
    "objectID": "posts/2023-11-24-dfusionrdr/index.html#goal",
    "href": "posts/2023-11-24-dfusionrdr/index.html#goal",
    "title": "arrow-extendr: extending R with Arrow and Rust",
    "section": "",
    "text": "For this tutorial we’re going to create a very simple Rust-based R package using extendr and arrow-extendr. The package will use the new and very powerful DataFusion crate to create a csv reader.\nWe’ll learn a little bit about how extendr, Rust, and arrow-extendr works throughout the process."
  },
  {
    "objectID": "posts/2023-11-24-dfusionrdr/index.html#adding-rust-dependencies",
    "href": "posts/2023-11-24-dfusionrdr/index.html#adding-rust-dependencies",
    "title": "arrow-extendr: extending R with Arrow and Rust",
    "section": "Adding Rust dependencies",
    "text": "Adding Rust dependencies\nMuch like how we like to use R packages to make our lives easier, we can use Rust crates (libraries) to make do crafty things. To do so, we will open up our Rust crate in our preferred editor. I prefer VS Code.\n\n\nIf you haven’t configured VS Code to use with Rust, there are like a million different ways to configure it. But at minimum, install the rust-analyzer, BetterTOML, and CodeLLDB extensions (I think CodeLLDB comes with the rust-analyzer though?)\nOpen src/rust/ in VS Code. Then we will add 3 additional dependencies. These are\n\ndatafusion\n\na powerful Arrow-based DataFrame library (like Polars but different)\n\ntokio\n\nwhich will give us the ability to run code lazily and asynchronously which is required by datafusion\n\narrow_extendr\n\nthis is a crate I built that lets us send Arrow data from Rust to R and back\n\n\nIn the terminal run the following\ncargo add datafusion\ncargo add tokio\ncargo add arrow_extendr --git https://github.com/JosiahParry/arrow-extendr\n\n\narrow-extendr is not published on crates.io yet so we need to pass the git flag to tell Rust where to find the library.\n\n\n\n\n\n\nNote\n\n\n\nThis is my preferred way of adding dependencies. If you open up Cargo.toml you’ll now see these libraries added under the [Dependencies] heading.\n\n\n\nMaking R work with DataFusion\nDataFusion requires one additional C library that we need to use we need to add it to our Makevars. This is not something you typically have to do, but DataFusion requires it from us.\nOpen Makevars and Makevars.win. One the line that starts with PKG_LIBS add -llzma to the end.\nAgain, this is not a common thing you have to do. This is specifically for our use case."
  },
  {
    "objectID": "posts/2023-11-24-dfusionrdr/index.html#building-our-csv-reader",
    "href": "posts/2023-11-24-dfusionrdr/index.html#building-our-csv-reader",
    "title": "arrow-extendr: extending R with Arrow and Rust",
    "section": "Building our CSV Reader",
    "text": "Building our CSV Reader\nOpen src/lib.rs. This is where your R package is defined. For larger packages you may want to break it up into multiple smaller files. But our use case is relatively small (and frankly, not that simple, lol!).\nLet’s first start by removing our hello_world example from our code. Delete the hello world function (lines 3-8) and remove it from the module declaration under mod dfusionrdr.\n\n\n\n\n\n\nTip\n\n\n\nIn order to make our Rust functions available to R, we need to include them in our extendr_module! macro call. Under mod dfusionrdr we can add additional functions there. Those incldued in there will be made available to R. If the have /// @export roxygen tag, then they will be exported in the R package as well.\nextendr_module! {\n    mod dfusionrdr;\n}\n\n\nLet’s create the scaffolding for our first function read_csv_dfusion()\n#[extendr]\n/// @export \nfn read_csv(csv_path: &str)  {\n  rprintln!(\"We will read the csv file at: `{csv_path}`\");\n}\n\n\n\nThe #[extendr] macro indicates that this function will be made available to R.\nWe add /// @export to indicate that our function will be exported to R. We can add roxygen2 documentation to our functions by prefixing with /// which a documentation comment wheras // is a normal comment.\n\nThis function prints a message indicating we will read a CSV at the path provided. It takes one argument csv_path which is an &str. A &str in Rust is a like a scalar character in R e.g. \"my-file.csv\"\nNext we need to make sure the function is available to R in the module.\nextendr_module! {\n    mod dfusionrdr;\n    fn read_csv_dfusion;\n}\nFrom RStudio, let’s build, document, and load again.\ndevtools::build()    # 1. \nrextendr::document() # 2. \ndevtools::load_all() # 3.\n\n\n\nOnly run if you haven’t built with cmd + shift + b\nThis brings functions into the NAMESPACE and updates arguments and outputs\nLoads everything from your package into memory\n\n\nImport dependencies\nIn order to use DataFusion to read dependencies we need to import it. A lot of Rust libraries have something called a prelude. The prelude is a special module that contains common structs, traits, enums, etc that are very useful for the crate. Notice that the top of your lib.rs includes use extendr_api::prelude::*; this brings all of the Rust based R objects into scope such as Robj, Doubles, Integers etc.\nDataFusion also has a useful prelude that we want to bring into scope. We will add use datafusion::prelude::*; to the top of our file (much like adding library()). This brings important objects into scope for us. We will also need tokio::runtime::Runtime as well.\nThe first 3 lines of your lib.rs should look like this:\nuse extendr_api::prelude::*;\nuse datafusion::prelude::*;\nuse tokio::runtime::Runtime;\n\n\nContext and Runtime\nDataFusion requires something called a SessionContext. The session context\n\n“maintains the state of the connection between a user and an instance of the DataFusion engine.”\n\nWe need to instantiate this struct inside of our function.\nfn read_csv_dfusion(csv_path: &str) {\n    let ctx = SessionContext::new();\n}\nWe now have a ctx object which we can use to read our csv. It has a method called read_csv(). It requires the path of a csv to read as well as a struct called CsvReadOptions which determines how it will be read into memory. We will pass csv_path to the first argument and create a default options struct with the new() method.\nfn read_csv_dfusion(csv_path: &str) {\n    let ctx = SessionContext::new();\n    let csv = ctx.read_csv(\n        csv_path,\n        CsvReadOptions::new()\n    );\n}\nThis will compile with a bunch of warnings about unused variables. But, more importantly, the csv variable we created is special. If you have your Rust analyzer configured you should see that it is of type impl Future&lt;Output = Result &lt;..., ...&gt;&gt;. That right there is problematic!\nWhen you see impl Future&lt;...&gt; that tells us it is an asynchronous result that needs to be polled and executed. async functions are lazy. They don’t do anything until you ask it to. The way to do this is by calling the .await attribute. We can then unwrap() the results and store it into another variable.\n\n\n\n\n\n\nWarning\n\n\n\n\n\nIt’s typically a pretty bad idea to use .unwrap() since the program will “panic!” if it does not get a result that it expected. But it’s a pretty handy way to get working code without error handling. I typically handle errors after I’ve gotten the bulk of what I want working.\n\n\n\nfn read_csv_dfusion(csv_path: &str) {\n    let ctx = SessionContext::new();\n    let csv = ctx.read_csv(\n        \"sdf\",\n        CsvReadOptions::new()\n    );\n\n    let csv_result = csv.await.unwrap();\n}\nIf we run cargo check in our terminal we will get the message:\nerror[E0728]: `await` is only allowed inside `async` functions and blocks\nOne way to get this to work would be to add async fn instead of fn but that isn’t supported by extendr since R is single threaded and doesn’t support async. So how do we get around this?\n\n\nasync with extendr and tokio\nIn order to run async functions we need to execute it in a runtime. tokio provides this for us with the Runtime struct. It lets us run impl Future&lt;...&gt; in a non async function!\nWe’ll modify our function definition to\nfn read_csv_dfusion(csv_path: &str) {\n    let rt = Runtime::new().unwrap();\n    let ctx = SessionContext::new();\n    let csv = ctx.read_csv(\n        csv_path,\n        CsvReadOptions::new()\n    );\n}\nWith the Runtime object rt we can call the block_on() method which takes a Future and runs it until it has completed. This means that we don’t get to use async functionality—e.g. executing 2 or more things at the same time—but we still get to take the result!\nLet’s read the csv into an object called df using the block_on() method.\nfn read_csv_dfusion(csv_path: &str) {\n    let rt = Runtime::new().unwrap();\n    let ctx = SessionContext::new();\n    let csv = ctx.read_csv(\n        csv_path,\n        CsvReadOptions::new()\n    );\n\n    let df = rt.block_on(csv).unwrap();\n}\nThe analyzer shows that this is a DataFrame. Awesome! Now, how can we get this into memory?\n\n\nSending DataFrames to R with arrow-extendr\nThis is where arrow-extendr comes into play. arrow-extendr provides a couple of traits which allow us to convert a number of arrow-rs types into an Robj.\n\n\nSee my post on Rust traits for R users\n\n\n\n\n\n\nTip\n\n\n\nAn Robj is extendr’s catch all for any type of object that can be returned to R\n\n\nThe IntoArrowRobj trait can convert a Vec&lt;RecordBatch&gt; into an Robj. The R documentation for a RecordBatch says\n\n“A record batch is a collection of equal-length arrays matching a particular Schema. It is a table-like data structure that is semantically a sequence of fields, each a contiguous Arrow Array.”\n\nBased on that, a Vec&lt;RecordBatch&gt; is a collection of chunks of a table-like data structures.\nDataFrames have a method .collect() which creates a Vec&lt;RecordBatch&gt;.\nLet’s modify our function to turn the DataFrame into a Vec&lt;RecordBatch&gt;.\n\n\n\n\n\n\nNote\n\n\n\nAll things with DataFusion are done async so we need to wrap them in rt.block_on().\n\n\nfn read_csv_dfusion(csv_path: &str) {\n    let rt = Runtime::new().unwrap();\n    let ctx = SessionContext::new();\n    let csv = ctx.read_csv(\n        csv_path,\n        CsvReadOptions::new()\n    );\n\n    // create a dataframe from the csv\n    let df = rt.block_on(csv).unwrap();\n\n    // collect the results into record batches\n    let res = rt.block_on(df.collect()).unwrap();\n}\nWith this, we can send the results to R with the into_arrow_robj() method! First we need to add use arrow_extendr::to::IntoArrowRobj; to the top of our script to bring the trait into scope.\nThen in our function we need to specify the return type as Robj (see the first line of the definition -&gt; Robj) and then turn res into an Robj\nfn read_csv_dfusion(csv_path: &str) -&gt; Robj {\n    let rt = Runtime::new().unwrap();\n    let ctx = SessionContext::new();\n    let csv = ctx.read_csv(\n        csv_path,\n        CsvReadOptions::new()\n    );\n\n    // create a dataframe from the csv\n    let df = rt.block_on(csv).unwrap();\n\n    // collect the results into record batches\n    let res = rt.block_on(df.collect()).unwrap();\n\n    res.into_arrow_robj().unwrap()\n}\n\n\nHandling arrow-rs from R\nLet’s rebuild and document our function again.\nI’ve added a csv of {palmerpenguins} to the inst/ folder of our package for testing. Let’ try reading this in.\nres &lt;- read_csv_dfusion(\"inst/penguins.csv\")\nres\n#&gt; &lt;nanoarrow_array_stream struct&lt;rowid: int64, species: string, island: string, bill_length_mm: string, bill_depth_mm: string, flipper_length_mm: string, body_mass_g: string, sex: string, year: int64&gt;&gt;\n#&gt;  $ get_schema:function ()  \n#&gt;  $ get_next  :function (schema = x$get_schema(), validate = TRUE)  \n#&gt;  $ release   :function ()  \nNow, this doesn’t look very familiar to most R users. This is an object from the {nanoarrow} R package called \"nanoarrow_array_stream\". This is how data is received from Rust in R. We can process batches from this “stream” using the method get_next(). But there’s a handy as.data.frame() method for it.\n\n\n\n\n\n\nTip\n\n\n\nThis is a good time to note that you should add nanoarrow as a dependency of your package explicitly with usethis::use_package(\"nanoarrow\").\n\n\nres &lt;- read_csv_dfusion(\"inst/penguins.csv\")\nas.data.frame(res) |&gt; \n  head()\n#&gt;   rowid species    island bill_length_mm bill_depth_mm flipper_length_mm\n#&gt; 1     1  Adelie Torgersen           39.1          18.7               181\n#&gt; 2     2  Adelie Torgersen           39.5          17.4               186\n#&gt; 3     3  Adelie Torgersen           40.3            18               195\n#&gt; 4     4  Adelie Torgersen             NA            NA                NA\n#&gt; 5     5  Adelie Torgersen           36.7          19.3               193\n#&gt; 6     6  Adelie Torgersen           39.3          20.6               190\n#&gt;   body_mass_g    sex year\n#&gt; 1        3750   male 2007\n#&gt; 2        3800 female 2007\n#&gt; 3        3250 female 2007\n#&gt; 4          NA     NA 2007\n#&gt; 5        3450 female 2007\n#&gt; 6        3650   male 2007\nBoom! We’ve written ourselves a reader! Let’s do a simple bench mark comparing it to readr.\nlibrary(dfusionrdr)\nbench::mark(\n  datafusion = as.data.frame(read_csv_dfusion(\"inst/penguins.csv\")),\n  readr = readr::read_csv(\"inst/penguins.csv\"),\n  check = FALSE\n)\n#&gt; # A tibble: 2 × 6\n#&gt;   expression      min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 datafusion  922.3µs   1.04ms     915.     1.07MB      0  \n#&gt; 2 readr        14.6ms  15.31ms      65.6    13.2MB     80.8\nInsanely fast!"
  },
  {
    "objectID": "posts/2023-10-28/index.html",
    "href": "posts/2023-10-28/index.html",
    "title": "Where am I in the sky?",
    "section": "",
    "text": "When I was flying back from the Spatial Data Science Across Langauge event from Frankfurt to Atlanta the plane I was bored beyond measure. The plane had no wifi to connect to. I had already watched a movie and couldn’t be bothered by a podcast. I wanted to know where I was.\nWhen looking at the onboard “About this flight” information, they didn’t show a map even. The gave us our coordinates in degrees and minutes. Helpful right?\nWell, in an attempt to figure out where the hell I was I wrote some code. Here it is.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(units)\n\nudunits database from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/units/share/udunits/udunits2.xml\n\n#' Given degrees and minutes calculate the coordinate\n#' in degrees\nas_degree &lt;- function(degrees, minutes) {\n  d &lt;- set_units(degrees, \"arc_degrees\")\n  m &lt;- set_units(minutes, \"arc_minutes\") |&gt; \n    set_units(\"arc_degrees\")\n  d + m\n}\n\n# get the country shapes\nx &lt;- rnaturalearthdata::countries50 |&gt;  st_as_sf() \n\n# filter to North America\nusa &lt;- x |&gt; \n  dplyr::filter(continent == \"North America\", \n                subregion == \"Northern America\") |&gt; \n  st_geometry() \n\n# Create a bounding box to crop myself to \ncrp &lt;- st_bbox(c(xmin = -128, xmax = 0, ymin = 18, ymax = 61))\n\n# plot N. America\nusa |&gt; \n  st_cast(\"POLYGON\") |&gt; \n  st_as_sf() |&gt; \n  st_filter(\n    st_as_sfc(crp) |&gt; \n      st_as_sf(crs = st_crs(usa))\n    ) |&gt;\n  plot()\n\n\n# add planes location.\nplot(\n  st_point(c(-as_degree(61, 19), as_degree(57, 46))),\n  add = TRUE,\n  col = \"red\",\n  pch = 16\n)"
  }
]