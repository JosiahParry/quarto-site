[
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html",
    "title": "genius tutorial",
    "section": "",
    "text": "knitr::opts_chunk$set(eval = FALSE)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#introducing-genius",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#introducing-genius",
    "title": "genius tutorial",
    "section": "Introducing genius",
    "text": "Introducing genius\nYou want to start analysing song lyrics, where do you go? There have been music information retrieval papers written on the topic of programmatically extracting lyrics from the web. Dozens of people have gone through the laborious task of scraping song lyrics from websites. Even a recent winner of the Shiny competition scraped lyrics from Genius.com.\nI too have been there. Scraping websites is not always the best use of your time. genius is an R package that will enable you to programatically download song lyrics in a tidy format ready for analysis. To begin using the package, it first must be installed, and loaded. In addition to genius, we will need our standard data manipulation tools from the tidyverse.\n\ninstall.packages(\"genius\")\n\n\nlibrary(genius)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#single-song-lyrics",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#single-song-lyrics",
    "title": "genius tutorial",
    "section": "Single song lyrics",
    "text": "Single song lyrics\nThe simplest method of extracting song lyrics is to get just a single song at a time. This is done with the genius_lyrics() function. It takes two main arguments: artist and song. These are the quoted name of the artist and song. Additionally there is a third argument info which determines what extra metadata you can get. The possible values are title, simple, artist, features, and all. I recommend trying them all to see how they work.\nIn this example we will work to retrieve the song lyrics for the upcoming musician Renny Conti.\n\nfloating <- genius_lyrics(\"renny conti\", \"people floating\")\nfloating"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#album-lyrics",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#album-lyrics",
    "title": "genius tutorial",
    "section": "Album Lyrics",
    "text": "Album Lyrics\nNow that you have the intuition for obtaining lyrics for a single song, we can now create a larger dataset for the lyrics of an entire album using genius_album(). Similar to genius_lyrics(), the arguments are artist, album, and info.\nIn the exercise below the lyrics for Snail Mail‚Äôs album Lush. Try retrieving the lyrics for an album of your own choosing.\n\nlush <- genius_album(\"Snail Mail\", \"Lush\")\nlush"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#adding-lyrics-to-a-data-frame",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#adding-lyrics-to-a-data-frame",
    "title": "genius tutorial",
    "section": "Adding Lyrics to a data frame",
    "text": "Adding Lyrics to a data frame\n\nMultiple songs\nA common use for lyric analysis is to compare the lyrics of one artist to another. In order to do that, you could potentially retrieve the lyrics for multiple songs and albums and then join them together. This has one major issue in my mind, it makes you create multiple object taking up precious memory. For this reason, the function add_genius() was developed. This enables you to create a tibble with a column for an artists name and their album or song title. add_genius() will then go through the entire tibble and add song lyrics for the tracks and albums that are available.\nLet‚Äôs try this with a tibble of three songs.\n\nthree_songs <- tribble(\n  ~ artist, ~ title,\n  \"Big Thief\", \"UFOF\",\n  \"Andrew Bird\", \"Imitosis\",\n  \"Sylvan Esso\", \"Slack Jaw\"\n)\n\nsong_lyrics <- three_songs %>% \n  add_genius(artist, title, type = \"lyrics\")\n\nsong_lyrics %>% \n  count(artist)\n\n\n\n\nMultiple albums\nadd_genius() also extends this functionality to albums.\n\nalbums <- tribble(\n  ~ artist, ~ title,\n  \"Andrew Bird\", \"Armchair Apocrypha\",\n  \"Andrew Bird\", \"Things are really great here sort of\"\n)\n\nalbum_lyrics <- albums %>% \n  add_genius(artist, title, type = \"album\")\n\nalbum_lyrics\n\nWhat is important to note here is that the warnings for this function are somewhat informative. When a 404 error occurs, this may be because that the song does not exist in Genius. Or, that the song is actually an instrumental which is the case here with Andrew Bird.\n\n\nAlbums and Songs\nIn the scenario that you want to mix single songs and lyrics, you can supply a column with the type value of each row. The example below illustrates this. First a tibble with artist, track or album title, and type columns are created. Next, the tibble is piped to add_genius() with the unquote column names for the artist, title, and type columns. This will then iterate over each row and fetch the appropriate song lyrics.\n\nsong_album <- tribble(\n  ~ artist, ~ title, ~ type,\n  \"Big Thief\", \"UFOF\", \"lyrics\",\n  \"Andrew Bird\", \"Imitosis\", \"lyrics\",\n  \"Sylvan Esso\", \"Slack Jaw\", \"lyrics\",\n  \"Movements\", \"Feel Something\", \"album\"\n)\n\nmixed_lyrics <- song_album %>% \n  add_genius(artist, title, type)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#self-similarity",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#self-similarity",
    "title": "genius tutorial",
    "section": "Self-similarity",
    "text": "Self-similarity\nAnother feature of genius is the ability to create self-similarity matrices to visualize lyrical patterns within a song. This idea was taken from Colin Morris‚Äô wonderful javascript based Song Sim project. Colin explains the interpretation of a self-similarity matrix in their TEDx talk. An even better description of the interpretation is available in this post.\nTo use Colin‚Äôs example we will look at the structure of Ke$ha‚Äôs Tik Tok.\nThe function calc_self_sim() will create a self-similarity matrix of a given song. The main arguments for this function are the tibble (df), and the column containing the lyrics (lyric_col). Ideally this is one line per observation as is default from the output of genius_*(). The tidy output compares every ith word with every word in the song. This measures repetition of words and will show us the structure of the lyrics.\n\ntik_tok <- genius_lyrics(\"Ke$ha\", \"Tik Tok\")\n\ntt_self_sim <- calc_self_sim(tik_tok, lyric, output = \"tidy\")\n\ntt_self_sim\n\ntt_self_sim %>% \n  ggplot(aes(x = x_id, y = y_id, fill = identical)) +\n  geom_tile() +\n  scale_fill_manual(values = c(\"white\", \"black\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        axis.text = element_blank()) +\n  scale_y_continuous(trans = \"reverse\") +\n  labs(title = \"Tik Tok\", subtitle = \"Self-similarity matrix\", x = \"\", y = \"\", \n       caption = \"The matrix displays that there are three choruses with a bridge between the last two. The bridge displays internal repetition.\")"
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html",
    "title": "Medium Data and Production API Pipeline",
    "section": "",
    "text": "‚Äú[P]arsing huge json strings is difficult and inefficient.‚Äù1 If you have an API that needs to receive a large amount of json, sending that over will be slow.1¬†https://www.opencpu.org/posts/jsonlite-streaming/\nQ: How can we improve that? A: Compression."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#background",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#background",
    "title": "Medium Data and Production API Pipeline",
    "section": "Background",
    "text": "Background\nAn API is an application programming interface. APIs are how machines talk to other machines. APIs are useful because they are language agnostic meaning that the same API request from Python, or R, or JavaScript will work and return the same results. To send data to an API we use a POST request. The data that we send is usually required to be in json format."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#context",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#context",
    "title": "Medium Data and Production API Pipeline",
    "section": "Context",
    "text": "Context\nProblem: With large data API POST requests can become extremely slow and take up a lot of storage space. This can cause a bottleneck.\nSolution: Compress your data and send a file instead of sending plain text json."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#standard-approach",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#standard-approach",
    "title": "Medium Data and Production API Pipeline",
    "section": "Standard approach",
    "text": "Standard approach\nInteracting with an API from R is usually done with the {httr} package. Imagine you want to send a dataframe to an API as json. We can do that by using the httr::POST(), providing a dataframe to the body, and encoding it to json by setting encode = \"json\".\nFirst let‚Äôs load our libraries:\n\nlibrary(httr)          # interacts with apis\nlibrary(jsonlite)      # works with json (for later)\nlibrary(nycflights13)  # data for posting \n\nNext, let‚Äôs create a sample POST() request to illustrate how posting a dataframe as json works.\n\n\nb_url <- \"http://httpbin.org/post\" # an easy to work with sample API POST endpoint\n\nPOST(url = b_url, \n     body = list(x = cars),\n     encode = \"json\")\n#> Response [http://httpbin.org/post]\n#>   Date: 2022-11-14 22:14\n#>   Status: 200\n#>   Content-Type: application/json\n#>   Size: 4.81 kB\n#> {\n#>   \"args\": {}, \n#>   \"data\": \"{\\\"x\\\":[{\\\"speed\\\":4,\\\"dist\\\":2},{\\\"speed\\\":4,\\\"dist\\\":10},{\\\"speed\\\":7,\\\"...\n#>   \"files\": {}, \n#>   \"form\": {}, \n#>   \"headers\": {\n#>     \"Accept\": \"application/json, text/xml, application/xml, */*\", \n#>     \"Accept-Encoding\": \"deflate, gzip\", \n#>     \"Content-Length\": \"1150\", \n#>     \"Content-Type\": \"application/json\", \n#> ..."
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#alternative-approach",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#alternative-approach",
    "title": "Medium Data and Production API Pipeline",
    "section": "Alternative approach",
    "text": "Alternative approach\nAn alternative approach would be to write our dataframe as json to a compressed gzip file. The process will be to:\n\nCreate a temporary file which will store our compressed json.\nCreate a gzip file connection to write the temporary file as a gzip.\nUpload the temporary file to the API.\nRemove the temporary file.\n\nWriting to a temporary gzipped file looks like:\n\n# create the tempfile \ntmp <- tempfile()\n\n# create a gzfile connection (to enable writing gz)\ngz_tmp <- gzfile(tmp)\n\n# write json to the gz file connection\nwrite_json(cars, gz_tmp)\n\n# close the gz file connection\nclose(gz_tmp)\n\nLet‚Äôs read the temp file to see what it contains.\n\n# read the temp file for illustration \nreadLines(tmp)\n#> [1] \"[{\\\"speed\\\":4,\\\"dist\\\":2},{\\\"speed\\\":4,\\\"dist\\\":10},{\\\"speed\\\":7,\\\"dist\\\":4},{\\\"speed\\\":7,\\\"dist\\\":22},{\\\"speed\\\":8,\\\"dist\\\":16},{\\\"speed\\\":9,\\\"dist\\\":10},{\\\"speed\\\":10,\\\"dist\\\":18},{\\\"speed\\\":10,\\\"dist\\\":26},{\\\"speed\\\":10,\\\"dist\\\":34},{\\\"speed\\\":11,\\\"dist\\\":17},{\\\"speed\\\":11,\\\"dist\\\":28},{\\\"speed\\\":12,\\\"dist\\\":14},{\\\"speed\\\":12,\\\"dist\\\":20},{\\\"speed\\\":12,\\\"dist\\\":24},{\\\"speed\\\":12,\\\"dist\\\":28},{\\\"speed\\\":13,\\\"dist\\\":26},{\\\"speed\\\":13,\\\"dist\\\":34},{\\\"speed\\\":13,\\\"dist\\\":34},{\\\"speed\\\":13,\\\"dist\\\":46},{\\\"speed\\\":14,\\\"dist\\\":26},{\\\"speed\\\":14,\\\"dist\\\":36},{\\\"speed\\\":14,\\\"dist\\\":60},{\\\"speed\\\":14,\\\"dist\\\":80},{\\\"speed\\\":15,\\\"dist\\\":20},{\\\"speed\\\":15,\\\"dist\\\":26},{\\\"speed\\\":15,\\\"dist\\\":54},{\\\"speed\\\":16,\\\"dist\\\":32},{\\\"speed\\\":16,\\\"dist\\\":40},{\\\"speed\\\":17,\\\"dist\\\":32},{\\\"speed\\\":17,\\\"dist\\\":40},{\\\"speed\\\":17,\\\"dist\\\":50},{\\\"speed\\\":18,\\\"dist\\\":42},{\\\"speed\\\":18,\\\"dist\\\":56},{\\\"speed\\\":18,\\\"dist\\\":76},{\\\"speed\\\":18,\\\"dist\\\":84},{\\\"speed\\\":19,\\\"dist\\\":36},{\\\"speed\\\":19,\\\"dist\\\":46},{\\\"speed\\\":19,\\\"dist\\\":68},{\\\"speed\\\":20,\\\"dist\\\":32},{\\\"speed\\\":20,\\\"dist\\\":48},{\\\"speed\\\":20,\\\"dist\\\":52},{\\\"speed\\\":20,\\\"dist\\\":56},{\\\"speed\\\":20,\\\"dist\\\":64},{\\\"speed\\\":22,\\\"dist\\\":66},{\\\"speed\\\":23,\\\"dist\\\":54},{\\\"speed\\\":24,\\\"dist\\\":70},{\\\"speed\\\":24,\\\"dist\\\":92},{\\\"speed\\\":24,\\\"dist\\\":93},{\\\"speed\\\":24,\\\"dist\\\":120},{\\\"speed\\\":25,\\\"dist\\\":85}]\"\n\n\nPOSTing a file\nTo post a file we use the function httr::upload_file(). The argument we provide is the path, in this case the file path is stored in the tmp object.\n\nPOST(b_url, body = list(x = upload_file(tmp)))\n#> Response [http://httpbin.org/post]\n#>   Date: 2022-11-14 22:14\n#>   Status: 200\n#>   Content-Type: application/json\n#>   Size: 874 B\n#> {\n#>   \"args\": {}, \n#>   \"data\": \"\", \n#>   \"files\": {\n#>     \"x\": \"data:text/plain;base64,H4sIAAAAAAAAA4XSPQ6DMAwF4L3HyMyQ+C8JV6m6wdCtEt0q7t6p...\n#>   }, \n#>   \"form\": {}, \n#>   \"headers\": {\n#>     \"Accept\": \"application/json, text/xml, application/xml, */*\", \n#>     \"Accept-Encoding\": \"deflate, gzip\", \n#> ...\n\n\n\nComparing R object to gzip\nNow, you may be asking, is this really that big of a difference? It actually is. If you‚Äôll notice from the first response where we POSTed the cars dataframe the response size was 4.81kB. This response with the compressed file was only 870B. Thats a whole lot smaller.\nWe can compare the object size to the file size for another look. The below is in bytes.\n\ncat(\" cars: \", object.size(cars), \"\\n\",\n    \"compressed cars: \", file.size(tmp))\n#>  cars:  1648 \n#>  compressed cars:  210"
  },
  {
    "objectID": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#benchmarking",
    "href": "posts/2020-09-05-medium-data-and-production-api-pipeline.html#benchmarking",
    "title": "Medium Data and Production API Pipeline",
    "section": "Benchmarking",
    "text": "Benchmarking\nLet‚Äôs extend this example to some larger datasets as well as benchmark the results. We‚Äôll use data from nycflights13. In this example we‚Äôll send two dataset to an API as the parameters metadata and data. Generally metadata is smaller than the data. So for this example we‚Äôll send 1,000 rows as the metadata and 10,000 rows as the data. We‚Äôll call on the weather and flights datasets from nycflights13.\n\nsmall_weather <- dplyr::sample_n(weather, 1000)\nsmall_flights <- dplyr::sample_n(flights, 10000)\n\n\nMaking it functional\nAs always, I recommend making your repetitive tasks into functions. Here we will create two functions. One for posting the data as gzip files and the second as pure json. These will be labeled post_gz() and post_json() respectively.\nThese functions will take two parameters: metadata and data.\nDefine post_gz()\n\npost_gz <- function(metadata, data) {\n  \n  # write metadata to temp file\n  tmp_meta <- tempfile(\"metadata\")\n  gz_temp_meta <- gzfile(tmp_meta)\n  write_json(metadata, gz_temp_meta)\n  close(gz_temp_meta)\n  \n  # write data to temp file\n  tmp_data <- tempfile(\"data\")\n  gz_temp_data <- gzfile(tmp_data)\n  write_json(data, gz_temp_data)\n  close(gz_temp_data)\n  \n  # post \n  q <- POST(b_url, \n       body = list(\n         metadata = upload_file(tmp_meta),\n         data = upload_file(tmp_data)\n       ))\n  \n  # remove temp files\n  unlink(tmp_meta)\n  unlink(tmp_data)\n  \n  # return a character for purposes of bench marking\n  \"Posted...\"\n}\n\nDefine post_json().\n\npost_json <- function(metadata, data) {\n  q <- POST(b_url, \n       body = list(\n         metadata = metadata,\n         data = data\n       ),\n       encode = \"json\") \n  \n  \"Posted...\"\n}\n\nNow that these functions have been defined, let‚Äôs compare their performance using the package bench. We‚Äôll run each function 50 times to get a good understanding of their respective performance.\n\nbm <- bench::mark(\n  post_gz(small_weather, small_flights),\n  post_json(small_weather, small_flights),\n  iterations = 5\n  )\n\nbm\n\n\nggplot2::autoplot(bm)"
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html",
    "href": "posts/2023-07-06-r-is-still-fast.html",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "",
    "text": "There‚Äôs this new blog post making the rounds making some claims about why they won‚Äôt put R into production. Most notably they‚Äôre wheeling the whole ‚ÄúR is slow thing‚Äù again. And there are few things that grind my gears more than that type of sentiment. It‚Äôs almost always ill informed. I find that to be the case here too.\nI wouldn‚Äôt have known about this had it 1) not mentioned my own Rust project Valve and 2) a kind stranger inform me about it on mastodon.\nI‚Äôve collected my reactions below as notes and sundry bench marks and bullet points."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#tldr",
    "href": "posts/2023-07-06-r-is-still-fast.html#tldr",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "TL;DR",
    "text": "TL;DR\n\nThere is a concurrent web server for R and I made it Valve\n\nPython is really fast at serializing json and R is slower\nPython is really slow at parsing json and R is so so soooo much faster\nTo handle types appropriately, sometimes you have to program\nThere are mock REST API testing libraries {httptest} and {webmockr}\n\nDemand your service providers to make the tools you want\nAsk and you shall receive\nR can go into production\nPLEASE JUST TRY VALVE YOU‚ÄôLL LOVE IT"
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#production-services",
    "href": "posts/2023-07-06-r-is-still-fast.html#production-services",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Production Services",
    "text": "Production Services\nThere are so many people using R in production in so many ways across the world. I wish Posit did a better job getting these stories out. As a former RStudio employee, I personally met people putting R in production in most amazing ways. From the US Department of State, Defense, Biotech companies, marketing agencies, national lotteries, and so much more. The one that sticks out the most is that Payam M., when at Tabcorp massively scaled their system using Plumber APIs and Posit Connect to such a ridiculous scale I couldn‚Äôt even believe.\nGunicorn, Web Servers, and Concurrency\n\n‚ÄúR has no widely-used web server to help it run concurrently.‚Äù\n\nThe premise of this whole blog post stems from the fact that there is no easily concurrent web server for R. Which is true and is the reason I built Valve. It doesn‚Äôt meet the criteria of widely used because no one has used it. In part, because of posts like this that discourage people from using R in production.\nTypes and Conversion\nThere‚Äôs this weird bit about how 1 and c(1, 2) are treated as the same class and unboxing of json. They provide the following python code as a desirable pattern for processing data.\nx = 1\ny = [1, 2]\n\njson.dump(x, sys.stdout)\n#> 1\njson.dump(y, sys.stdout)\n#> [1, 2]\nThey want scalars to be unboxed and lists to remain lists. This is the same behavior as jsonlite, though.\n\njsonlite::toJSON(1, auto_unbox = TRUE)\n\n1 \n\njsonlite::toJSON(1:2, auto_unbox = TRUE)\n\n[1,2] \n\n\nThere‚Äôs a difference here: one that the author fails to recognize is that a length 1 vector is handled appropriately. What the author is saying is that they don‚Äôt like that R doesn‚Äôt behave the same way as Python. You, as a developer should be able to guarantee that a value is length 1. It‚Äôs easy. length(x) == 1, or if you want is_scalar <- function(x) length(x) == 1. This is the type system in R and json libraries handle the ‚Äúedge case‚Äù appropriately. There is nothing wrong here. The reprex is the same as the python library.\n\n‚ÄúR (and Plumber) also do not enforce types of parameters to your API, as opposed to FastAPI, for instance, which does via the use of pydantic.‚Äù\n\nPython does not type check nor does FastAPI. You opt in to type checking with FastAPI. You can do the same with Plumber. A quick perusal of the docs will show you this. Find the @param section. There is some concessions here, though. The truthful part here is the type annotations do type conversion for only dynamic routes. Which, I don‚Äôt know if FastAPI does. Type handling for static parameters is an outstanding issue of mine for plumber since 2021.\nI‚Äôve followed up on the issue above and within minutes the maintainer responded. There is an existing PR to handle this issue.\nThis just goes to show if that you want something done in the open source world, just ask for it. More than likely its already there or just waiting for the slight nudge from someone else.\nWhile I know it‚Äôs not ‚Äúseemless‚Äù adding an as.integer() and a stopifnot(is.integer(n)) isn‚Äôt the wildest thing for a developer to do.\nThere is a comparison between type checking in R and Python with the python example using type hints which are, again, opt-in. An unfair comparison when you say ‚Äúif you don‚Äôt use the opt-in features of plumber but use the opt-in features of FastAPI, FastAPI is better.‚Äù\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/types\")\nasync def types(n: int) -> int:\n  return n * 2\nClients and Testing\nI haven‚Äôt done much testing of API endpoints but I do know that there are two de facto packages for this:\n\n\n{httptest} and\n\n{webmockr}.\n\nThese are pretty easy to find. Not so sure why they weren‚Äôt mentioned or even tested.\nPerformance\nJSON serialization is a quite interesting thing to base performance off of. I‚Äôve never seen how fast pandas serialization is. Quite impressive! But, keep with me, because you‚Äôll see, this is fibbing with benchmarks.\nI do have thoughts on the use of jsonlite and it‚Äôs ubiquity. jsonlite is slow. I don‚Äôt like it. My belief is that everyone should use {jsonify} when creating json. It‚Äôs damn good.\nSo, when I run these bench marks on my machine for parsing I get:\n\nmicrobenchmark::microbenchmark(\n  jsonify = jsonify::to_json(iris),\n  jsonlite = jsonlite::toJSON(iris),\n  unit = \"ms\", \n  times = 1000\n)\n\nWarning in microbenchmark::microbenchmark(jsonify = jsonify::to_json(iris), :\nless accurate nanosecond times to avoid potential integer overflows\n\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\nUnit: milliseconds\n     expr      min       lq      mean    median       uq      max neval cld\n  jsonify 0.258218 0.265024 0.3224672 0.2698005 0.280850 35.20330  1000  a \n jsonlite 0.346245 0.360759 0.4169715 0.3719110 0.399012 20.00181  1000   b\n\n\nA very noticable difference in using jsonify over jsonlite. The same benchmark using pandas is holy sh!t fast!\nfrom timeit import timeit\nimport pandas as pd\n\niris = pd.read_csv(\"fastapi-example/iris.csv\")\n\nN = 1000\n\nprint(\n  \"Mean runtime:\", \n  round(1000 * timeit('iris.to_json(orient = \"records\")', globals = locals(), number = N) / N, 4), \n  \"milliseconds\"\n)\n#> Mean runtime: 0.0721 milliseconds\nNow, this is only half the story. This is serialization. What about the other part? Where you ingest it.\nHere, I will also say, again, that you shouldn‚Äôt use jsonlite because it is slow. Instead, you should use {RcppSimdJson}. Because its\n\n\nLet‚Äôs run another benchmark\n\njsn <- jsonify::to_json(iris)\n\nmicrobenchmark::microbenchmark(\n  simd = RcppSimdJson::fparse(jsn),\n  jsonlite = jsonlite::fromJSON(jsn),\n  unit = \"ms\",\n  times = 1000\n)\n\nUnit: milliseconds\n     expr      min        lq       mean   median        uq      max neval cld\n     simd 0.052275 0.0551040 0.06672631 0.057933 0.0634885 4.316275  1000  a \n jsonlite 0.433165 0.4531525 0.48931155 0.467359 0.4919795 4.352232  1000   b\n\n\nRcppSimdJson is ~8 times faster than jsonlite.\nLet‚Äôs do a similar benchmark in python.\njsn = iris.to_json(orient = \"records\")\n\nprint(\n  \"Mean runtime:\", \n  round(1000 * timeit('pd.read_json(jsn)', globals = locals(), number = N) / N, 4), \n  \"milliseconds\"\n)\n#> Mean runtime: 1.2629 milliseconds\nPython is 3x slower than jsonlite in this case and 25x slower than RcppSimdJson. Which is very slow. While serializing is an important thing to be fast in, so is parsing the incoming json you are receiving. How nice it is to show only half the story! Use RcppSimdJson and embarrass pandas‚Äô json parsing.\nIntegration with Tooling\nI have literally no idea about any of these except Launchdarkly because one of my close homies worked there for years. These are all paid services so I‚Äôm not sure how they work :)\nI would say to checkout Posit Connect for deploying R and python into production. But if your only use case is to deploy a single model, then yeah, I‚Äôd say that‚Äôs overkill.\nI wish more companies would create tooling for R and their services. The way to do this, is to lean into using R in production and demanding (not asking) providers to make wrappers for them. When you pay for a service, you have leverage. Use it. I think too many people fall over when what they need isn‚Äôt there immediately. Be sure to be the squeeky wheel that makes change.\nI also think that if you‚Äôre in the position where you can make a wrapper for something, you should. I did this when using Databricks in my last role and provided them with a lot of feedback. Have they taken it? I‚Äôm not sure. I‚Äôm not there to harass them anymore."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#workarounds",
    "href": "posts/2023-07-06-r-is-still-fast.html#workarounds",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Workarounds",
    "text": "Workarounds\nThese are good workarounds. I would suggest looking at ndexr.io as a way to scale these R based services as well. They utilize the NGINX approach described here."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#addenda",
    "href": "posts/2023-07-06-r-is-still-fast.html#addenda",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Addenda",
    "text": "Addenda\nClearly, this is where I care a lot. I am the author of Valve. Valve is exactly what the author was clamoring for in the beginning of the blog post. It is a web server that runs Plumber APIs in parallel written in Rust using Tokio, Axum, and Deadpool. Valve auto-scales on its own up to a maximum number of worker threads. So it‚Äôs not always taking up space and running more compute than it needs.\nValve overview:\n\nConcurrent webserver to auto-scale plumber APIs\nwritten in Rust using Tokio, Axum, and Deadpool\nspawns and kills plumber APIs based on demand\nintegration with {vetiver} of of the box\n\nFirst things first, I want to address ‚Äúit‚Äôs not on CRAN.‚Äù You‚Äôre right. That‚Äôs because it is a Rust crate. Crates don‚Äôt go on CRAN. I‚Äôve made an R package around it to lower the bar to entry. But it is a CLI tool at the core.\nObviously, it is new. It is untested. I wish I could tell everyone to use it, but I can‚Äôt. I think anyone who used it would be floored by its performance and ease of use. It is SO simple.\nI‚Äôll push it to crates.io and CRAN in the coming weeks. Nothing like h8rs to inspire."
  },
  {
    "objectID": "posts/2019-12-14-spss-haven.html",
    "href": "posts/2019-12-14-spss-haven.html",
    "title": "Finding an SPSS {haven}",
    "section": "",
    "text": "Note (2022-11-14): the dataset that was used can no longer be access from the url I provided in this blog post. I have found it at a zip file in a blog post at https://blog.faradars.org/wp-content/uploads/2020/07/noisedata.zip if you want to try downloading it from there.\nMy education as a social scientist‚Äîundergratuate studies in sociology and anthropology‚Äîwas largely focused on theory and the application of theory to social problems. For the most part, I taught myself how to apply those methods through R. I was fortunate enough to have avoided ever using SPSS. Perhaps that is good. Perhaps it is not. The use of R in the social sciences is increasing and I will go as far as to say that that is great news. However, there are still holdouts.\nVery recently I came across data exported from SPSS in the wild. In the work that I get to engage in at the Boston Area Research Initiative (BARI) we receive data from various sources, whether these be municipal organizations, the police department, or non-profits, and as Tolstoy said:\nIn this post I want to illustrate two of the main pain points I encountered while working with data from SPSS.I also go rather deep into the weeds about R data structures and how we can manipulate them. While this is ‚Äúabout‚Äù SPSS data, folks looking for a better understanding of R object may benefit from this as well (specifically Gripe 2)."
  },
  {
    "objectID": "posts/2019-12-14-spss-haven.html#haven",
    "href": "posts/2019-12-14-spss-haven.html#haven",
    "title": "Finding an SPSS {haven}",
    "section": "{haven}",
    "text": "{haven}\nI am not sure where the inspiration for the name ‚Äúhaven‚Äù came from. But I am sure that its name does indeed speak for itself. haven enables R programmers that are SPSS‚ÄîStata and SAS as well‚Äîilliterate to work with data that is not naturally intended for R use. There are two key behaviors of SPSS data that I was unaware of and that plagued me. I break these down into three gripes below."
  },
  {
    "objectID": "posts/2019-12-14-spss-haven.html#gripes",
    "href": "posts/2019-12-14-spss-haven.html#gripes",
    "title": "Finding an SPSS {haven}",
    "section": "Gripes",
    "text": "Gripes\nI maintain three gripes about SPSS data.\n\nFactor values are represented numerically and the values that they represent are stored in metadata.\nColumn names are vague. The values they represent are stored as a label (metadata).\nMissing values can be represented innumerably. Each column can have user defined missing values, i.e.¬†9999.\n\nNow, I must say that I have found it best practice to try and combat my own gripes.\nIn regards to the former two gripes, this is not unheard of behavior nor is it rare. Associating numeric values with related values‚Äîoh, I don‚Äôt know‚Ä¶think of a user ID and an email address‚Äîis in essence the core of relational database systems (RDBMS). One may even have the gall to argue that RDBMS power many of the tools I use and take for granted. I would most likely be willing to concede that point.\nThe third gripe can be quite easily countered if I am to be frank. Missing data is in of itself data. An NA is an NA is an NA may not be a generalizable statement. Providing values such as 9999 in place of a missing value in some cases may be a relic of antiquity where missingness could not be handled by software. Or, perhaps we can frame missingness other ways. Let‚Äôs imagine we are conducting a study and we want to keep track of why there was missing data. This could have been from a non-response, or a withdrawal, or erroneously entered data, or any other number of reasons. Keeping a record of that missing data may useful.\n\nGripe 1: numbers representing characters (or labels)\nSometimes I really would like stringsAsFactors = TRUE. Working with survey data tends to be one of those times. R has a robust method of creating, handling, and manipulating factors1 and because of this, we aren‚Äôt required to numerically encode our data. This may be a personal preference, but I really like to be reminded of what I am working with and seeing the factor levels clearly spelled out for me is quite nice.1¬†Check out forcats for working with factors.\nSince the data I am working with at BARI is confidential, I‚Äôve found some SPSS data hosted by the University of Bath2 to illustrate this with.2¬†Anthony Horowitz wrote a rather fun murder mystery novel titled Magpie Murders which takes place in a small town outside of bath. I recommend it.\nReading in data is rather straightforward. SPSS data come with a .sav file extension. Following standard readr convention we can read a .sav file with haven::read_sav().\nNote: the syntax above is used for referencing an exported function from a namespace (package name). The syntax is pkgname::function().\nThe below line reads in the sav file from the University of Bath.\nNote: by wrapping an object assignment expression, such as the below, in parentheses the object is then printed (I just recently figured this out).\n\nlibrary(haven)\n\n# read in sav file\n(noise <- haven::read_sav(\"http://staff.bath.ac.uk/pssiw/stats2/noisedata.sav\"))\n\nThe above shows GENDER codes as a numeric value. But if you print out the tibble to your console you will see labels. So in this case, where there is a 1 under GENDER, the printed console shows [male], and the same is true for 2 and [female]. We can get a sense of this by viewing the structure of the tibble.\n\nstr(noise)\n\nAbove we can see that GENDER has an attribute labels which is a named numeric vector. The unique values are 1 and 2 representing ‚Äúmale‚Äù and ‚Äúfemale‚Äù respectively. I struggle to keep this mental association. I‚Äôd prefer to have this shown explicitly. Fortunately, haven provides the function haven::as_factor() which will convert these pesky integer columns to their respective factor values. We just need to pass the data frame as the only argument‚Äîsure, there are other arguments if you want to get fancy.\n\n# selecting just 2 columns and 2 rows for simlicity.\nsmall_lil_df <- slice(noise, 1, 20) %>% \n  select(1, 2)\n\n# convert coded response to response text\nhaven::as_factor(small_lil_df)\n\nAnd now we can just forget those integers ever existed in the firstplace!\n\n\nGripe 2: uninformative column names\nThere are three key rules to tidy data.\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nEach variable forms a column. Got it. So this means that any thing that can help describe our observation should be a column. Say we have a table of survey respondents. In this case each row should be a respondent and each column should be a variable (or feature, or predictor, or x, or whatever) associate with that respondent. This could be something like age, birth date, or the respondents response to a survey question.\nIn the tidyverse style guide Hadley Wickham writes\n\nGenerally, variable names should be nouns and function names should be verbs. Strive for names that are concise and meaningful (this is not easy!).\n\nI personaly try to extend this to column names as well. Feature names are important so I, as a researcher, can remember what is what. From my encounters with SPSS data, I‚Äôve found that feature names can be rather uninformative e.g.¬†‚ÄúQ12.‚Äù Much like factors, columns may have associated information hidden somewhere within them.\nWe read in the personality data set from the Universit of Bath below.\n\n# read in sav file with column labels\n(personality <- haven::read_sav(\"http://staff.bath.ac.uk/pssiw/stats2/personality.sav\"))\n\nThe first thing that I notice is rather apalling: each column represents a person. oof, untidy. But this issue isn‚Äôt what brought me to these data. If you print the data to the console, you see something similar as what is above. If you view (View(df)) the data, the story is different. There is associated information underneath each column header.\n\nstr(personality[,1:5])\n\nYikes. These labels seem to be character judgements! Whatever the labels represent, I want them, and I want them as the column headers.\nFrom looking at the structure of the data frame we can glean that each column has a label.\nWarning: I‚Äôm going to walk through a fair bit of the theory and under the hood work to make these labels column names. Scroll to the bottom of this section to find the function definition.\n\nAside: R theory:\n\nEach column of a data frame is actually just a vector. Each vector can have it‚Äôs own attributes (as above).\nA data frame is actually just a list of equal length vectors (same number of observations).3\n‚ÄúAll objects can have arbitrary additional attributes, used to store metadata about the object.‚Äù4\nWe can fetch list elements using [[ notation, e.g.¬†my_list[[1]]\n\npurrr::pluck() is an alternative to using [[ for grabbing the underlying elements inside of a data structure. This means we can use pluck(my_list, 1) in place of my_list[[1]]\n\n\n3¬†http://adv-r.had.co.nz/Data-structures.html#data-frames4¬†http://adv-r.had.co.nz/Data-structures.html#attributesOkay, but how does one actually get the label from the vector? The first step is to actually grab the vector. Below I use purrr::pluck()to pull the first column. Note that slice() is used for grabing specific row indexes. The below code is equivalent to personality[1:10,][[1]]. I prefer using the purrr functions because they are more legible.\n\ncol_1 <- slice(personality, 1:10) %>% \n  pluck(1)\n\nWe can access all of the vectors attributes with, you guessed it, the attributes() function.\n\nattributes(col_1)\n\nThis returns a named list. We can access (or set) specific attributes using the attr() function. The two arguments we must supply are x, the object, and which, which attribute we seek. In this case the values are col_1 and label respectively.\n\nattr(col_1, \"label\")\n\npurrr yet again makes working with list objects easy. purrr exports a function factory5 called purrr::attr_getter(). This function generates a function which accesses specific attributes. We can create a function get_label() using attr_getter() all we have to do is tell it which attribute we would like.5¬†A function factory is an object which creates other function objects.\n\n# generate a get_label f(x) via purr\nget_label <- purrr::attr_getter(\"label\")\n\nget_label(col_1)\n\nWell, lovely. Let‚Äôs just use this on our personality data frame.\n\nget_label(personality)\n\nOpe. Welp. That didn‚Äôt work. We should just give up ü§∑.\n\nThe reason this didn‚Äôt work is because we tried to use get_label() on a tibble which didn‚Äôt have the ‚Äúlabel‚Äù attribute. We can verify this by looking at the list names of the attributes of personality.\n\nnames(attributes(personality))\n\nBut what about the attribtues of each column? We can iterate over each column using map() to look at the attributes. Below I iterate over the first five columns. More on map()6.6¬†https://adv-r.hadley.nz/functionals.html#map\n\nmap(select(personality, 1:5), ~names(attributes(.)))\n\nWell, now that we‚Äôve iterated over the columns and illustrated that the attributes live there, why not iterate over the columns and use get_label()?\n\n# use get_label to retrieve column labels\nmap_chr(select(personality, 1:5), get_label)\n\nAgain, yikes @ the labels. Let‚Äôs store these results into a vector so we can rename the original columns.\n\npers_labels <- map_chr(personality, get_label)\n\nWe can now change the names using setNames() from base R. We will then make the column headers tidy (personal definition of tidy column names) using janitor::clean_names().\n\nsetNames(personality, pers_labels) %>% \n  janitor::clean_names() %>% \n  select(1:5)\n\nIn the case that a column doesn‚Äôt have a label, get_label() will return NULL and then setNames() will fail. To work around this, you can use the name of the column rather than the label value. Below is a function definition which handles this for you and, optionally, lets you specify which columns to rename based on a regex pattern. I think we‚Äôve done enough list manipulation for the day. If you have questions about the function definition I‚Äôd be happy to work through it one on one with you via twitter DMs.\n\nlabel_to_colname <- function(df, pattern) {\n  get_label <- purrr::attr_getter(\"label\")\n  col_labels <- purrr::map(df, get_label)\n\n  col_labels[unlist(map(col_labels, is.null))]  <- names(col_labels[unlist(purrr::map(col_labels, is.null))])\n\n  if (missing(pattern)) {\n    names_to_replace <- rep(TRUE, ncol(df))\n  } else {\n    names_to_replace <- stringr::str_detect(names(col_labels), pattern)\n  }\n  \n  colnames(df)[names_to_replace] <- janitor::make_clean_names(unlist(col_labels[names_to_replace]))\n\n  haven::zap_label(df)\n}\n\nlabel_to_colname(personality) %>% \n  select(1:5)\n\n# heh.\nlabel_to_colname(personality, \"PERS37\") %>% \n  select(37)\n\n\n\n\nGripe 3: user defined missing values\nI‚Äôll keep this one short. If there are user defined missing values in a .sav file, you can encode these as NA by setting the user_na arugment to TRUE.\n\n# there aren't any missing values but you get the idea\nnoise <- haven::read_sav(\"http://staff.bath.ac.uk/pssiw/stats2/noisedata.sav\",\n                         user_na = TRUE)\n\nAnd if for any reason that did not suffice, you can replace missing values with replace_na()7.7¬†https://tidyr.tidyverse.org/reference/replace_na.html"
  },
  {
    "objectID": "posts/2019-12-14-spss-haven.html#take-aways",
    "href": "posts/2019-12-14-spss-haven.html#take-aways",
    "title": "Finding an SPSS {haven}",
    "section": "Take aways",
    "text": "Take aways\n\nAll data is messy in it‚Äôs own way.\nhaven::read_sav() will read SPSS data.\nhaven::as_factor() will apply column labels in place of the numeric values (if present).\n\nReplace user defined NA values by setting user_na = TRUE i.e.¬†haven::read_sav(\"filepath.sav\", user_na = TRUE)\nAll R objects can have attributes.\nYou can access attributes using attributes() or attr().\nData frames are made of vectors.\nData frames are actually just lists masquerading as rectangles."
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "",
    "text": "Installing python has never been an easy task for me. I remember back in 2016 I wanted to learn how to use pyspark and thus python, I couldn‚Äôt figure out how to install python so I gave up. In graduate school I couldn‚Äôt install python so I used a docker container my professor created and never changed a thing. When working at RStudio I used the Jupyter Lab instance in RStudio Workbench when I couldn‚Äôt install it locally.\nNow, I want to compare pysal results to some functionality I‚Äôve written in R. To do that, I need a python installation. I‚Äôve heard extra horror stories about installing Python on the new Mac M1 chip‚Äîwhich I have."
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#installing-python",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#installing-python",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Installing Python",
    "text": "Installing Python\nThe steps to install python, at least for me, was very simple.\n\nInstall reticulate\nInstall miniconda\n\ninstall.packages(\"reticulate\")\nreticulate::install_miniconda()\nThat‚Äôs it. That‚Äôs all it took."
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#creating-my-first-conda-environment",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#creating-my-first-conda-environment",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Creating my first conda environment",
    "text": "Creating my first conda environment\nAfter installing python, I restarted R, and began building my first conda environment. I created a conda environment called geo for my geospatial work. I installed libpysal, geopandas, and esda. These installed every other dependency I needed‚Äìe.g.¬†pandas, and numpy.\nreticulate::conda_create(\"geo\")\nreticulate::use_condaenv(\"geo\")\nreticulate::conda_install(\"geo\", c(\"libpysal\", \"geopandas\", \"esda\"))"
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#using-my-conda-environment",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#using-my-conda-environment",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Using my conda environment",
    "text": "Using my conda environment\nTo begin using my new conda environment, I opened up a fresh R session and a fresh R Markdown document. In my first code chunk I told reticulate which conda environment to use. Then my following code chunks were python which opened up the python repl. Make sure that you start your code chunk with ```{python}\n\nreticulate::use_condaenv(\"geo\")\n\nIn the following example I utilize esda to calculate a local join count.\n\nimport libpysal\nimport geopandas as gpd\nfrom esda.join_counts_local import Join_Counts_Local\n\nfp = libpysal.examples.root + \"/guerry/\" + \"Guerry.shp\" \n\nguerry_ds = gpd.read_file(fp)\nguerry_ds['SELECTED'] = 0\nguerry_ds.loc[(guerry_ds['Donatns'] > 10997), 'SELECTED'] = 1\n\nw = libpysal.weights.Queen.from_dataframe(guerry_ds)\n\nLJC_uni = Join_Counts_Local(connectivity=w).fit(guerry_ds['SELECTED'])\n\nLJC_uni.p_sim\n\n## array([  nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan, 0.435,   nan, 0.025, 0.025,   nan, 0.328,\n##          nan,   nan,   nan,   nan,   nan,   nan, 0.342,   nan, 0.334,\n##          nan,   nan,   nan,   nan,   nan,   nan, 0.329,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan, 0.481,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan, 0.02 ,   nan,   nan,   nan,   nan,   nan, 0.125,\n##          nan, 0.043,   nan,   nan])"
  },
  {
    "objectID": "posts/2023-08-22-valve-for-production/2023-08-22-valve-for-production.html",
    "href": "posts/2023-08-22-valve-for-production/2023-08-22-valve-for-production.html",
    "title": "Valve: putting R in production",
    "section": "",
    "text": "This blog post is based on my most recent YouTube video. Please give it a watch!\n\n\n\n\n\nI‚Äôve been grinding on a new tool for a few months now. And I‚Äôm hyped to formally introduce you to it. It‚Äôs called Valve And Valve is going to make R in production kick a$$.üî•\nWe‚Äôve all seen those click bait articles saying ‚ÄúDon‚Äôt Put R in production‚Äù or telling you that R can‚Äôt make machine learning models for production. Those ‚Äúhot takes‚Äù are uninformed and can be applied to other languages such as Python. That‚Äôs a bunch of malarkey.\n\n\n\n\nLet‚Äôs get right down to it. Let‚Äôs talk ‚Äúproduction.‚Äù And let me be clear: R belongs in production. But we as the R community need to learn how to do that and be better advocates.\nWhen I say ‚Äúproduction‚Äù I‚Äôm talking about making your code work with any other system. And that‚Äôs where RESTful APIs come in. If I‚Äôve lost you at ‚ÄúRESTful‚Äù, watch my previous video here.\n\n\n\n\n\n\nREST relies on HTTP, which is the foundation of the internet and is a common tongue. It‚Äôs like if Esperanto actually worked. REST APIs provide a language-agnostic way to expose functionality over the web.\nPlumber is an R package that converts your R functions into a RESTful API meaning any tool that can communicate in HTTP can call your R functions. It converts R code like this into an http endpoint.\nAt it‚Äôs core Valve is a web server that runs multiple {plumber} APIs in parallel. Valve spins up and down plumber APIs as needed.\n\n\n\n\nIt‚Äôs designed to work with any existing plumber API. And because of that it supports {vetiver} out of the box.\n\n\n\nVetiver is a framework built by Julia Silge and Isabel Zimmerman from Posit that simplifies putting machine learning models built with tidymodels into a production setting. And since, the goal is R in production, Valve can be easily integrated into Docker containers and deployed with DigitalOcean, AWS, Azure, or whatever other orchestration tools you have available.\nValve is akin to Gunicorn for Flask apps and FastAPI.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo understand why Valve is so powerful, we need to first understand how plumber works and its limitations. Plumber works by writing a function definition and providing annotations using a special comment character #*. Let‚Äôs take a look at a very simple example.\nThe three main components of a plumber API are:\n\nthe function definition\nthe request type @post\n\nendpoint /add-2\n\n\nIn a nutshell plumber works by spawning a single web server using the {httpuv} R package. The webserver captures incoming http requests, captures the provided parameters, body, and requested endpoint. Based on the endpoint, it passes the parameters to the function. The result is then ‚Äúserialized‚Äù into the correct output type. By default, this is json.\n\nFor example, we might be calling the /add-2 endpoint. The process looks a bit like this. We have a GET request. The endpoint is colored red. Then the parameters are colored blue. The request is captured by the web-server. The endpoints are checked. Then the parameters are passed to the function and the user gets the result.\n\nYou can see how this is powerful! But there is one major thing holding this back. This is all running in a single R process. R, like Python, is single threaded. That means each request that comes in has to be added to a queue. The next request cannot be processed until the previous one has been.\nValve helps by running multiple plumber APIs concurrently. Valve is built specifically for plumber, in Rust, and by leveraging the power Tokio framework. Instead of having a single plumber API and a single R process handling all requests, there is another web server handling all incoming requests. This web server is build using Tokio.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe app has a number of pre-specified worker threads. Each worker is capable of taking an incoming request, processing it, and returning a response. These worker threads will delegate the request to another plumber API. These plumber APIs are sitting in a connection pool waiting to be accessed. The APIs will spawn and de-spawn according to the amount of incoming traffic.\n\nWhat this means is that instead of being able to handle 1 request at a time, we can handle as many requests as there are workers concurrently. This allows us to take advantage of more than a single R process at a time and, as a result, we can utilize more of the compute resources available to us.\nSo how do you install Valve? There are two ways in which you can install Valve. The first is to use the Rust package manager Cargo. This is my personal recommendation. If you don‚Äôt have Rust and cargo installed, don‚Äôt worry it is the second easiest language I‚Äôve ever installed.\n\nFollow this one liner and it‚Äôll handle the installation for you.\nTo install Valve with cargo run\ncargo install valve-rs --no-default-features\nDoing this will install the Valve binary and make it available to you as a command line tool. Alternatively, if you want to install valve as an R package you can do so via the R-universe. The R-universe version has pre-built binaries for Windows, Mac, and Ubuntu which means you do not need to have rust installed. But again, its easy, so give it a shot!\ninstall.packages(\n    \"valve\", \n    repos = c(\"https://josiahparry.r-universe.dev\", \"https://cloud.r-project.org\")\n)\nTo follow along with the rest of these demos you can check out code in the github repository.\nHere I want to demo just how easy it is to use Valve and what the experience is like. For this simple example we will run a plumber API with one endpoint /zzz which will sleep for a specified amount of time. We‚Äôll create a Valve app with 10 workers and plumber APIs.\nvalve -n 10 -w 10\nYou‚Äôll notice that only one API is spawned at the start. This is because connections are spawned based on incoming demand. As we send more requests, the plumber APIs will spawn. If, after a specified amount of time, they go stale, they will de-spawn. However, you do have the ability to control the minimum number of plumber APIs.\nValve starting at: 127.0.0.1:3000\nSpawning plumber API at 127.0.0.1:11094\nWe‚Äôre going to create a simple function sleep() which will call the zzz endpoint at a specified port for a specified amount of time. We‚Äôll use furrr to create 10 sessions and call the function 10 times on valve app.\n\nsleep <- function(port, secs) {\n  httr2::request(\n        paste0(\"127.0.0.1:\", port, \"/sleep?zzz=\", secs)\n    ) |> \n    httr2::req_perform() |> \n    httr2::resp_body_string()\n}\n\nNow with the function defined we can use furrr to run the function in parallel\n\nlibrary(furrr)\nplan(multisession, workers = 10)\n\nWe will call the function 10 times using future_map() . The first time this runs we can see that more plumber APIs are being spawned. This takes somewhere between 3 and 4 seconds the first time we run it.\n\nstart <- Sys.time()\nfurrr::future_map(1:10, ~ sleep(3000, 2))\nmulti_total <- Sys.time() - start\nmulti_total \n#> Time difference of 3.653488 secs\n\nIf you watch your terminal, you will see additional plumber connections being spawned.\nValve starting at: 127.0.0.1:3000\nSpawning plumber API at 127.0.0.1:11094\nSpawning plumber API at 127.0.0.1:35714\nSpawning plumber API at 127.0.0.1:15674\nSpawning plumber API at 127.0.0.1:30746\nSpawning plumber API at 127.0.0.1:26860\nSpawning plumber API at 127.0.0.1:54939\nSpawning plumber API at 127.0.0.1:5592\nSpawning plumber API at 127.0.0.1:46549\nSpawning plumber API at 127.0.0.1:53346\nSpawning plumber API at 127.0.0.1:44956\nIf we run this again, we get something much closer to two seconds total for sending all 10 requests.\n\nstart <- Sys.time()\nfurrr::future_map(1:10, ~ sleep(3000, 2))\nmulti_total <- Sys.time() - start\nmulti_total \n#> Time difference of 2.013385 secs\n\nNow, we can do the same thing with all 10 workers calling just one of the spawned plumber APIs.\n\nstart <- Sys.time()\nfurrr::future_map(1:10, ~ sleep(24817, 2))\n(total <- Sys.time() - start)\n#> Time difference of 20.04956 secs\n\nThat‚Äôs a huge different. That is a lot more performance that we are squeezing out of this plumber API by creating multiple to run concurrently.\nIn an R session load {valve}.\nlibrary(valve)\nNext, we will use the function valve_run() to run our plumber API. This function has a lot of handy defaults to moderately scale your plumber API. By default it looks for the file plumber.R in your working directory.\n\nvalve_run(\"plumber.R\", n_max = 10)\n#> Valve app hosted at <http://127.0.0.1:3000>\n#> Spawning plumber API at 127.0.0.1:49015\n\nThe CLI works just like the R function with two differences. We call it from the command line and the syntax is a smidgen different.\nFrom the command line we can run valve ‚Äìhelp to see the arguments that we can provide. The CLI has the same defaults as the R package.\nvalve --help\nUsage: valve [-h <host>] [-p <port>] [-n <n-max>] [-w <workers>] [-f <file>] [--check-unused <check-unused>] [--max-age <max-age>] [--n-min <n-min>]\n\nDistribute your plumber API in parallel.\n\nOptions:\n  -h, --host        host to serve APIs on\n  -p, --port        the port to serve the main application on\n  -n, --n-max       the maximum number of plumber APIs to spawn\n  -w, --workers     number of Tokio workers to spawn to handle requests\n  -f, --file        path to the plumber API (default `plumber.R`)\n  --check-unused    default 10. Interval in seconds when to check for unused\n                    connections\n  --max-age         default 5 mins. How long an API can go unused before being\n                    killed in seconds.\n  --n-min           the maximum number of plumber APIs to spawn\n  --help            display usage information\nNow I want to illustrate scaling a machine learning model with {vetiver} and valve. They do so by wrapping the model into a plumber API. I‚Äôve created a sample plumber API based on Julia‚Äôs recent Tidy Tuesday screencast in which she creates an XGBoost model.\n\nI‚Äôve taken this example and used vetiver to create a plumber API to serve predictions from this model. One could deploy this API as is with Docker or something like Posit Connect. If going down the Docker approach, we can make this a bit more performant by using Valve.\n\n\n\n\n\n\nNote\n\n\n\nThe scripts to generate the vetiver model and API are in the Github repo.\n\n\nTo make this into a Valve app all we need to do is pass provide the plumber API file to valve and we‚Äôre on our way! I‚Äôve written some simple bench marks using drill to compare the performance of the two approaches. With valve we will use 5 concurrent processes and test it.\nvalve -f vetiver-api.R -n 5 -w 5"
  },
  {
    "objectID": "projects/pkgs/h3o.html",
    "href": "projects/pkgs/h3o.html",
    "title": "h3o for H3 indexing",
    "section": "",
    "text": "{h3o} is an R package that offers high-performance geospatial indexing using the H3 grid system. The package is built using {extendr} and provides bindings to the Rust library of the same name.\nThe Rust community built h3o which is a pure rust implementation of Uber‚Äôs H3 hierarchical hexagon grid system. Since h3o is a pure rust library it is typically safer to use, just as fast, and dependency free."
  },
  {
    "objectID": "projects/pkgs/h3o.html#benefits-of-h3o",
    "href": "projects/pkgs/h3o.html#benefits-of-h3o",
    "title": "h3o for H3 indexing",
    "section": "Benefits of h3o",
    "text": "Benefits of h3o\nSince h3o is built purely in Rust and R it is system dependency free and can be compiled for multiple platforms including Linux, MacOS, and Windows, making it easy to use across different OS.\nh3o benefits greatly from the type safety of Rust and provides robust error handling often returning 0 length vectors or NA values when appropriate where errors would typically occur using another H3 library.\nAnd moreover, it is very fast!"
  },
  {
    "objectID": "projects/pkgs/h3o.html#features",
    "href": "projects/pkgs/h3o.html#features",
    "title": "h3o for H3 indexing",
    "section": "Features",
    "text": "Features\nh3o supports all of the functionality that is provided by the C library and the Rust library h3o.\n\n\nIf there are any features missing, please make an issue on GitHub and I‚Äôll be sure to address it!\nh3o was built with sf objects and the tidyverse in mind. h3o objects can be created from sf objects and vice versa. Compatibility with the tidyverse is accomplished via the vctrs package.\n\n\nsf::st_as_sfc() methods for H3 and H3Edge vectors\nautomatic nesting by creating lists of H3 and H3Edge vectors\n\nvectorized output will never return more objects than inputs\n\n\n\nExample\nCreate some points in the bounding box of Wyoming.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(h3o)\n\n# create a bounding box\nbbox_raw <- setNames(\n  c(-111.056888, 40.994746, -104.05216, 45.005904),\n  c(\"xmin\", \"ymin\", \"xmax\", \"ymax\")\n)\n\n# create some points\npnts <- st_bbox(bbox_raw) |> \n  st_as_sfc() |> \n  st_set_crs(4326) |> \n  st_sample(25)\n\n# convert to H3 index\nhexs <- h3_from_points(pnts, 4) \nhexs\n\n<H3[25]>\n [1] 8426a69ffffffff 8426b3bffffffff 8426a57ffffffff 8426b45ffffffff\n [5] 8426a09ffffffff 8426a15ffffffff 8426a3bffffffff 84279abffffffff\n [9] 84278b7ffffffff 8426a6dffffffff 8426b29ffffffff 8426a41ffffffff\n[13] 8426b03ffffffff 8426a27ffffffff 8426a43ffffffff 8426849ffffffff\n[17] 8426949ffffffff 8426949ffffffff 8426ae1ffffffff 8426b33ffffffff\n[21] 8426b49ffffffff 8426b55ffffffff 8426ae1ffffffff 8426a0bffffffff\n[25] 8426b07ffffffff\n\n\nThe H3 vectors can be easily visualized by converting to sf objects. The st_as_sfc() method is defined for H3 vectors. While you may be familair with st_as_sf() the _sfc variant is used for creating columns and should be used on a vector not a dataframe. This way you can use it in a dplyr pipe.\n\npolys <- st_as_sfc(hexs)\npolys\n\nGeometry set for 25 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -110.9645 ymin: 41.19652 xmax: -103.8429 ymax: 45.20037\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOLYGON ((-106.4372 43.90391, -106.1669 44.0391...\n\n\nPOLYGON ((-108.9169 41.57597, -108.6564 41.7208...\n\n\nPOLYGON ((-104.2496 43.00512, -103.9796 43.1365...\n\n\nPOLYGON ((-109.9082 43.59055, -109.6438 43.7344...\n\n\nPOLYGON ((-105.1656 43.29784, -104.8956 43.4310...\n\n\nThis can be plotted.\n\nplot(polys)\n\n\n\n\nTo illustrate tidyverse compatibility lets create an sf object and create a column of H3 indexes.\n\nlibrary(dplyr, warn.conflicts = FALSE)\n\nhexs <- tibble(geometry = pnts) |> \n  st_as_sf() |> \n  mutate(h3 = h3_from_points(geometry, 4))\n\nhexs\n\nSimple feature collection with 25 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -110.8122 ymin: 41.34477 xmax: -104.076 ymax: 44.9885\nGeodetic CRS:  WGS 84\n# A tibble: 25 √ó 2\n               geometry              h3\n *          <POINT [¬∞]>            <H3>\n 1  (-106.4329 44.2269) 8426a69ffffffff\n 2  (-108.884 41.71222) 8426b3bffffffff\n 3 (-104.5573 43.29705) 8426a57ffffffff\n 4  (-110.137 43.82095) 8426b45ffffffff\n 5 (-105.2453 43.45441) 8426a09ffffffff\n 6 (-104.6144 42.59424) 8426a15ffffffff\n 7 (-104.9215 42.31367) 8426a3bffffffff\n 8  (-108.5317 44.9885) 84279abffffffff\n 9 (-104.1934 44.22573) 84278b7ffffffff\n10 (-107.2504 44.14613) 8426a6dffffffff\n# ‚Ñπ 15 more rows\n\n\nAfterwards, lets create a K = 3 disk around each grid cell, create a compact disk by compacting the cells, then unnest into a longer data frame, and update our geometries.\n\ncompact_hexs <- hexs |> \n  mutate(\n    disks = grid_disk(h3, 3),\n    compact_disks = purrr::map(disks, compact_cells)\n  ) |> \n  tidyr::unnest_longer(compact_disks) |> \n  mutate(geometry = st_as_sfc(compact_disks)) |> \n  st_as_sf() \n\nUse ggplot2 to make a simple visualization.\n\nlibrary(ggplot2)\n\nggplot(compact_hexs) +\n  geom_sf(fill = NA) +\n  theme_void()"
  },
  {
    "objectID": "projects/pkgs/sysreqs.html",
    "href": "projects/pkgs/sysreqs.html",
    "title": "R package system requirements",
    "section": "",
    "text": "GitHub repo\nRelated blog post\n\nThe goal of sysreqs is to make it easy to identify R package system dependencies. There are two components to this package: an ‚ÄúAPI‚Äù and a wrapper package.\nThis API and package is based on rstudio/r-system-requirements and the API client for RStudio Package Manager. The functionality is inspired by pak::pkg_system_requirements()."
  },
  {
    "objectID": "projects/pkgs/spdep.html",
    "href": "projects/pkgs/spdep.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "spdep (contributor)\nspdep is an absolute powerhouse of an R package. spdep was first released in 2002 and is one of, if not the, first package to implement many of the most important spatial statistics for aerial data.\nMy contributions include:\n\nLocal Geary C (univariate and multivariate)\nLocal bivariate Moran‚Äôs I\nGlobal bivariate Moran‚Äôs I\nLocal univariate join count\nLocal bivariate join count"
  },
  {
    "objectID": "projects/pkgs/genius.html",
    "href": "projects/pkgs/genius.html",
    "title": "genius (retired)",
    "section": "",
    "text": "genius was my first R package and my first digital child. genius provided a way to programatically access song lyrics from genius.com. This included ways to fetch single songs, albums, and track lists. It was, at one point, integrated with the spotifyr package.\nThis R package was the basis of much of my learning of the data science ecosystem. Including APIs and Docker. Creating stacked ensemble machine learning models using LDA outputs as model inputs (see online guide)\n\n GitHub"
  },
  {
    "objectID": "projects/pkgs/sfdep.html",
    "href": "projects/pkgs/sfdep.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "sfdep is an R package that acts as a tidy interface to the spdep package. In addition it provides some statistics that are not available elsewhere in the R ecosystem such as the neighbor match test.\n\n\n\nEmerging Hot Spot Analysis\nColocation Quotients\nsfnetworks integrations\nLocal Neighbor Match Test\n\n\n\n\n\n GitHub\nRStudio Conf 2022L\nNew York Open Statistical Programming Meetup"
  },
  {
    "objectID": "projects/writing/uitk.html",
    "href": "projects/writing/uitk.html",
    "title": "Urban Informatics Toolkit",
    "section": "",
    "text": "The Urban Informatics Toolkit (uitk) is an open text book I wrote with the intention of teaching first year graduate students in Urban Informatics the R programming language.\nWithin it are two years of study in the Urban Informatics Program at Northeastern University, five years of self-directed education in R, two years of teaching R, and innumerable hours learning R."
  },
  {
    "objectID": "projects/writing/r4campaigns.html",
    "href": "projects/writing/r4campaigns.html",
    "title": "R for Progressive Campaigns",
    "section": "",
    "text": "Read the book here.\nIn 2008, the Obama campaign revolutionized the use of data in political campaigns. Since then, data teams have expanded and grown in size, capacity, and complexity.\nThis short bookdown project is intended to illustrate how data can be used in campaigns through the statistical programming language R. This is a collection of small ‚Äúrecipes‚Äù that I have created that are intended to aid data teams in leveraging R."
  },
  {
    "objectID": "projects/writing/mirr.html",
    "href": "projects/writing/mirr.html",
    "title": "mirrr - song genre classification",
    "section": "",
    "text": "Tidy Music Information Retrieval was a bookdown project I wrote back in 2019 that created a stacked ensemble model that predicted musical genre from song audio features and song lyrics.\nI created 3 models. The first utilized LDA text classification outputs from song lyrics as inputs into a classification model. The second used song audio features from spotify. The third model used the outputs of both to create a stacked ensemble model."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "Packages & contributions\n\n\n\n\n\n\n\nR package system requirements\n\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenius (retired)\n\n\n\npackage\n\n\nmusic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nh3o for H3 indexing\n\n\n\nspatial\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspdep (contributor)\n\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nThings I‚Äôve written\n\n\n\n\n\n\n\nR for Progressive Campaigns\n\n\n\nwriting\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban Informatics Toolkit\n\n\n\nwriting\n\n\nurban-informatics\n\n\ntextbook\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmirrr - song genre classification\n\n\n\nwriting\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "Welcome to my personal website. I always aspire to keep pumping out content, but I don‚Äôt.\nHere you will find my ‚Äúrecent‚Äù blog posts, possibly outdated biographical information, my socials, and links to projects I have made or worked on.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\nsome key takeaways\n\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\nand how they‚Äôll make your package better\n\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\n\n\n\n\n\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\n\n\n\n\n\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\n\n\n\n\n\n\n\nUnderstanding the Narrow Corridor\n\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\n\n\n\n\n\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\n\n\nResource created for training at EPA\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\n\nA tidy wrapper for gtrendsR\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\n\n\n\n\n\n\n\nQuantifying constituency representation\n\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\n\n\n\n\n\n\n\nExtracting and plotting feature importance\n\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\n\n\n\n\n\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\n\n\n\n\n\n\n\nThe Connecticut Compromise and it‚Äôs lasting effects\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\n\n\n\n\n\n\n\nWriting data subsets\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\n\nReading chunked csv files\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\n\nThe Tidy Approach\n\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "employment: Senior Product Engineer @ Esri\neducation:\n\nMS Urban Informatic, Northeastern University (2020)\nBA Sociology, Plymouth State University\n\nMinor, General Mathematics\nProfessional Certificate GIS\nI am a Senior Product Engineer on the Spatial Analysis team at Esri. Previously, I was at The NPD Group as a Research Analyst where I worked to modernize our data science infrastructure to use Databricks, Docker, and Spark. Before that, I was at RStudio, PBC on the customer success team enabling public sector adoption of data science tools. In 2020 I received my master‚Äôs degree in Urban Informatics from Northeastern University following my bachelor‚Äôs degree in sociology with focuses in geographic information systems and general mathematics from Plymouth State University in 2018."
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "about",
    "section": "Contact me",
    "text": "Contact me\nIf you want to get in contact with me please send me an email at josiah.parry at gmail dot com.\n\n\ntalks i‚Äôve given\n\n\nExploratory Spatal Data Analysis in the tidyverse\n\nJuly 28th, 2022 rstudio::conf(2022L)\n\nExploratory Spatial Data Analysis in R\n\nRecording\nApril 28th, 2022\n\nAPIs: you‚Äôre probably not using them and why you probably should\n\nGovernment Advances in Statistical Programming\nNovember 6th, 2020\n\n‚ÄúOld Town Road‚Äù Rap or Country?: Putting R in Production with Tidymodels, Plumber, and Shiny\n\nBoston useR group\nDecember 10th, 2019\n\nTidy Lyrical Analysis\n\nBoston useR group\nJuly 17th, 2018\n\nNewfound Lake Landscape Value Analysis: Exploring the efficacy of PPGIS, NESTVAL 2016\n\nNew England St.¬†Lawrence River Valley regional American Associations of Geographers Conference\n2016"
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html",
    "href": "posts/2023-09-20-sdsl/index.html",
    "title": "Spatial Data Science Across Languages",
    "section": "",
    "text": "I feel very fortunate to have been invited to the first Spatial Data Science Across Languages (SDSL) workshop at the University of M√ºnster. I am even more fortunate that I have an employer who sees the value in an event such as this and be my patron for it.\nThe event brought together package maintainers from Julia, Python, and R languages to just discuss. The event was loosely framed around a few broad discussion topics that were varied and drifted.\nIn general, the theme of the workshop was ‚Äústandards.‚Äù We need standards be able to ensure cohesion not only within languages, but across them. Users should be able to move between languages and be able to expect similar behavior, have similar terminology, and expect the same analysis results."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#arrow",
    "href": "posts/2023-09-20-sdsl/index.html#arrow",
    "title": "Spatial Data Science Across Languages",
    "section": "Arrow",
    "text": "Arrow\nWe started everything off by discussing Arrow which set the theme of ‚Äústandards.‚Äù Arrow gets conflated at many thing all at once‚ÄîI do that. At the core Arrow is a memory format specification. It describes how data should be held in memory.\nR holds objects in memory one way, Python another, and Julia another as well. Arrow describes just one way that specific types of object can be held in memory. GeoArrow is an extension of Arrow that specifies the memory layout for geometry arrays.\n\nGeoArrow\nSpecifications like well-known text (WKT) and well-known binary (WKB) are encodings of a single geometry. GeoArrow recognizes that we almost never work with scalar objects alone. GeoArrow is a memory layout for an array of geometries.\n\nIf each language can hold Arrow arrays in memory, they can be passed from one tool to another with 0 cost. Python can create an arrow array and R can pick it up if it knows where it exists.\nThe current approach looks something like this. Each tool serializes its data in one way. In order for another tool to use it, the data needs to be copied (memory inefficient) and converted (computationally expensive) into the appropriate format.\n\nThe Arrow specification would allow data handoff between tools to be much more seamless and look like so:\n\n\n\nMaybe we ought to start framing adoption of Arrow as an effort to be more ‚Äúgreen.‚Äù If we spend less time computing we use less energy which is overall a net positive for the world.\nThis is a massive productivity improvement. There‚Äôs no computation cost in converting between one format to another saving time, energy, and money.\n\n\nThere‚Äôs a good chance that in order to adopt Arrow in {sf} there will be breaking changes. I am an advocate for breaking changes when they are for a good reason. Being on the leading edge is how to make a language succeed.\nI also think if we can move towards a ‚Äútrait-driven‚Äù approach to spatial data frames, we can support both GeoArrow geometries as well as current sfc objects.\nRead my spatial data frame manifesto.\nThe key thing though, is that in order for Arrow to be useful, it has to be adopted widely. If GeoPandas uses Arrow and {sf} does not, we have to go through the copy and convert process anyways.\n\n\nWhy GeoArrow excites me\nThe promise of Arrow and GeoArrow is that memory can be handed off between tools without any additional cost. This (in theory) lowers the bar for what is needed to hand off between tools and languages. Hopefully ending the language wars\nKyle Barron demonstrated really cool example use-case where he created GeoArrow arrays using GeoPolars. That array was then written to a buffer and picked up by javascript. Since there was no serialization or deserialization it was unbelievably fast!\n\nAdditionally, we are seeing WebAssembly proliferate in the data science community. WebR provides R users with the ability to execute R in the browser. This is also possible in Python, Rust, Go, and I‚Äôm sure many others. Each language can be compiled to be used in the browser and hand off components between them.\nClient side computation will reduce the need for server side operations. If we can reduce the amount of hours that servers are constantly running by offloading lighter operations into the browser, we may be able to save money, energy, be more green, and create tools that do not necessarily require an active internet connection."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#spatial-support",
    "href": "posts/2023-09-20-sdsl/index.html#spatial-support",
    "title": "Spatial Data Science Across Languages",
    "section": "Spatial Support",
    "text": "Spatial Support\nWe also discussed the more esoteric topic of spatial support. This was completely new to me. Support defines the relationship between an attribute to the geometry. There are two kinds:\n\npoint support - a constant values associated with every location in a geometry\n\nexample: temperature measurement at a weather station\n\nblock support - a value derived from aggregating measures over space\n\nexample: population count in a census tract\n\n\n\n\n\n\n\n\nTip\n\n\n\nRead chapter 1.6 of Spatial Data Science (SDS) for more on this topic.\n\n\nWhen geometries are manipulated and the associated attributes come along for the ride, support assumptions are often violated resulting in inaccurate calculations or maps.\n\nAttribute-Geometry Relationships\nSDS formalizes the relationship between attributes and geometry a bit further in something they call the Attribute-Geometry Relationship (AGR). Attributes of spatial features can have one of 3 types of AGR:\n\nconstant value (i.e.¬†point support)\naggregate value (i.e.¬†block support)\nidentity (i.e.¬†attribute unique to a geometry)\n\nKnowing the relationships between geometries can be useful in tracking the assumptions of analyses. For example, taking the mean of an aggregate attribute such as median age, creates as assumptions of homogeneity in the aggregated areas and can contribute to the modifiable areal unit problem (MAUP).\n\n\nIntensive vs Extensive\nSpatial intensive vs extensive variables were also discussed in the context of spatial interpolation. I‚Äôm still quite unclear on how to conceptualize intensive and extensive variables. Tobias G pointed out that these terms come from physics and provided a useful non-geometry motivating example.\n\n‚ÄúThe price of an ingot of gold is an extensive property and its temperature would be intensive.‚Äù\n\n\n\nThe common example is that population is extensive and population density is intensive. This requires the assumption that population is constant across space. So the examples are more confusing than helpful. I have yet to come up with an example of a spatially intensive variable that makes sense.\nIf you can think of one, please comment on below!\nExtensive variables are one that are associated with the physical geometry itself. Intensive ones do not change when a geometry is modified.\nIf an ingot of gold is split into half the price changes, each piece is now worth less than the whole. But, assuming the room temperature didn‚Äôt change, the temperature of each piece remained the same.\n\n\nDomains\nThese properties of attributes are quite important but are forgotten about. One of the ideas raised in discussions was adding attribute-geometry relationship and a flag like is_intensive to a field domain.\nA Domain is a concept that I think originated at Esri. It allows you to specify the field type, range of valid values, as well as policies that determine how fields behave when they are split or merged. Field domains were added to GDAL in version 3.3.\nIs there utility in adding AGR and (ex/in)tensive flags to a field domain?\n\n\nArrow allows for embedded metadata at an array and table level. Perhaps there should be a GeoArrow table (data frame) format spec too? I‚Äôd like that. It would fit with my generic spatial data frame manifesto as well."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#geodesic-first",
    "href": "posts/2023-09-20-sdsl/index.html#geodesic-first",
    "title": "Spatial Data Science Across Languages",
    "section": "Geodesic first",
    "text": "Geodesic first\nA good amount of attention was paid to geodesic coordinate operations. The conversation was kicked off by this ‚Äúmotivating example.‚Äù\n\n\n\nReally, I think this was just an excuse for Edzer to poke fun at the GeoPandas devs! üòµ‚Äçüí´ü§£\nThe example shows an area calculation on a dataset that uses a geographic coordinate system (GCS). Area, though, is typically calculated under the assumption that coordinates are on a plane (rectangle). With GCS, the data is on a circle. So if we calculate the area of angles the result is definitely wrong.\n\n\nI think about it like calculating the area but cutting through the Earth like so: \nThe open source ecosystem is behind on support of geodetic calculations. Data more often than not is captured using GCS and users often fail to project their data. It would be nice if tools did this.\nR supports spherical geometries by using Google‚Äôs S2 library. Python is presently building out support for S2. Some operations like buffering still aren‚Äôt made available by S2.\n\nThe ‚Äúfull polygon‚Äù\nOne interesting point that was brought up is that in a GCS there is a concept of a full polygon. This is the polygon that covers the entire ellipsoid. There is no way to capture this using typical coordinates."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#spatial-weights-matrix-serialization",
    "href": "posts/2023-09-20-sdsl/index.html#spatial-weights-matrix-serialization",
    "title": "Spatial Data Science Across Languages",
    "section": "Spatial Weights Matrix Serialization",
    "text": "Spatial Weights Matrix Serialization\nProbably the second most interesting topic to me was around how to store spatial weights matrixes. We didn‚Äôt really discuss the ‚Äúhow‚Äù of holding it memory. Though I think they can be held in memory as a ragged Arrow array of indices or IDs. What was quite interesting was the concept of serializing the spatial weights matrix.\nMartin mentioned that in Pysal they had moved to serializing spatial weights as a parquet file which greatly improved their speed. In essence, spatial weights are stored in a 3 column table.\n\n\n\nSpatial Weights Matrix\n\n\ni\nj\nwij\n\n\n\n\n1\n3\n0.5\n\n\n1\n2\n0.75\n\n\n1\n9\n0.3\n\n\n2\n7\n0.2\n\n\n\nIt was noted that additional metadata can be associated at the table level or column level. This can be very handy to keep track of things like the method for identifying neighbors, the weighting mechanism used, storing flags to know if the focal feature is included and maybe even remove weights if there is a constant value.\nAdditionally, since this is parquet, it can be handled and stored by a number of databases.\nOne benefit of using arrow here, is that we can conceivably have a future where spatial weights matrices are interchangeable between spdep, pysal, geoda, and ArcGIS."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#closing",
    "href": "posts/2023-09-20-sdsl/index.html#closing",
    "title": "Spatial Data Science Across Languages",
    "section": "Closing",
    "text": "Closing\nI‚Äôm about to hop on a flight back to the US now‚Äî10 hours without internet is going to be a test of monk-like fortitude. I have left my strongest feels for another time. Chatting with devs from other languages makes is clear how great CRAN is as a package storage and testing mechanism but yet how utterly abysmal it is as a developer. I will write another post soon on the topics of retirement and how I think we can make CRAN a better place for developers."
  },
  {
    "objectID": "posts/2023-09-20-sdsl/index.html#links",
    "href": "posts/2023-09-20-sdsl/index.html#links",
    "title": "Spatial Data Science Across Languages",
    "section": "Additional links",
    "text": "Additional links\nI‚Äôll add more here as I can (hopefully).\n\nMartin‚Äôs SDSL blog post"
  },
  {
    "objectID": "posts/2023-10-28-python-fns/index.html",
    "href": "posts/2023-10-28-python-fns/index.html",
    "title": "Export Python functions in R packages",
    "section": "",
    "text": "I was asked a really interesting question by @benyamindsmith yesterday. The question was essentially:\nI proposed my solution as a very minimal R package called {pyfns}.\nIt is an R package with one function: hello_world()."
  },
  {
    "objectID": "posts/2023-10-28-python-fns/index.html#how-it-works",
    "href": "posts/2023-10-28-python-fns/index.html#how-it-works",
    "title": "Export Python functions in R packages",
    "section": "How it works",
    "text": "How it works\nThe process is fairly simple.\n\nWe create an environment inside of our package\nOn package start-up we source python scripts using reticulate::source_python() into the new environment\nWe create R wrapper functions that call the reticulated function.\n\nExample usage:\n\npyfns::hello_world()\n\n[1] \"Helloooo world\""
  },
  {
    "objectID": "posts/2023-10-28-python-fns/index.html#storing-python-scripts",
    "href": "posts/2023-10-28-python-fns/index.html#storing-python-scripts",
    "title": "Export Python functions in R packages",
    "section": "Storing Python Scripts",
    "text": "Storing Python Scripts\nStore python scripts inside of inst/. These files can be read using system.file(). In this example inst/helloworld.py contains\ndef hello_world():\n  return \"Helloooo world\""
  },
  {
    "objectID": "posts/2023-10-28-python-fns/index.html#creating-an-environment",
    "href": "posts/2023-10-28-python-fns/index.html#creating-an-environment",
    "title": "Export Python functions in R packages",
    "section": "Creating an environment",
    "text": "Creating an environment\nBefore we can source python scripts, we must create an environment to soure them into. This is done in R/env.R like so\npyfn_env <- rlang::env()"
  },
  {
    "objectID": "posts/2023-10-28-python-fns/index.html#sourcing-scripts",
    "href": "posts/2023-10-28-python-fns/index.html#sourcing-scripts",
    "title": "Export Python functions in R packages",
    "section": "Sourcing scripts",
    "text": "Sourcing scripts\nScripts are sourced in R/zzz.R in which there is an .onLoad() function call. This gets called only once when the package is loaded.\n.onLoad <- function(libname, pkgname){\n  reticulate::source_python(\n    system.file(\"helloworld.py\", package = \"pyfns\"),\n    envir = pyfn_env\n  )\n}\nIn this chunk we use reticulate::source_python() to bring the python function into scope. The function needs a path to the python script that we want to source. This is where system.file() comes into play. It can access files stored in inst. Note that it does not include inst. And most importantly we set envir = pyfn_env which is the environment we created in R/env.R"
  },
  {
    "objectID": "posts/2023-10-28-python-fns/index.html#wrapper-functions",
    "href": "posts/2023-10-28-python-fns/index.html#wrapper-functions",
    "title": "Export Python functions in R packages",
    "section": "Wrapper functions",
    "text": "Wrapper functions\nSince the functions are being sourced into pyfn_env they can be called from the environment directly. In R/env.R, the R function hello_world() is just calling the hello_world() python function from the pyfn_env. If there were arguments we can pass them in using ... in the outer function or recreating the same function arguments.\n#'@export\nhello_world <- function() {\n  pyfn_env$hello_world()\n}"
  },
  {
    "objectID": "posts/2023-10-28/index.html",
    "href": "posts/2023-10-28/index.html",
    "title": "Where am I in the sky?",
    "section": "",
    "text": "When I was flying back from the Spatial Data Science Across Langauge event from Frankfurt to Atlanta the plane I was bored beyond measure. The plane had no wifi to connect to. I had already watched a movie and couldn‚Äôt be bothered by a podcast. I wanted to know where I was.\nWhen looking at the onboard ‚ÄúAbout this flight‚Äù information, they didn‚Äôt show a map even. The gave us our coordinates in degrees and minutes. Helpful right?\nWell, in an attempt to figure out where the hell I was I wrote some code. Here it is.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(units)\n\nudunits database from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/units/share/udunits/udunits2.xml\n\n#' Given degrees and minutes calculate the coordinate\n#' in degrees\nas_degree <- function(degrees, minutes) {\n  d <- set_units(degrees, \"arc_degrees\")\n  m <- set_units(minutes, \"arc_minutes\") |> \n    set_units(\"arc_degrees\")\n  d + m\n}\n\n# get the country shapes\nx <- rnaturalearthdata::countries50 |>  st_as_sf() \n\n# filter to North America\nusa <- x |> \n  dplyr::filter(continent == \"North America\", \n                subregion == \"Northern America\") |> \n  st_geometry() \n\n# Create a bounding box to crop myself to \ncrp <- st_bbox(c(xmin = -128, xmax = 0, ymin = 18, ymax = 61))\n\n# plot N. America\nusa |> \n  st_cast(\"POLYGON\") |> \n  st_as_sf() |> \n  st_filter(\n    st_as_sfc(crp) |> \n      st_as_sf(crs = st_crs(usa))\n    ) |>\n  plot()\n\n\n# add planes location.\nplot(\n  st_point(c(-as_degree(61, 19), as_degree(57, 46))),\n  add = TRUE,\n  col = \"red\",\n  pch = 16\n)"
  }
]