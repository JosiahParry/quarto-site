[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "Packages & contributions\n\n\n\n\n\n\n\nR package system requirements\n\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenius (retired)\n\n\n\npackage\n\n\nmusic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nh3o for H3 indexing\n\n\n\nspatial\n\n\npackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspdep (contributor)\n\n\n\npackage\n\n\nspatial\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nThings I’ve written\n\n\n\n\n\n\n\nR for Progressive Campaigns\n\n\n\nwriting\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUrban Informatics Toolkit\n\n\n\nwriting\n\n\nurban-informatics\n\n\ntextbook\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmirrr - song genre classification\n\n\n\nwriting\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "projects"
    ]
  },
  {
    "objectID": "projects/writing/mirr.html",
    "href": "projects/writing/mirr.html",
    "title": "mirrr - song genre classification",
    "section": "",
    "text": "Tidy Music Information Retrieval was a bookdown project I wrote back in 2019 that created a stacked ensemble model that predicted musical genre from song audio features and song lyrics.\nI created 3 models. The first utilized LDA text classification outputs from song lyrics as inputs into a classification model. The second used song audio features from spotify. The third model used the outputs of both to create a stacked ensemble model."
  },
  {
    "objectID": "projects/pkgs/genius.html",
    "href": "projects/pkgs/genius.html",
    "title": "genius (retired)",
    "section": "",
    "text": "genius was my first R package and my first digital child. genius provided a way to programatically access song lyrics from genius.com. This included ways to fetch single songs, albums, and track lists. It was, at one point, integrated with the spotifyr package.\nThis R package was the basis of much of my learning of the data science ecosystem. Including APIs and Docker. Creating stacked ensemble machine learning models using LDA outputs as model inputs (see online guide)\n\n GitHub"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html",
    "href": "posts/2023-11-10-enums-in-r/index.html",
    "title": "Enums in R: towards type safe R",
    "section": "",
    "text": "Hadley Wickham has recently dropped a new draft section of his book Tidy Design Principles on enumerations and their use in R.\nIn short, enumerations enumerate (list out) the possible values that something might take on. In R we see this most often in function signatures where an argument takes a scalar value but all possible values are listed out."
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#enums-in-r",
    "href": "posts/2023-11-10-enums-in-r/index.html#enums-in-r",
    "title": "Enums in R: towards type safe R",
    "section": "Enums in R",
    "text": "Enums in R\nA good example is the cor() function from the base package stats.\n\nargs(cor)\n\nfunction (x, y = NULL, use = \"everything\", method = c(\"pearson\", \n    \"kendall\", \"spearman\")) \nNULL\n\n\nThe possible values for method are \"pearson\", \"kendall\", or \"spearman\" but all values are listed inside of the function definition.\nInside of the function, though, match.arg(method) is used to ensure that the provided value to the method argument is one of the provided values.\nHadley makes the argument that we should prefer an enumeration to a boolean flag such as TRUE or FALSE. I agree!\nA real world example\nA post on mastodon makes a point that the function sf::st_make_grid() has an argument square = TRUE where when set to FALSE hexagons are returned.\n\n\nIn this case, it’s very clear that an enum would be better! For example we can improve the signature like so:\nst_make_grid &lt;- function(x, grid_shape = c(\"square\", \"hexagon\"), ...) {\n  # ensure only one of the provided grid shapes are used\n  match.arg(grid_shape)\n  # ... rest of function \n}"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#enums-in-rust",
    "href": "posts/2023-11-10-enums-in-r/index.html#enums-in-rust",
    "title": "Enums in R: towards type safe R",
    "section": "Enums in Rust",
    "text": "Enums in Rust\nWhen I first started using rust enums made no sense to me. In Rust, enums are a first class citizen that are treated as their own thing.\n\n\nI’m not really sure what to call things in Rust. Are they all objects?\nWe make them by defining the name of the enum and the variants they may take on.\nenum GridShape {\n  Square,\n  Hexagon\n}\nNow you can use this enum GridShape to specify one of two types: Square or Hexagon. Syntactically, this is written GridShape::Square and GridShape::Hexagon.\nEnums are very nice because we can match on the variants and do different things based on them. For example we can have a function like so:\nfn which_shape(x: GridShape) {\n    match x {\n        GridShape::Square =&gt; println!(\"We have a square!\"),\n        GridShape::Hexagon =&gt; println!(\"Hexagons are the bestagons\")\n    }\n}\nIt takes an argument x which is a GridShape enum. We match on the possible variants and then do something.\n\n\nInside of the match statement each of the possible variants of the enum have to be written out. These are called match arms. The left side lists the variant where as the right portion (after =&gt;) indicates what will be executed if the left side is matched (essentially if the condition is true).\nWith this function we can pass in specific variants and get different behavior.\n\n\nGridShape::Hexagon\nGridShape::Square\n\n\n\nwhich_shape(GridShape::Hexagon)\n#&gt; Hexagons are the bestagons\n\n\nwhich_shape(GridShape::Square)\n#&gt; We have a square!"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#making-an-s7-enum-object-in-r",
    "href": "posts/2023-11-10-enums-in-r/index.html#making-an-s7-enum-object-in-r",
    "title": "Enums in R: towards type safe R",
    "section": "Making an S7 enum object in R",
    "text": "Making an S7 enum object in R\nI think R would benefit from having a “real” enum type object. Having a character vector of valid variants and checking against them using match.arg() or rlang::arg_match() is great but I think we can go further.\n\n\nSince learning Rust, I think having more strictness can make our code much better and more robust. I think adding enums would be a good step towards that\nI’ve prototyped an Enum type in R using the new S7 object system that might point us towards what an enum object in the future might look like for R users.\nDesign of an Enum\nFor an enum we need to know what the valid variants are and what the current value of the enum is. These would be the two properties.\nAn enum S7 object must also make sure that a value of an Enum is one of the valid variants. Using the GridShape enum the valid variants would be \"Square\" and \"Hexagon\". A GridShape enum could not take, for example, \"Circle\" since it is not a listed variant.\nUsing an abstract class\nTo start, we will create an abstract S7 class called Enum.\n\n“_an abstract class is a generic class (or type of object) used as a basis for creating specific objects that conform to its protocol, or the set of operations it supports” — Source\n\nThe Enum class will be used to create other Enum objects.\n\nlibrary(S7)\n\n# create a new Enum abstract class\nEnum &lt;- new_class(\n  \"Enum\",\n  properties = list(\n    Value = class_character,\n    Variants = class_character\n  ),\n  validator = function(self) { \n    if (length(self@Value) != 1L) {\n      \"enum value's are length 1\"\n    } else if (!(self@Value %in% self@Variants)) {\n      \"enum value must be one of possible variants\"\n    }\n  }, \n  abstract = TRUE\n)\n\nIn this code chunk we specify that there are 2 properties: Value and Variant each must be a character type. Value will be the value of the enum. It would be the right hand side of GridShape::Square in Rust’s enum, for example. Variants is a character vector of all of the possible values it may be able to take on. The validator ensures that Value must only have 1 value. It also ensures that Value is one of the enumerated Variants. This Enum class will be used to generate other enums and cannot be instantiated by itself.\nWe can create a new enum factory function with the arguments:\n\n\nenum_class the class of the enum we are creating\n\nvariants a character vector of the valid variant values\n\n\n# create a new enum constructor \nnew_enum_class &lt;- function(enum_class, variants) {\n  new_class(\n    enum_class,\n    parent = Enum,\n    properties = list(\n      Value = class_character,\n      Variants = new_property(class_character, default = variants)\n    ),\n    constructor = function(Value) {\n      new_object(S7_object(), Value = Value, Variants = variants)\n    }\n  )\n}\n\n\n\nNote that the constructor here only takes a Value argument. We do this so that users cannot circumvent the pre-defined variants.\nWith this we can now create a GridShape enum in R!\n\nGridShape &lt;- new_enum_class(\n  \"GridShape\",\n  c(\"Square\", \"Hexagon\")\n)\n\nGridShape\n\n&lt;GridShape&gt; class\n@ parent     : &lt;Enum&gt;\n@ constructor: function(Value) {...}\n@ validator  : &lt;NULL&gt;\n@ properties :\n $ Value   : &lt;character&gt;\n $ Variants: &lt;character&gt;\n\n\nThis new object will construct new GridShape enums for us.\n\nGridShape(\"Square\")\n\n&lt;GridShape&gt;\n @ Value   : chr \"Square\"\n @ Variants: chr [1:2] \"Square\" \"Hexagon\"\n\n\nWhen we try to create a GridShape that is not one of the valid variants we will get an error.\n\nGridShape(\"Triangle\")\n\nError: &lt;GridShape&gt; object is invalid:\n- enum value must be one of possible variants\n\n\nMaking a print method\nFor fun, I would like Enum objects to print like how I would use them in Rust. To do this we can create a custom print method\n\n# print method for enums\n# since this is an abstract class we get the first class (super)\n# to print\nprint.Enum &lt;- function(x, ...) {\n  cat(class(x)[1], \"::\", x@Value, sep = \"\")\n  invisible(x)\n}\n\nSince Enums will only ever be a sub-class we can confidently grab the first element of the class(enum_obj) which is the super-class of the enum. We paste that together with the value of the enum.\n\nsquare  &lt;- GridShape(\"Square\")\nsquare\n\nGridShape::Square"
  },
  {
    "objectID": "posts/2023-11-10-enums-in-r/index.html#drawing-even-more-from-rust",
    "href": "posts/2023-11-10-enums-in-r/index.html#drawing-even-more-from-rust",
    "title": "Enums in R: towards type safe R",
    "section": "Drawing even more from Rust",
    "text": "Drawing even more from Rust\nRust enums are even more powerful than what I briefly introduced. Each variant of an enum can actually be typed!!! Take a look at the example from The Book™.\nenum Message {\n    Quit,\n    Move { x: i32, y: i32 },\n    Write(String),\n    ChangeColor(i32, i32, i32),\n}\nIn this enum there are 4 variants. The first Quit doesn’t have any associated data with it. But the other three do! The second one Move has two fields x and y which contain integer values. Write is a tuple with a string in it and ChangeColor has 3 integer values in its tuple. These can be extracted.\nA silly example function that illustrates how each value can be used can be\nfn which_msg(x: Message) {\n    match x {\n        Message::Quit =&gt; println!(\"I'm a quitter\"),\n        Message::Move { x, y } =&gt;  println!(\"Move over {x} and up {y}\"),\n        Message::Write(msg) =&gt; println!(\"your message is: {msg}\"),\n        Message::ChangeColor(r, g, b) =&gt;  println!(\"Your RGB ({r}, {g}, {b})\"),\n    }\n}\nWhen a variant with data is passed in the values can be used. For example\nwhich_msg(Message::ChangeColor(0, 155, 200));\n#&gt; Your RGB (0, 155, 200)\nExtending it to R\nWhat would this look like if we extended it to an R based enum object? I suspect the Variants would be a list of prototypes such as those from {vctrs}. The Value would have to be validated against all of the provided prototypes to ensure that it is one of the provided types.\nI’m not sure how I would code this up, but I think that would be a great thing to have."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html",
    "href": "posts/2023-07-06-r-is-still-fast.html",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "",
    "text": "There’s this new blog post making the rounds making some claims about why they won’t put R into production. Most notably they’re wheeling the whole “R is slow thing” again. And there are few things that grind my gears more than that type of sentiment. It’s almost always ill informed. I find that to be the case here too.\nI wouldn’t have known about this had it 1) not mentioned my own Rust project Valve and 2) a kind stranger inform me about it on mastodon.\nI’ve collected my reactions below as notes and sundry bench marks and bullet points."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#tldr",
    "href": "posts/2023-07-06-r-is-still-fast.html#tldr",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "TL;DR",
    "text": "TL;DR\n\nThere is a concurrent web server for R and I made it Valve\n\nPython is really fast at serializing json and R is slower\nPython is really slow at parsing json and R is so so soooo much faster\nTo handle types appropriately, sometimes you have to program\nThere are mock REST API testing libraries {httptest} and {webmockr}\n\nDemand your service providers to make the tools you want\nAsk and you shall receive\nR can go into production\nPLEASE JUST TRY VALVE YOU’LL LOVE IT"
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#production-services",
    "href": "posts/2023-07-06-r-is-still-fast.html#production-services",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Production Services",
    "text": "Production Services\nThere are so many people using R in production in so many ways across the world. I wish Posit did a better job getting these stories out. As a former RStudio employee, I personally met people putting R in production in most amazing ways. From the US Department of State, Defense, Biotech companies, marketing agencies, national lotteries, and so much more. The one that sticks out the most is that Payam M., when at Tabcorp massively scaled their system using Plumber APIs and Posit Connect to such a ridiculous scale I couldn’t even believe.\nGunicorn, Web Servers, and Concurrency\n\n“R has no widely-used web server to help it run concurrently.”\n\nThe premise of this whole blog post stems from the fact that there is no easily concurrent web server for R. Which is true and is the reason I built Valve. It doesn’t meet the criteria of widely used because no one has used it. In part, because of posts like this that discourage people from using R in production.\nTypes and Conversion\nThere’s this weird bit about how 1 and c(1, 2) are treated as the same class and unboxing of json. They provide the following python code as a desirable pattern for processing data.\nx = 1\ny = [1, 2]\n\njson.dump(x, sys.stdout)\n#&gt; 1\njson.dump(y, sys.stdout)\n#&gt; [1, 2]\nThey want scalars to be unboxed and lists to remain lists. This is the same behavior as jsonlite, though.\n\njsonlite::toJSON(1, auto_unbox = TRUE)\n\n1 \n\njsonlite::toJSON(1:2, auto_unbox = TRUE)\n\n[1,2] \n\n\nThere’s a difference here: one that the author fails to recognize is that a length 1 vector is handled appropriately. What the author is saying is that they don’t like that R doesn’t behave the same way as Python. You, as a developer should be able to guarantee that a value is length 1. It’s easy. length(x) == 1, or if you want is_scalar &lt;- function(x) length(x) == 1. This is the type system in R and json libraries handle the “edge case” appropriately. There is nothing wrong here. The reprex is the same as the python library.\n\n“R (and Plumber) also do not enforce types of parameters to your API, as opposed to FastAPI, for instance, which does via the use of pydantic.”\n\nPython does not type check nor does FastAPI. You opt in to type checking with FastAPI. You can do the same with Plumber. A quick perusal of the docs will show you this. Find the @param section. There is some concessions here, though. The truthful part here is the type annotations do type conversion for only dynamic routes. Which, I don’t know if FastAPI does. Type handling for static parameters is an outstanding issue of mine for plumber since 2021.\nI’ve followed up on the issue above and within minutes the maintainer responded. There is an existing PR to handle this issue.\nThis just goes to show if that you want something done in the open source world, just ask for it. More than likely its already there or just waiting for the slight nudge from someone else.\nWhile I know it’s not “seemless” adding an as.integer() and a stopifnot(is.integer(n)) isn’t the wildest thing for a developer to do.\nThere is a comparison between type checking in R and Python with the python example using type hints which are, again, opt-in. An unfair comparison when you say “if you don’t use the opt-in features of plumber but use the opt-in features of FastAPI, FastAPI is better.”\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/types\")\nasync def types(n: int) -&gt; int:\n  return n * 2\nClients and Testing\nI haven’t done much testing of API endpoints but I do know that there are two de facto packages for this:\n\n\n{httptest} and\n\n{webmockr}.\n\nThese are pretty easy to find. Not so sure why they weren’t mentioned or even tested.\nPerformance\nJSON serialization is a quite interesting thing to base performance off of. I’ve never seen how fast pandas serialization is. Quite impressive! But, keep with me, because you’ll see, this is fibbing with benchmarks.\nI do have thoughts on the use of jsonlite and it’s ubiquity. jsonlite is slow. I don’t like it. My belief is that everyone should use {jsonify} when creating json. It’s damn good.\nSo, when I run these bench marks on my machine for parsing I get:\n\nmicrobenchmark::microbenchmark(\n  jsonify = jsonify::to_json(iris),\n  jsonlite = jsonlite::toJSON(iris),\n  unit = \"ms\", \n  times = 1000\n)\n\nWarning in microbenchmark::microbenchmark(jsonify = jsonify::to_json(iris), :\nless accurate nanosecond times to avoid potential integer overflows\n\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\nUnit: milliseconds\n     expr      min       lq      mean    median       uq      max neval cld\n  jsonify 0.258218 0.265024 0.3224672 0.2698005 0.280850 35.20330  1000  a \n jsonlite 0.346245 0.360759 0.4169715 0.3719110 0.399012 20.00181  1000   b\n\n\nA very noticable difference in using jsonify over jsonlite. The same benchmark using pandas is holy sh!t fast!\nfrom timeit import timeit\nimport pandas as pd\n\niris = pd.read_csv(\"fastapi-example/iris.csv\")\n\nN = 1000\n\nprint(\n  \"Mean runtime:\", \n  round(1000 * timeit('iris.to_json(orient = \"records\")', globals = locals(), number = N) / N, 4), \n  \"milliseconds\"\n)\n#&gt; Mean runtime: 0.0721 milliseconds\nNow, this is only half the story. This is serialization. What about the other part? Where you ingest it.\nHere, I will also say, again, that you shouldn’t use jsonlite because it is slow. Instead, you should use {RcppSimdJson}. Because its\n\n\nLet’s run another benchmark\n\njsn &lt;- jsonify::to_json(iris)\n\nmicrobenchmark::microbenchmark(\n  simd = RcppSimdJson::fparse(jsn),\n  jsonlite = jsonlite::fromJSON(jsn),\n  unit = \"ms\",\n  times = 1000\n)\n\nUnit: milliseconds\n     expr      min        lq       mean   median        uq      max neval cld\n     simd 0.052275 0.0551040 0.06672631 0.057933 0.0634885 4.316275  1000  a \n jsonlite 0.433165 0.4531525 0.48931155 0.467359 0.4919795 4.352232  1000   b\n\n\nRcppSimdJson is ~8 times faster than jsonlite.\nLet’s do a similar benchmark in python.\njsn = iris.to_json(orient = \"records\")\n\nprint(\n  \"Mean runtime:\", \n  round(1000 * timeit('pd.read_json(jsn)', globals = locals(), number = N) / N, 4), \n  \"milliseconds\"\n)\n#&gt; Mean runtime: 1.2629 milliseconds\nPython is 3x slower than jsonlite in this case and 25x slower than RcppSimdJson. Which is very slow. While serializing is an important thing to be fast in, so is parsing the incoming json you are receiving. How nice it is to show only half the story! Use RcppSimdJson and embarrass pandas’ json parsing.\nIntegration with Tooling\nI have literally no idea about any of these except Launchdarkly because one of my close homies worked there for years. These are all paid services so I’m not sure how they work :)\nI would say to checkout Posit Connect for deploying R and python into production. But if your only use case is to deploy a single model, then yeah, I’d say that’s overkill.\nI wish more companies would create tooling for R and their services. The way to do this, is to lean into using R in production and demanding (not asking) providers to make wrappers for them. When you pay for a service, you have leverage. Use it. I think too many people fall over when what they need isn’t there immediately. Be sure to be the squeeky wheel that makes change.\nI also think that if you’re in the position where you can make a wrapper for something, you should. I did this when using Databricks in my last role and provided them with a lot of feedback. Have they taken it? I’m not sure. I’m not there to harass them anymore."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#workarounds",
    "href": "posts/2023-07-06-r-is-still-fast.html#workarounds",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Workarounds",
    "text": "Workarounds\nThese are good workarounds. I would suggest looking at ndexr.io as a way to scale these R based services as well. They utilize the NGINX approach described here."
  },
  {
    "objectID": "posts/2023-07-06-r-is-still-fast.html#addenda",
    "href": "posts/2023-07-06-r-is-still-fast.html#addenda",
    "title": "R is still fast: a salty reaction to a salty blog post",
    "section": "Addenda",
    "text": "Addenda\nClearly, this is where I care a lot. I am the author of Valve. Valve is exactly what the author was clamoring for in the beginning of the blog post. It is a web server that runs Plumber APIs in parallel written in Rust using Tokio, Axum, and Deadpool. Valve auto-scales on its own up to a maximum number of worker threads. So it’s not always taking up space and running more compute than it needs.\nValve overview:\n\nConcurrent webserver to auto-scale plumber APIs\nwritten in Rust using Tokio, Axum, and Deadpool\nspawns and kills plumber APIs based on demand\nintegration with {vetiver} of of the box\n\nFirst things first, I want to address “it’s not on CRAN.” You’re right. That’s because it is a Rust crate. Crates don’t go on CRAN. I’ve made an R package around it to lower the bar to entry. But it is a CLI tool at the core.\nObviously, it is new. It is untested. I wish I could tell everyone to use it, but I can’t. I think anyone who used it would be floored by its performance and ease of use. It is SO simple.\nI’ll push it to crates.io and CRAN in the coming weeks. Nothing like h8rs to inspire."
  },
  {
    "objectID": "posts/2019-08-04-my-parts.html",
    "href": "posts/2019-08-04-my-parts.html",
    "title": "∑ { my parts }",
    "section": "",
    "text": "library(tidyverse)\n\nterrorists &lt;- readr::read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSqNhpFX_69klKgJCVobc3fjHYVE9pNosrDi9h6irLlCtSSLpR704iu9VqI7CxdRi0iKt3p1FDYbu8Y/pub?gid=956062857&single=true&output=csv\")\n\n\n\n\n\nterrorist_by_race\n#&gt; # A tibble: 7 × 6\n#&gt;   race                n fatalities injured total_victims   `%`\n#&gt;   &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 white              63        554    1067          1621 69.3 \n#&gt; 2 other               5         90     115           205  8.77\n#&gt; 3 black              19        108      89           197  8.43\n#&gt; 4 asian               8         77      33           110  4.70\n#&gt; 5 unclear             6         40      61           101  4.32\n#&gt; 6 latino             10         44      33            77  3.29\n#&gt; 7 native american     3         19       8            27  1.15\n\n\n\n\n\nterrorist_by_gender\n#&gt; # A tibble: 3 × 6\n#&gt;   gender            n fatalities injured total_victims    `%`\n#&gt;   &lt;chr&gt;         &lt;int&gt;      &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 male            110        903    1380          2283 97.6  \n#&gt; 2 male & female     1         14      21            35  1.50 \n#&gt; 3 female            3         15       5            20  0.855\n\n\nterrorist &lt;- c(\"angry\", \"white\", \"male\")\nmy_parts &lt;- c(\"angry\", \"white\", \"male\")\n\n\nmy_parts == terrorist\n#&gt; [1] TRUE TRUE TRUE\n\n\n`I am` &gt; sum(my_parts)\n\n\n#&gt; [1] TRUE\n\n\n`I am` == sum(terrorist)\n\n\nFALSE\n#&gt; [1] FALSE\n\n\nwhite_males &lt;- filter(terrorists,\n                      race == \"white\",\n                      tolower(gender) == \"male\",\n                      !is.na(name))\n\npull(white_males, name)\n#&gt;  [1] \"Jordan Witmer\"             \"Zephen A. Xaver\"           \"Robert D. Bowers\"         \n#&gt;  [4] \"Jarrod W. Ramos\"           \"Dimitrios Pagourtzis\"      \"Travis Reinking\"          \n#&gt;  [7] \"Nikolas J. Cruz\"           \"Timothy O'Brien Smith\"     \"Kevin Janson Neal\"        \n#&gt; [10] \"Devin Patrick Kelley\"      \"Scott Allen Ostrem\"        \"Stephen Craig Paddock\"    \n#&gt; [13] \"Randy Stair\"               \"Thomas Hartless\"           \"Jason B. Dalton\"          \n#&gt; [16] \"Robert Lewis Dear\"         \"Noah Harpham\"              \"Dylann Storm Roof\"        \n#&gt; [19] \"Elliot Rodger\"             \"John Zawahri\"              \"Kurt Myers\"               \n#&gt; [22] \"Adam Lanza\"                \"Andrew Engeldinger\"        \"Wade Michael Page\"        \n#&gt; [25] \"James Holmes\"              \"Ian Stawicki\"              \"Scott Evans Dekraai\"      \n#&gt; [28] \"Jared Loughner\"            \"Robert Stewart\"            \"Wesley Neal Higdon\"       \n#&gt; [31] \"Steven Kazmierczak\"        \"Robert A. Hawkins\"         \"Tyler Peterson\"           \n#&gt; [34] \"Sulejman Talović\\u0087\"    \"Charles Carl Roberts\"      \"Kyle Aaron Huff\"          \n#&gt; [37] \"Terry Michael Ratzmann\"    \"Nathan Gale\"               \"Douglas Williams\"         \n#&gt; [40] \"Michael McDermott\"         \"Larry Gene Ashbrook\"       \"Day trader Mark O. Barton\"\n#&gt; [43] \"Eric Harris\"               \"Kipland P. Kinkel\"         \"Mitchell Scott Johnson\"   \n#&gt; [46] \"Matthew Beck\"              \"Dean Allen Mellberg\"       \"Kenneth Junior French\"    \n#&gt; [49] \"Gian Luigi Ferri\"          \"John T. Miller\"            \"Eric Houston\"             \n#&gt; [52] \"Thomas McIlvane\"           \"George Hennard\"            \"Joseph T. Wesbecker\"      \n#&gt; [55] \"Patrick Purdy\"             \"Richard Farley\"            \"William Cruse\"            \n#&gt; [58] \"Patrick Sherrill\"          \"James Oliver Huberty\"      \"Abdelkrim Belachheb\"      \n#&gt; [61] \"Carl Robert Brown\"\n\n\nam_i &lt;- function(terrorist) {\n  msg &lt;- paste(\"am i ==\", terrorist)\n  print(msg)\n  print(`am i` == terrorist)\n}\n\n\npull(white_males, name) %&gt;% \n  walk(~am_i(.))\n#&gt; [1] \"`am i` == Jordan Witmer\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Zephen A. Xaver\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert D. Bowers\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Jarrod W. Ramos\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Dimitrios Pagourtzis\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Travis Reinking\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Nikolas J. Cruz\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Timothy O'Brien Smith\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kevin Janson Neal\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Devin Patrick Kelley\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Scott Allen Ostrem\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Stephen Craig Paddock\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Randy Stair\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Thomas Hartless\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Jason B. Dalton\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert Lewis Dear\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Noah Harpham\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Dylann Storm Roof\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Elliot Rodger\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == John Zawahri\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kurt Myers\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Adam Lanza\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Andrew Engeldinger\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Wade Michael Page\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == James Holmes\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Ian Stawicki\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Scott Evans Dekraai\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Jared Loughner\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert Stewart\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Wesley Neal Higdon\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Steven Kazmierczak\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Robert A. Hawkins\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Tyler Peterson\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Sulejman Talović\\u0087\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Charles Carl Roberts\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kyle Aaron Huff\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Terry Michael Ratzmann\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Nathan Gale\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Douglas Williams\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Michael McDermott\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Larry Gene Ashbrook\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Day trader Mark O. Barton\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Eric Harris\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kipland P. Kinkel\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Mitchell Scott Johnson\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Matthew Beck\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Dean Allen Mellberg\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Kenneth Junior French\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Gian Luigi Ferri\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == John T. Miller\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Eric Houston\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Thomas McIlvane\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == George Hennard\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Joseph T. Wesbecker\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Patrick Purdy\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Richard Farley\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == William Cruse\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Patrick Sherrill\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == James Oliver Huberty\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Abdelkrim Belachheb\"\n#&gt; [1] FALSE\n#&gt; [1] \"`am i` == Carl Robert Brown\"\n#&gt; [1] FALSE\n\n\n`I am` &gt; sum(my_parts)"
  },
  {
    "objectID": "posts/2018-10-27-read_multiple_csv.html",
    "href": "posts/2018-10-27-read_multiple_csv.html",
    "title": "Reading Multiple csvs as 1 data frame",
    "section": "",
    "text": "In an earlier posting I wrote about having to break a single csv into multiple csvs. In other scenarios one data set maybe provided as multiple a csvs.\nThankfully purrr has a beautiful function called map_df() which will make this into a two liner. This process has essentially 3 steps.\n\nCreate a vector of all .csv files that should be merged together.\nRead each file using readr::read_csv()\nCombine each dataframe into one.\n\nmap_df() maps (applys) a function to each value of an object and produces a dataframe of all outputs.\nFor this example I will use the csvs I created in a previous tutorial utilizing a dataset from the Quantitative Social Science book.\n\n# Get all csv file names \nfile_names &lt;- list.files(\"../../static/data/chunk_data\", pattern = \"\\\\.csv\", full.names = TRUE)\nfile_names\n#&gt;  [1] \"../../static/data/chunk_data/social_chunked_1.csv\" \n#&gt;  [2] \"../../static/data/chunk_data/social_chunked_10.csv\"\n#&gt;  [3] \"../../static/data/chunk_data/social_chunked_11.csv\"\n#&gt;  [4] \"../../static/data/chunk_data/social_chunked_12.csv\"\n#&gt;  [5] \"../../static/data/chunk_data/social_chunked_13.csv\"\n#&gt;  [6] \"../../static/data/chunk_data/social_chunked_2.csv\" \n#&gt;  [7] \"../../static/data/chunk_data/social_chunked_3.csv\" \n#&gt;  [8] \"../../static/data/chunk_data/social_chunked_4.csv\" \n#&gt;  [9] \"../../static/data/chunk_data/social_chunked_5.csv\" \n#&gt; [10] \"../../static/data/chunk_data/social_chunked_6.csv\" \n#&gt; [11] \"../../static/data/chunk_data/social_chunked_7.csv\" \n#&gt; [12] \"../../static/data/chunk_data/social_chunked_8.csv\" \n#&gt; [13] \"../../static/data/chunk_data/social_chunked_9.csv\"\n\n\nlibrary(tidyverse)\n# apply \nall_csvs &lt;- map_df(file_names, read_csv)\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 5866 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n#&gt; Rows: 25000 Columns: 6\n#&gt; ── Column specification ────────────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (2): sex, messages\n#&gt; dbl (4): yearofbirth, primary2004, primary2006, hhsize\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# preview the data\nhead(all_csvs)\n#&gt; # A tibble: 6 × 6\n#&gt;   sex    yearofbirth primary2004 messages   primary2006 hhsize\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 male          1941           0 Civic Duty           0      2\n#&gt; 2 female        1947           0 Civic Duty           0      2\n#&gt; 3 male          1951           0 Hawthorne            1      3\n#&gt; 4 female        1950           0 Hawthorne            1      3\n#&gt; 5 female        1982           0 Hawthorne            1      3\n#&gt; 6 male          1981           0 Control              0      3"
  },
  {
    "objectID": "posts/2021-04-28-user-apr-27-2021.html",
    "href": "posts/2021-04-28-user-apr-27-2021.html",
    "title": "{cpcinema} & associated journey",
    "section": "",
    "text": "Yesterday I had the chance to discuss my R package, {cpcinema} with the Boston useR group. If you’re not familiar with {cpcinema}, please read my previous post. I intended for my talk to be focused on the package’s functionality, how / why I made it, and then briefly on why I didn’t share it widely, my feelings after my seeing the response to my tweet, and why contributing to the open source community in any manner is always appreciated. But as I was preparing my slides on Monday night I encountered some challenges. That became the subject of much of my talk.\nThe talk can be found here and the passcode is D=L?nHQ0.\n\n\n\n\nAfter providing a brief overview of the functionality of {cpcinema} I touched upon how this idea formed in my head. My head is like an unorganized floating abyss with numerous ideas, concepts, and facts just floating. Occasionally I’ll make a connection between two or more ideas. This will happen over a long period of time. In the case of this package I had a number of thoughts regarding data visualization—probably because I had seen Alberto Cairo present at Northeastern towards the end of 2019.\n\nGraphs are informative\nGraphs can be boring\nPeople like pretty things\nPretty graphs are just as informative as normal graphs\nMake graphs pretty and people will enjoy them\n\nAdditionally, I had come to the realization that most data scientists want to make informative charts, but also aren’t going to be exceptionally adept at the design portion of this—nor should they be! Moving beyond the default colors is an important first step in making evocative visualizations.\nSometime in between, I discovered or was shown the instagram page @colorpalette.cinema which provides beautiful color palettes from stills of films. How sweet would it be to use those colors for your plots? Rather sweet.\nIn January at rstudio::conf(2020L) I saw Jesse Sadler present on making custom S3 vector classes with the {vctrs} package (slides). After seeing this I was toying with the idea of making my own custom S3 vector class.\nThen in February it all clicked. How cool would it be to get a colorpalette.cinema picture, extract the colors, and then store them in a custom S3 class which printed the color?! So I did that with hours of struggling and a lot of following along with Jesse’s resources.\nTo start any project of any sort, always **start small*. My workflow consisted of:\n\nDownloading a sample image to work with.\nScour StackOverflow, Google, and Rbloggers for resources on extracting colors.\nExtract the colors from an image manually.\nExtract colors from an image path.\nMake it into a function to replicate.\nTry and get an image from Instagram URL.\n\nWell, apparently you can’t! I tried and I tried to find different ways to extracting these images. But it wasn’t possible. The Instagram API is meant for web developers and businesses, not data scientists trying to get small pieces of data. I left it alone and left it to myself.\nAfter my tweet, I was inspired to find a solution to the Instagram image URL problem. And with a lot of coffee, wifi hotpsotting, the Chrome devtools, I found a workable solution.\nThen Monday night I was making slides and…\n\npal_from_post(\"https://www.instagram.com/p/CMC26FaHbqP/\")\n\nError in download.file(fp, tmp, mode = \"wb\", quiet = TRUE) : \n  cannot open URL 'NA'\nThe function pal_from_post() didn’t work. I did what I always do when a function doesn’t work. I printed out the function body to figure out what the function does.\n\ncpcinema::pal_from_post\n#&gt; function (post_url) \n#&gt; {\n#&gt;     res &lt;- httr::POST(\"https://igram.io/api/\", body = list(url = post_url, \n#&gt;         lang_code = \"en\", vers = \"2\"), encode = \"form\") %&gt;% httr::content()\n#&gt;     imgs &lt;- res %&gt;% rvest::html_nodes(\".py-2 img\") %&gt;% rvest::html_attr(\"src\") %&gt;% \n#&gt;         unique()\n#&gt;     img_paths &lt;- imgs[2:length(imgs)]\n#&gt;     purrr::map(img_paths, extract_cpc_pal)\n#&gt; }\n#&gt; &lt;bytecode: 0x294dac290&gt;\n#&gt; &lt;environment: namespace:cpcinema&gt;\n\nAlways start the beginning. I ran the original API query and I received the following\n\nnonation &lt;- \"https://www.instagram.com/p/CNdEkV4HAXF\"\n\nres &lt;- httr::POST(\"https://igram.io/api/\",\n                  body = list(url = nonation,\n                              lang_code = \"en\",\n                              vers = \"2\"),\n                  encode  = \"form\") \n\nResponse [https://igram.io/api/]\n  Date: 2021-04-26 22:46\n  Status: 403\n  Content-Type: text/html; charset=UTF-8\n  Size: 3.26 kB\n&lt;!DOCTYPE html&gt;\n&lt;!--[if lt IE 7]&gt; &lt;html class=\"no-js ie6 oldie\" lang=\"en-US\"&gt; &lt;![endif]--&gt;\n&lt;!--[if IE 7]&gt;    &lt;html class=\"no-js ie7 oldie\" lang=\"en-US\"&gt; &lt;![endif]--&gt;\n&lt;!--[if IE 8]&gt;    &lt;html class=\"no-js ie8 oldie\" lang=\"en-US\"&gt; &lt;![endif]--&gt;\n&lt;!--[if gt IE 8]&gt;&lt;!--&gt; &lt;html class=\"no-js\" lang=\"en-US\"&gt; &lt;!--&lt;![endif]--&gt;\n&lt;head&gt;\n&lt;title&gt;Access denied | igram.io used Cloudflare to restrict access&lt;/title&gt;\n&lt;meta charset=\"UTF-8\" /&gt;\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" /&gt;\n&lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge,chrome=1\" /&gt;\n...\nMy next step here was to see if it was just me. Next I went to use RStudio Server Pro which has a different IP address. The same query had the different response\nResponse [https://igram.io/api/]\n  Date: 2021-04-26 22:43\n  Status: 503\n  Content-Type: text/html; charset=UTF-8\n  Size: 9.48 kB\n&lt;!DOCTYPE HTML&gt;\n&lt;html lang=\"en-US\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\" /&gt;\n  &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" /&gt;\n  &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge,chrome=1\" /&gt;\n  &lt;meta name=\"robots\" content=\"noindex, nofollow\" /&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" /&gt;\n  &lt;title&gt;Just a moment...&lt;/title&gt;\n  &lt;style type=\"text/css\"&gt;\nA different error code. I saw this coming.\nI was web scraping a website that was unquestionably also web scraping. This is a very grey zone of the internet and is questionable at best. These sources are ephemeral at best.\n\n“If it doesn’t work, try the same thing again until it does work” - Me\n\nI went to the same website I was using. I opened opened up the developer tools and watched the requests come in! The request that the browser uses had a change in their url! From https://igram.io/api/ to https://igram.io/i/. Frankly, a super easy fix!\nIt’s now Wednesday and I still haven’t made that change. So, what’s next? (HELP WANTED)\n\nChange the API url in pal_from_post()\nIntegrate with ggplot2 better. Inspo can be taken from {paletteer}\nColor sorting!\nBetter type casting!"
  },
  {
    "objectID": "posts/2019-08-09-epa-waterquality.html",
    "href": "posts/2019-08-09-epa-waterquality.html",
    "title": "Water Quality Analysis",
    "section": "",
    "text": "This small tutorial was developed for a talk / workshop that Phil Bowsher gave at the EPA. This serves as a quick example of using the tidyverse for spatial analysis, modeling, and interactive mapping.\nThe source code and data can be found here."
  },
  {
    "objectID": "posts/2019-08-09-epa-waterquality.html#data-cleaning",
    "href": "posts/2019-08-09-epa-waterquality.html#data-cleaning",
    "title": "Water Quality Analysis",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThis section outlines the process needed for cleaning data taken from EPA.gov.\nThere are two datasets:\n\nWater Chemistry\nSample Site Information\n\nThe first dataset contains data pertaining to water quality samples at a given site. The second data set contains information relating to that site such as latitude and longitude data. We will need to combine these datasets.\nIn order to join any two datasets there must be a common field(s). This case it is the site_id.\nThe below code chunk:\n\nLoad the tidyverse\nCreates variables that store the URL of the csv files\nRead the datasets and standardizes the column names using the clean_names() function from janitor.\n\n\nlibrary(tidyverse)\n\n# identify water quality csv\nwater_url &lt;- \"https://www.epa.gov/sites/production/files/2014-10/nla2007_chemical_conditionestimates_20091123.csv\"\n\n# site info csv (w lat lon data)\nsite_url &lt;- \"https://www.epa.gov/sites/production/files/2014-01/nla2007_sampledlakeinformation_20091113.csv\"\n\n# read sites\nsites &lt;- read_csv(site_url) %&gt;% \n  janitor::clean_names()\n\n# read water\nwater_q &lt;- read_csv(water_url) %&gt;% \n  janitor::clean_names()\n\nNow that we have these datasets we will need to join them together.In this case we will join three tables together:\n\nWater Quality dataset\nSite location data\nState abbreviation and region data\n\nWe first take only a few columns of interest from the sites dataset. This is then piped (%\\&gt;% ) into an inner_join() (all columns from x and y where there is a match between x and y). The resultant table is then passed forward into a left_join() (all columns from x and y where returning all rows from x). In this join the y table is explicitly created from the built in R objects state.abb and state.region. Then, a select() statement is used to change some column names, select only the columns of interest. Finally, the tibble is written to the data directory (run mkdir(\"data\")) if the directory does not exist.\n\n# join together\nclean &lt;- select(sites, lon_dd, lat_dd, lakename, site_id, state_name, st) %&gt;% \n  inner_join(water_q, by = \"site_id\") %&gt;% \n  # join a table that has region info\n  left_join(\n    tibble(st = state.abb,\n           region = state.region), by = c(\"st.y\" = \"st\")\n  ) %&gt;% \n  # select only data of interest\n  select(contains(\"_cond\"), ptl, ntl, chla, st = st.x, region,\n         lon_dd = lon_dd.x, lat_dd = lat_dd.x, lakename)\n\n\n#write_csv(clean, \"data/water_quality.csv\")"
  },
  {
    "objectID": "posts/2019-08-09-epa-waterquality.html#mapping-data",
    "href": "posts/2019-08-09-epa-waterquality.html#mapping-data",
    "title": "Water Quality Analysis",
    "section": "Mapping data",
    "text": "Mapping data\nTo map data we can take advantage of leaflet and sf. We will create a simple feature object which has a column containing geometry information. We use st_as_sf() to convert to a spatial object. Use the argument coords to tell which columns correspond to latitude and logitude.\n\nlibrary(sf)\n\n\nwater_sf &lt;- water %&gt;% \n  st_as_sf(coords = c(\"lon_dd\", \"lat_dd\"))\n\nclass(water_sf)\n#&gt; [1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n## [1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\nYou can see now that this is still a data frame but is also of class sf.\nWe can use this sf object to plot some markers with leaflet.\n\nlibrary(leaflet)\n\nleaflet(water_sf) %&gt;% \n        addTiles() %&gt;% \n        addMarkers()\n\n\n\n\n```\n\nThis creates markers for each measurement, but it would be nice to have a popup message associated with each one. We can create a message with mutate() and glue(). Note that the &lt;br&gt; tag is an html tag that creates a new line.\n\nwater_sf %&gt;% \n  mutate(msg = glue::glue({\n    \"Name: {lakename}&lt;br&gt;\n    Chlorphylla: {chla}&lt;br&gt;\n    Nitrogen: {ntl}&lt;br&gt;\n    Phosphorus: {ptl}&lt;br&gt;\"})) %&gt;% \n  leaflet() %&gt;% \n  addTiles() %&gt;% \n  addMarkers(popup = ~msg)\n\n```{=html}\n\n\n```"
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html",
    "title": "What is critical race theory, anyways?",
    "section": "",
    "text": "TL;DR critical race theory is a mental framework used for understanding racial inequality that focuses on power imbalances.\nTrump recently suggested that all educational institutions stop teaching critical race theory and, if they fail to do so, lose funding. Like most things Trump does I was appalled. But this came from a different place. This came from a fear of educational censorship and suppression of science.\nBut what the hell is critical race theory, anyways? What does it matter?\nTo understand critical race theory we need to understand where it came from. Critical race theory came from the sociological critical theory. And sociological critical theory came from what is called conflict theory. And conflict theory came from—now don’t lose it—Karl Marx. Let’s try and grasp each theory in chronological order from when they were created to understand how each new theory came to be.\nThe very rough timeline looks like the below."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#historical-materialism",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#historical-materialism",
    "title": "What is critical race theory, anyways?",
    "section": "Historical Materialism",
    "text": "Historical Materialism\nOkay, Karl Marx. Sure, sure, he’s considered the father of communism. But he is really so much more than that. His theories have been absolutely critical to the social sciences for decades.\nMarx believed that there were two broad categories of people in a society, the proletariat and the bourgeoisie. Think of them as the workers and the business owners / managers, respectively. These two groups are always in tension with each other. Workers want better pay and better working conditions. Business owners want to save costs by paying workers less to increase their earnings at the margin (margin is jargon for each additional unit of goods and services). In order to get along they each need to concede to achieve something in the middle ground. This meeting in the middle is how progress is supposedly made.\n\n“Meeting in the middle” is a simplification of the German philosopher Hegel’s idea of a dialectic. A dialectic is described as “thesis, antithesis, synthesis.” In normal people words a dialectic is two opposites (thesis and antithesis) creating something (synthesis)."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#conflict-theory",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#conflict-theory",
    "title": "What is critical race theory, anyways?",
    "section": "Conflict Theory",
    "text": "Conflict Theory\nConflict theory is a bit more general than Marx’s Historical Materialism. Conflict theory suggests that social structures are created from power struggles between different groups of people—not just proletariat and bourgeoisie. One group may have more authority and resources at hand and are using it to the detriment of the other group. We can think of students and teacher, homeless and housed, secular and religious, so on and so forth.\nIn sociology, conflict theory is considered the antithesis (the direct opposite) of functionalism. Functionalism states that each social institution exists to serve some purpose. For example, policing serves the social function of reducing crime. Functionalists tend to think that’s a good thing. Conflict theorists are likely to disagree because they see the power imbalance between the oppressed and the oppressor causing crime. These social institutions that we create tend to reinforce the status quo."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-theory",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-theory",
    "title": "What is critical race theory, anyways?",
    "section": "Critical Theory",
    "text": "Critical Theory\nOkay so we’re getting closer to critical race theory. To recap:\n\nWe can trace critical race theory’s origin to Marx’s Historical Materialism.\nHistorical Materialism says that history is a product of economic struggle between workers and business owners.\nConflict theory generalizes Marx’s theory to say that social structures are created by tension between groups based on interests, resources, and power.\n\nCritical theory is famously defined as\n\n“an essential element in historical effort to create a world which satisfied the needs and powers of men…[and] its goal is man’s emancipation from slavery” - Horkheimer\n\nCritical theory is essentially conflict theory but with an embedded social critique. It’s goal is to improve the human condition by illustrating power imbalance and to improve the conditions of the oppressed. Because of this desire to improve society, critical theory is often used by activists.\n\n\n\n\n\n\nThe above diagram attempts to illustrate the hierarchical nature of these theories."
  },
  {
    "objectID": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-race-theory",
    "href": "posts/2020-09-14-what-the-hell-is-critical-race-theory.html#critical-race-theory",
    "title": "What is critical race theory, anyways?",
    "section": "Critical Race Theory",
    "text": "Critical Race Theory\nNow making the leap from conflict theory and critical theory to critical race theory isn’t all that difficult. We understand that there is historical conflict between groups of people. This conflict creates social structures and reinforces the relative power of one group to another. The created social structures perpetuate and often exacerbate inequalities. A lot of times the inequalities between groups, from a philosophical standpoint, are incongruent with our beliefs and need to be rectified or improved.\nCritical race theory, then, is the application of critical theory to the concept of race. In the American context we can understand critical race theory going all the way back to the 17th century.\n\nCritical Race Theory in the United States: the really, really, and I cannot stress this enough, really, short version\n(White) Europeans and then Americans enslaved Africans to be their source of labor. Europeans built institutions to maintain slave trading. The economy of the new world was built almost entirely on “free” labor. Social structures were modified to reinforce ownership rights between people and of people.\nThen one day on July 4th, 1776 this really important document was written which said\n\n“We hold these truths to be self-evident, that all men [people] are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.” – Declaration of Independence\n\nBut that wasn’t true at all. Those words were aspirational at best. In the south, slavery was so deeply embedded that a life without it was deemed problematic enough to start a war over. When Mississippi seceded from the United States prior to the Civil War they enumerated their grievances (much like the Declaration of Independence did) to justify their departure from the US and wrote:\n\n“Our position is thoroughly identified with the institution of slavery—the greatest material interest of the world. Its labor supplies the product which constitutes by far the largest and most important portions of commerce of the earth.” – A Declaration of the Immediate Causes which Induce and Justify the Secession of the State of Mississippi from the Federal Union, 1861\n\nIt wasn’t until 1865 when the 13th Amendment was ratified, 1868 for the 14th Amendment and still that wasn’t enough. In 1896 we had the landmark Plessy v. Ferguson court case which paved the way for Jim Crow laws—a.k.a. American apartheid. It wasn’t until 1964 until the Civil Rights Act was passed. But the passing of laws doesn’t doesn’t change our thinking or our behaviors right away. We still have social structures and institutions which condition and alter our thinking. They don’t disappear with the stroke of a pen.\nIn short, sh!t has been f*cked up in the United States for a very long time and things still aren’t perfect. They’re better. But they’re not good. Critical race theory helps us understand how we got here, why we’re still having problems, and suggests some ways that we can improve it.\nIf we suppress critical race theory we’re also tossing aside critical theory, conflict theory, and historical materialism. These are theories that have helped us make sense of the world for over one hundred years. If we allow this censorship we’re giving way to an unjust power imbalance—the very thing these theories help us understand. If we throw away textbooks we’re walking down the same path that was taken in 1930s Germany and in 1960s China. If we allow suppression of free thought we’re giving way to authoritarians who don’t want you to challenge injustices."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "recent posts",
    "section": "",
    "text": "Writing S3 head() methods\n\n\na note to self for later\n\n\n\npkg-dev\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a DataFusion CSV reader with arrow-extendr\n\n\nextending R with Arrow and Rust\n\n\n\nrust\n\n\npkg-dev\n\n\nextendr\n\n\narrow\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nEnums in R: towards type safe R\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhere am I in the sky?\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhere am I in the sky?\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExport Python functions in R packages\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExport Python functions in R packages\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Science Across Languages\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Science Across Languages\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nValve: putting R in production\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nR is still fast: a salty reaction to a salty blog post\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s so special about arrays?\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFeeling rusty: counting characters\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFeeling rusty: counting characters\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nRust traits for R users\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nlearning rust\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nJHU talk (slides)\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nRaw strings in R\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically Create Formulas in R\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nYouTube Videos & what not\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFishnets and overlapping polygons\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nComplete spatial randomness\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nspacetime representations aren’t good—yet\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMake your R scripts Databricks notebooks\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Spatial Data Analysis in R\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMy new IDE theme: xxEmoCandyLandxx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nActually identifying R package System Requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe heck is a statistical moment??\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Python on my M1 in under 10 minutes\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSLICED! a brief reflection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n{cpcinema} & associated journey\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nOSINT in 7 minutes\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nAPIs: the language agnostic love story\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nPython & R in production — the API way\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nColor Palette Cinema\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nSecure R Package Environments\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is critical race theory, anyways?\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nDemographic Change, White Fear, and Social Construction of Race\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nMedium Data and Production API Pipeline\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Red Queen Effect\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nExcel in pRod\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nDesign Paradigms in R\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nR Security Concerns and 2019 CRAN downloads\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nNon-interactive user tokens with googlesheets4\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nFinding an SPSS {haven}\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Tidy Modeling\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWater Quality Analysis\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n∑ { my parts }\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Trends for Campaigns\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nWeb-scraping for Campaigns\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing trendyy\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\ngenius tutorial\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\ngenius Plumber API\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Fallacy of one person, one vote\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cost of Gridlock\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nxgboost feature importance\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\n[Not so] generic functions\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nUS Representation: Part I\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing: Letters to a layperson\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nChunking your csv\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nReading Multiple csvs as 1 data frame\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nCoursera R-Programming: Week 2 Problems\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing geniusR\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\nJosiah Parry\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "posts"
    ]
  },
  {
    "objectID": "posts/2020-09-07-demographic-change-white-fear-and-social-construction-of-race.html",
    "href": "posts/2020-09-07-demographic-change-white-fear-and-social-construction-of-race.html",
    "title": "Demographic Change, White Fear, and Social Construction of Race",
    "section": "",
    "text": "Two or three weeks ago, somewhere between Carter Dome and Mount Hight in the White Mountains of New Hampshire my friend posed a thought experiment to me. It’s one that I have heard dozens of times whether at a bar top, a fire pit, or an inflatable tube on the Pemigewasset River. It goes something like this.\n\nNote that this is rather extreme example and may not be comfortable for some readers. But thought experiments are supposed to be uncomfortable.\n\n“Take the country Iceland, it has a small population of about 350,000. Say, 100,000 Chinese immigrants move to the country within the period of a year. Is it still Iceland?”\n“Yes, of course.”\n“Okay, say this new population brings a massive baby boom. We know the fertility rate in China is much greater than that of Iceland. This new population has parity with the original 350,000 Icelanders. Making 700,000 total. A massive election is held and there is complete overturn of elected officials and each new official is either from the massive Chinese influx or immediate descendants of the Chinese immigrants. This new government enacts laws that greatly resemble China. Is this country no longer Iceland? What about the Icelandic culture? How can it be preserved? Are you okay with the destruction of a culture?”\nAt this point, for some reason, I’ve always found it tough to provide an argument that can persuade him. Upon reflection, it’s likely because the conversation shifts abruptly from one of pure demographic consideration to one of cultural preservation. The thought experiment feels challenging mainly because the idea of an ethnic and cultural Iceland is portrayed as some static, unshifting, unyielding, monolith. And that is what is at the crux of this.\nThere is an extant fear of racial elimination as a product of demographic growth. Research shows that when white individuals learn about a projected demographic shift from being a majority to minority of the population they show racial preferences for their identified race (source). This has consequences for political party preference as well. White Americans who express concern become more “conservative”—a term I increasingly struggle to use or condone the use of—political views and lead to a great partisan divide (source). Rather prescient, right?\nIceland, while they do not maintain official statistics on race, we do know that approximately 94% of the population are ethnically Icelandic. If we take the complement as entirely people of color (POC) that makes Iceland at most 6% POC. It is likely much less. But what does it mean to be ethnically Icelandic?\nIceland is a discovered land. At the time of its settlement by Norwegians in the 9th century, the land was uninhabited. Icelandic settlers, confirmed by genomic study, are largely from the Scandinavian countries, Ireland, and Scotland. Thus, in the one thousand and change years since its inception, ethnic Icelanders were derived from a melange of Northern Europeans. It would be unreasonable to think that sex would only occur between people of the same homeland indefinitely—that small genetic pool would lead to things like the Hapsburg Jaw. This is illustrative of two points pertinent to the thought experiment.\n\nEthnic Icelanders are descendants of other ethnic groups. Or, put another way, ethnicity is a social construction.\nThe movement of people is a constant in human history.\n\nSay, for the sake of the mental experiment, we give way to the idea that there is an Icelandic culture which can be nailed down and is not in flux. When was it in its purest state? Surely, if we take a snapshot of Icelandic culture of today, it would be unrecognizable to people a century ago, or maybe considerably different than even a few decades ago.\nIf, however, we define culture as an artifact of the history of Iceland—as we rightly should—where the past is important in informing the present, then we must be willing to concede that was is happening presently will become context for understanding Icelandic culture in the future. And that what will happen is soon to be the present and, following, the past (time is a construct I still don’t fully grasp). This is all to say is that culture is a constantly changing (in a state of flux) and that the concept of indefinite cultural preservation is unattainable—and, I’d argue, undesirable. We must accept that populations grow and change; that movement of peoples is a constant in human history; that culture is not a monolith and is constantly shifting; and that ethnicity and race is are myths.\nAt the end of the day, his thought experiment isn’t so much a thought experiment but rather an argument against in-migration. People will move across borders—another social construction, but perhaps with more contemporary utility than that of ethnicity—and the directionality is not only in, but it is also out. For every immigration problem there is an emigration problem. Call me a globalist, but I believe international borders should be more open.\nThere is a scene from Parks and Recreation where Leslie Knope refers to Ann Perkins as racially ambiguous. Today, this might be a problematic statement, but it is a reality of the future.\n\nAll of our descendants within a few generation will be ethnically ambiguous and new ethnic and racial identities will emerge. Fearing change solely based on the color of others’ skin and your preconceptions of them is not a good reason for fearing change. In this thought experiment, the concern ought not be about culture and race. But rather the focus of my argument should have been that of infrastructure. How can we ensure that there is enough housing? Nourishment? Education? Opportunity? You know, the things that truly matter to humanity."
  },
  {
    "objectID": "posts/2019-05-25-introducing-trendyy.html",
    "href": "posts/2019-05-25-introducing-trendyy.html",
    "title": "Introducing trendyy",
    "section": "",
    "text": "trendyy is a package for querying Google Trends. It is build around Philippe Massicotte’s package gtrendsR which accesses this data wonderfully.\nThe inspiration for this package was to provide a tidy interface to the trends data."
  },
  {
    "objectID": "posts/2019-05-25-introducing-trendyy.html#getting-started",
    "href": "posts/2019-05-25-introducing-trendyy.html#getting-started",
    "title": "Introducing trendyy",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstallation\nYou can install trendyy from CRAN using install.packages(\"trendyy\").\n\n\nUsage\nUse trendy() to search Google Trends. The only mandatory argument is search_terms. This is a character vector with the terms of interest. It is important to note that Google Trends is only capable of comparing up to five terms. Thus, if your search_terms vector is longer than 5, it will search each term individually. This will remove the direct comparative advantage that Google Trends gives you.\n\nAdditional arguments\n\n\nfrom: The beginning date of the query in \"YYYY-MM-DD\" format.\nto: The end date of the query in \"YYYY-MM-DD\" format.\n... : any additional arguments that would be passed to gtrendsR::gtrends(). Note that it might be useful to indicate the geography of interest. See gtrendsR::countries for list of possible geographies.\n\n\n\nAccessor Functions\n\nget_interest(): Retrieve interest over time\nget_interest_city(): Retrieve interest by city\nget_interest_country(): Retrieve interest by country\nget_interest_dma(): Retrieve interest by DMA\nget_interest_region(): Retrieve interest by region\nget_related_queries(): Retrieve related queries\nget_related_topics(): Retrieve related topics"
  },
  {
    "objectID": "posts/2019-05-25-introducing-trendyy.html#example",
    "href": "posts/2019-05-25-introducing-trendyy.html#example",
    "title": "Introducing trendyy",
    "section": "Example",
    "text": "Example\nSeeing as I found an interest in this due to the relatively pervasive use of Google Trends in political analysis, I will compare the top five polling candidates in the 2020 Democratic Primary. As of May 22nd, they were Joe Biden, Kamala Harris, Beto O’Rourke, Bernie Sanders, and Elizabeth Warren.\nFirst, I will create a vector of my desired search terms. Second, I will pass that vector to trendy() specifying my query date range from the first of 2019 until today (May 25th, 2019).\n\ncandidates &lt;- c(\"Joe Biden\", \"Kamala Harris\", \"Beto O'Rourke\", \"Bernie Sanders\", \"Elizabeth Warren\")\n\ncandidate_trends &lt;- trendy(candidates, from = \"2019-01-01\", to = Sys.Date())\n\nNow that we have a trendy object, we can print it out to get a summary of the trends.\n\ncandidate_trends\n#&gt; ~Trendy results~\n#&gt; \n#&gt; Search Terms: Joe Biden, Kamala Harris, Beto O'Rourke, Bernie Sanders, Elizabeth Warren\n#&gt; \n#&gt; (&gt;^.^)&gt; ~~~~~~~~~~~~~~~~~~~~ summary ~~~~~~~~~~~~~~~~~~~~ &lt;(^.^&lt;)\n#&gt; # A tibble: 5 × 5\n#&gt;   keyword          max_hits min_hits from       to        \n#&gt;   &lt;chr&gt;               &lt;int&gt;    &lt;int&gt; &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 Bernie Sanders         21        1 2019-01-06 2022-11-06\n#&gt; 2 Beto O'Rourke           1        0 2019-01-06 2022-11-06\n#&gt; 3 Elizabeth Warren        8        1 2019-01-06 2022-11-06\n#&gt; 4 Joe Biden             100        1 2019-01-06 2022-11-06\n#&gt; 5 Kamala Harris          48        1 2019-01-06 2022-11-06\n\nIn order to retrieve the trend data, use get_interest(). Note, that this is dplyr friendly.\n\nget_interest(candidate_trends)\n#&gt; # A tibble: 1,005 × 7\n#&gt;    date                 hits keyword   geo   time                  gprop category      \n#&gt;    &lt;dttm&gt;              &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;                 &lt;chr&gt; &lt;chr&gt;         \n#&gt;  1 2019-01-06 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  2 2019-01-13 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  3 2019-01-20 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  4 2019-01-27 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  5 2019-02-03 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  6 2019-02-10 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  7 2019-02-17 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  8 2019-02-24 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt;  9 2019-03-03 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt; 10 2019-03-10 00:00:00     1 Joe Biden world 2019-01-01 2022-11-14 web   All categories\n#&gt; # … with 995 more rows\n#&gt; # ℹ Use `print(n = ...)` to see more rows\n\n\nPlotting Interest\n\ncandidate_trends %&gt;% \n  get_interest() %&gt;% \n  ggplot(aes(date, hits, color = keyword)) +\n  geom_line() +\n  geom_point(alpha = .2) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"\", \n       y = \"Relative Search Popularity\",\n       title = \"Google Search Popularity\")\n\n\n\nIt is also possible to view the related search queries for a given set of keywords using get_related_queries().\n\ncandidate_trends %&gt;% \n  get_related_queries() %&gt;% \n  group_by(keyword) %&gt;% \n  sample_n(2)\n#&gt; # A tibble: 10 × 5\n#&gt; # Groups:   keyword [5]\n#&gt;    subject  related_queries value                        keyword          category      \n#&gt;    &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;                        &lt;chr&gt;            &lt;chr&gt;         \n#&gt;  1 +3,450%  rising          klobuchar                    Bernie Sanders   All categories\n#&gt;  2 81       top             joe biden                    Bernie Sanders   All categories\n#&gt;  3 32       top             kamala harris                Beto ORourke     All categories\n#&gt;  4 Breakout rising          beto orourke announcement    Beto ORourke     All categories\n#&gt;  5 Breakout rising          elizabeth warren beer video  Elizabeth Warren All categories\n#&gt;  6 40       top             elizabeth warren net worth   Elizabeth Warren All categories\n#&gt;  7 Breakout rising          joe biden stimulus           Joe Biden        All categories\n#&gt;  8 Breakout rising          joe biden senile             Joe Biden        All categories\n#&gt;  9 Breakout rising          kamala harris husbands       Kamala Harris    All categories\n#&gt; 10 30       top             vice president kamala harris Kamala Harris    All categories\n\n\n\nUseful Resources\n\nHow Trends Data Is Adjusted\nPost by Google News Lab"
  },
  {
    "objectID": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html",
    "href": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html",
    "title": "Exploratory Spatial Data Analysis in R",
    "section": "",
    "text": "Geospatial data is becoming increasingly common across domains and industries. Spatial data is no longer only in the hands of soil scientists, meteorologists, and criminologists, but in marketing, retail, finance, etc. It is common for spatial data to be treated as any other tabular data set. However, there is information to be drawn from our data’s relation to space. The standard exploratory data analysis toolkit will not always suffice. In this talk I introduce the basics of exploratory spatial data analysis (ESDA) and the {sfdep} package. {sfdep} builds on the shoulders of {spdep} for spatial dependence, emphasizes the use of simple features and the {sf} package, and integrates within your tidyverse-centric workflow. By the end of this talk users will understand the basics of ESDA and know how to start incorporating these skills in their own work."
  },
  {
    "objectID": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#about-the-talk",
    "href": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#about-the-talk",
    "title": "Exploratory Spatial Data Analysis in R",
    "section": "",
    "text": "Geospatial data is becoming increasingly common across domains and industries. Spatial data is no longer only in the hands of soil scientists, meteorologists, and criminologists, but in marketing, retail, finance, etc. It is common for spatial data to be treated as any other tabular data set. However, there is information to be drawn from our data’s relation to space. The standard exploratory data analysis toolkit will not always suffice. In this talk I introduce the basics of exploratory spatial data analysis (ESDA) and the {sfdep} package. {sfdep} builds on the shoulders of {spdep} for spatial dependence, emphasizes the use of simple features and the {sf} package, and integrates within your tidyverse-centric workflow. By the end of this talk users will understand the basics of ESDA and know how to start incorporating these skills in their own work."
  },
  {
    "objectID": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#recording",
    "href": "posts/2022-05-11-exploratory-spatial-data-analysis-in-r.html#recording",
    "title": "Exploratory Spatial Data Analysis in R",
    "section": "Recording",
    "text": "Recording"
  },
  {
    "objectID": "posts/2022-02-12-the-heck-is-a-statistical-moment.html",
    "href": "posts/2022-02-12-the-heck-is-a-statistical-moment.html",
    "title": "The heck is a statistical moment??",
    "section": "",
    "text": "I wrote myself a short story to help me remember what the moments are.\n\n“The first moment I looked at the distribution I thought only of the average. The second, I thought of the variance. Soon after, I thought then of the skewness. Only then did I think about the kurtosis.”\n\nThis all started when reading Luc Anselin’s “Spatial Regression Analysis in R: A Workbook”, I encountered the following:\n\n“Under the normal assumption for the null, the theoretical moments of Moran’s I only depend on the characteristics of the weights matrix.”\n\nThe moments? The what? Under the normal assumption of my study habits I would skip over this word and continue to the next sentence. However, this was critical for understanding the formula for Moran’s I: \\(E[I] = \\frac{-1}{n - 1}\\).\nWikipedia was likely written by the same gatekeepers. I turned to the article on “Method of moments (statistics)” which writes\n\n“Those expressions are then set equal to the sample moments. The number of such equations is the same as the number of parameters to be estimated. Those equations are then solved for the parameters of interest. The solutions are estimates of those parameters.”\n\nNaturally, I turned to twitter to vent.\n\n\nThe use of the word \"moment\" in statistics is cruel.\n\n— jos (@JosiahParry) October 23, 2021\n\n\nThanks to Nicole Radziwill for a very helpful link from the US Naval Academy.\nThe method of moments is no more than simple summary statistics from a distribution. There are four “moments”.\n\nMean\nVariance\nSkew\nKurtosis\n\nWhy would one use these words? To gatekeep, of course. Academia uses needlessly complex language quite often.\nRemember, friends. Use clear and concise language. Let’s remove “moments” from our statistical lexicon."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html",
    "href": "posts/2022-10-03-spacetime-representations.html",
    "title": "spacetime representations aren’t good—yet",
    "section": "",
    "text": "My beliefs can be summarized somewhat succinctly.\nWe should not limit space-time data to dates or timestamps.\nThe R ecosystem should always utilize a normalized approach as described above. Further, a representation should use friendly R objects. The friendliest object is a data frame. A new representation should allow context switching between geometries and temporal data. That new representation should always use time-long formats and the geometries should never be repeated.\nA spacetime representation should give users complete and total freedom to manipulate their data as they see fit (e.g. dplyr or data.table operations).\nThe only time to be strict in the format of spacetime data is when statstics are going to be derived from the data."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#background",
    "href": "posts/2022-10-03-spacetime-representations.html#background",
    "title": "spacetime representations aren’t good—yet",
    "section": "Background",
    "text": "Background\nWhile implementing emerging hotspot analysis in sfdep I encountered the need for a formalized spacetime class in R. As my focus in sfdep has been tidyverse-centric functionality, I desired a “tidy” data frame that could be used as a spacetime representation. Moreover, space (in the spacetime representation) should be represented as an sf or sfc object. In sfdep I introduced the new S3 class spacetime based on Edzer Pebesma’s 2012 article “spacetime: Spatio-Temporal Data in R” and Thomas Lin Pederson’s tidygraph package."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#representations-of-spatial-data",
    "href": "posts/2022-10-03-spacetime-representations.html#representations-of-spatial-data",
    "title": "spacetime representations aren’t good—yet",
    "section": "Representations of Spatial Data",
    "text": "Representations of Spatial Data\nBefore describing my preferences in a spacetime representation in R, I want to review possible representations of spacetime data.\nPebesma (2012) outlines three tabular representations of spatio-temporal data.\n\n“Time-wide: Where different columns reflect different moments in time.\n\nSpace-wide: Where different columns reflect different measurement locations or areas.\n\nLong formats: Where each record reflects a single time and space combination.\n\nThe “long format” is what we may consider “tidy” per Wickham (2014). In this case, both time and space are variables with unique combinations as rows.\nPebesma further qualifies spatial data representation into a “sparse grid” and a “full grid.” Say we have a variable X. In a spatio temporal full grid we will store all combinations of time (t) and locations (i) . If Xi is missing at any of those location and time combinations (Xit is missing), the value of X is recorded as a missing value. Whereas in a sparse grid, if there is any missing data, the observation is omitted. Necessarily, in a full grid there will be i x t number of rows. In a sparse grid there will be fewer than i x t rows.\nVery recently in an r-spatial blog post, “Vector Data Cubes”, Edzer describes another approach to representing spacetime using a database normalization approach. Database normalization is a process that reduces redundancy by creating a number of smaller tables containing IDs and values. These tables can then be joined only when needed. When we consider spacetime data, we have repeating geometries across time. It is inefficient to to keep multiple copies of the geometry. Instead, we can keep track of the unique ID of a geometry and store the geometry in another table."
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#sfdep-spacetime-representation",
    "href": "posts/2022-10-03-spacetime-representations.html#sfdep-spacetime-representation",
    "title": "spacetime representations aren’t good—yet",
    "section": "sfdep spacetime representation",
    "text": "sfdep spacetime representation\nThe spacetime class in sfdep is in essence a database normalization approach (see above blog post). It is implemented with the database normalization approach and the ergonomics of tidygraph in mind.\nThe objective of the spacetime class in sfdep is to\n\nallow complete freedom of data manipulation via data.frame objects,\nprevent duplication of geometries,\nand provide leeway in what “time” can be defined as.\n\nSimilar to tidygraph, spacetime provides access to two contexts: data and geometry. The data context is a data frame and the geometry context. These are linked based on a unqie identifie that is present in both contexts.\nR code\n\nlibrary(dplyr)\n\ntimes &lt;- seq(\n  Sys.time(), \n  Sys.time() + lubridate::hours(5),\n  length.out = 5\n)\n\nlocations &lt;- c(\"001\", \"002\")\n\ndata_context &lt;- tidyr::crossing(\n  location = locations,\n  time = times\n) |&gt; \n  mutate(value = rnorm(n())) |&gt; \n  arrange(location)\n\n\nlibrary(sf)\n\nLinking to GEOS 3.9.1, GDAL 3.2.3, PROJ 7.2.1; sf_use_s2() is TRUE\n\ngeometry_context &lt;- st_sfc(\n  list(st_point(c(0, 1)), st_point(c(1, 1)))\n  ) |&gt; \n  st_as_sf() |&gt; \n  mutate(location = c(\"001\", \"002\"))\n\nUse the spacetime constructor\n\nlibrary(sfdep)\nspt &lt;- spacetime(\n  .data = data_context,\n  .geometry = geometry_context, \n  .loc_col = \"location\", \n  .time_col = \"time\"\n)\n\nSwap contexts with activate\nactivate(spt, \"geometry\")\nspacetime ────\nContext:`geometry`\n2 locations `location`\n5 time periods `time`\n── geometry context ────────────────────────────────────────────────────────────\nSimple feature collection with 2 features and 1 field Geometry type: POINT Dimension: XY Bounding box: xmin: 0 ymin: 1 xmax: 1 ymax: 1 CRS: NA x location 1 POINT (0 1) 001 2 POINT (1 1) 002\nOne of my very strong beliefs is that temporal data does not, and should not, always be represented as a date or a timestamp. This paradigm is too limiting. What about panel data where you’re measuring cohorts along periods 1 - 10? Should these be represented as dates? No, definitely not. Because of this, sfdep allows you to utilize any numeric column that can be sorted.\n\nPerhaps I’ve just spent too much time listening to ecometricians…\n\nexample of using integers\n\nspacetime(\n  mutate(data_context, period = row_number()),\n  geometry_context, \n  .loc_col = \"location\",\n  .time_col = \"period\"\n)\n\nspacetime ────\n\n\nContext:`data`\n\n\n2 locations `location`\n\n\n10 time periods `period`\n\n\n── data context ────────────────────────────────────────────────────────────────\n\n\n# A tibble: 10 × 4\n   location time                 value period\n * &lt;chr&gt;    &lt;dttm&gt;               &lt;dbl&gt;  &lt;int&gt;\n 1 001      2022-11-15 08:23:08 -0.206      1\n 2 001      2022-11-15 09:38:08 -0.289      2\n 3 001      2022-11-15 10:53:08 -0.275      3\n 4 001      2022-11-15 12:08:08  0.136      4\n 5 001      2022-11-15 13:23:08 -1.48       5\n 6 002      2022-11-15 08:23:08  1.01       6\n 7 002      2022-11-15 09:38:08  2.11       7\n 8 002      2022-11-15 10:53:08 -1.68       8\n 9 002      2022-11-15 12:08:08  0.880      9\n10 002      2022-11-15 13:23:08  0.698     10"
  },
  {
    "objectID": "posts/2022-10-03-spacetime-representations.html#qualifiers",
    "href": "posts/2022-10-03-spacetime-representations.html#qualifiers",
    "title": "spacetime representations aren’t good—yet",
    "section": "Qualifiers",
    "text": "Qualifiers\nI don’t think my spacetime class is the panacea. I don’t have the technical chops to make a great data format. I also don’t want to have that burden. Additionally, the class is desgned with lattice data in mind. I don’t think it is sufficient for trajectories or point pattern without repeating locations.\nThere’s a new R package called cubble for spatio-temporal data. I’ve not explored it. It may be better suited to your tidy-centric spatio-temporal data."
  },
  {
    "objectID": "posts/2022-05-11-make-your-r-scripts-databricks-notebooks.html",
    "href": "posts/2022-05-11-make-your-r-scripts-databricks-notebooks.html",
    "title": "Make your R scripts Databricks notebooks",
    "section": "",
    "text": "I’ve never had a good reason to deviate from the canonical .R file extension until today.\nAs you may have seen over the past few month from my numerous rage-tweets and Databricks related threads, I’ve been doing a lot of work getting figuring out Databricks as an R user so we can get onboard with adoption here at NPD.\nOne of my biggest qualms about Databricks is that it’s tailored to their notebooks. The notebooks get magical superpowers that aren’t available anywhere else. Notebooks get root permissions, they have access to dbutils, and are the only thing that can actually be scheduled by Databricks outside of a jar file or SparkSQL code.\nI’ve spent quite a bit of time thinking about how we can schedule R scripts through a notebook. If you’re wondering, have the notebook kickoff the R script with a shell command.\nBut, alas, I’ve learned something today. If you connect your Git repo to Databricks through their \"Repos\", you can have your R scripts be accessible as notebooks with quite literally only two changes.\nFirst, R scripts need to have the less-preferable, though equally functional, file extension .r. Second, the first line of the script should be a comment that says # Databricks notebook source. And that’s it. Then once the git repo has been connected, it will recognize those as notebooks.\nIf you want to create cells in your code write a comment # COMMAND ----------—that’s 10 hyphens at the end.\nIf you create a file main.r which contains the body\n# Databricks notebook source\n\nprint(\"Hello world\")\n\n# COMMAND ---------\n\nprint(\"Databricks! I've figured you out—sorta....\")\n\n# COMMAND ---------\n\nprint(\"I feel powerful.\")\nYou will have an R script that is recognized as a notebook by Databricks that can be scheduled using Databricks’ scheduling wizard."
  },
  {
    "objectID": "posts/2020-12-09-secure-package-environment.html",
    "href": "posts/2020-12-09-secure-package-environment.html",
    "title": "Secure R Package Environments",
    "section": "",
    "text": "One of the biggest challenges faced by public sector organizations and other security conscious groups is package management. These groups are typically characterized air gapped network environments—i.e. no internet connectivity to the outside world. The purpose of an air gapped network is to get rid of any possibility of an intrusion in your network from an unwanted visitor. Air gapped installations come with some challenges particularly with package management.\nTypically, when you want to install a new package it comes from The Comprehensive R Archive Network (CRAN). While CRAN has a comprehensive testing system as part of their software development life cycle, security teams are still hesitant to trust any domains outside of their network.\nAt RStudio, our solution to this is our RStudio Package Manager or RSPM for short. “RStudio Package Manager is a repository management server to organize and centralize R packages across your team, department, or entire organization.” With RSPM there are a few different ways of addressing this concern. Here I’ll walk through some different approaches. Each subsequent approach is stricter than the last.\n\n\nThe first approach to do this is to install and configure RSPM in your air gapped network. However, RSPM will need special permission to reach out to our sync service (https://rspm-sync.rstudio.com). In many cases security teams are willing to open up an outbound internet connection to just RStudio’s sync service—we hope you trust us!\nThis solution is the easiest as it is a quick configuration. Moreover, packages will only be installed as they are requested. Giving access to the sync service also enables your team to be able to download the latest versions of packages from CRAN.\n\n\n\nThe limitation of the first approach is that is permits a constant outbound internet connection. For some groups, this is a no go. The next best approach then is to have a completely air-gapped CRAN mirror. To do this you will need an internet connection for a brief amount of time—there’s no way to have data magically appear on your server! During the brief period in which your proxy is open you will have to copy all of CRAN, binaries and source, into your server. RSPM provides a utility tool to do this. Once complete, you can close your network again and be confident that there is no possibility of having any connection with the outside world.\nOnce you’ve completed moving data into your server everything behaves as expected—just ensure your options('repos') is set properly. The one downside to this approach is that you will not be able to have access to the latest versions of packages. To rectify this, you can sync on periodic basis.\n\n\n\nOften there are even further restrictions placed on data scientists which limit what packages can be used for their work. We refer to this as a validated set of packages or a curated CRAN. Packages are often “validated” and through that validation process are promoted to the CRAN repository. The upside to this approach is that teams can be confident in the packages their team are using.\nSome approaches to validating the package environment include selecting the top n packages from CRAN (post on identifying those packages here), having a subject matter expert provide a list of preferred packages, or a ticketing system. The ticketing system is the least scalable, most restrictive, and will likely hinder your work. I don’t recommend it.\nThe limitations with this are rather straight forward: your data scientists do not have too much leeway in utilizing packages that may expedite or even enable their work.\n\n\n\nWith approach 3 there are usually two repositories: 1) a mirror of CRAN and 2) a subset of CRAN. While the subset of CRAN is preferred there is nothing stopping users from using the CRAN repository if they know the URL. To prevent this you can implement strict rules with your proxy to prevent users installing from the CRAN mirror thus forcing users to use the subset. In essence, approach 4 is approach 3 but with an enforcement mechanism."
  },
  {
    "objectID": "posts/2020-12-09-secure-package-environment.html#securing-your-r-package-environment",
    "href": "posts/2020-12-09-secure-package-environment.html#securing-your-r-package-environment",
    "title": "Secure R Package Environments",
    "section": "",
    "text": "One of the biggest challenges faced by public sector organizations and other security conscious groups is package management. These groups are typically characterized air gapped network environments—i.e. no internet connectivity to the outside world. The purpose of an air gapped network is to get rid of any possibility of an intrusion in your network from an unwanted visitor. Air gapped installations come with some challenges particularly with package management.\nTypically, when you want to install a new package it comes from The Comprehensive R Archive Network (CRAN). While CRAN has a comprehensive testing system as part of their software development life cycle, security teams are still hesitant to trust any domains outside of their network.\nAt RStudio, our solution to this is our RStudio Package Manager or RSPM for short. “RStudio Package Manager is a repository management server to organize and centralize R packages across your team, department, or entire organization.” With RSPM there are a few different ways of addressing this concern. Here I’ll walk through some different approaches. Each subsequent approach is stricter than the last.\n\n\nThe first approach to do this is to install and configure RSPM in your air gapped network. However, RSPM will need special permission to reach out to our sync service (https://rspm-sync.rstudio.com). In many cases security teams are willing to open up an outbound internet connection to just RStudio’s sync service—we hope you trust us!\nThis solution is the easiest as it is a quick configuration. Moreover, packages will only be installed as they are requested. Giving access to the sync service also enables your team to be able to download the latest versions of packages from CRAN.\n\n\n\nThe limitation of the first approach is that is permits a constant outbound internet connection. For some groups, this is a no go. The next best approach then is to have a completely air-gapped CRAN mirror. To do this you will need an internet connection for a brief amount of time—there’s no way to have data magically appear on your server! During the brief period in which your proxy is open you will have to copy all of CRAN, binaries and source, into your server. RSPM provides a utility tool to do this. Once complete, you can close your network again and be confident that there is no possibility of having any connection with the outside world.\nOnce you’ve completed moving data into your server everything behaves as expected—just ensure your options('repos') is set properly. The one downside to this approach is that you will not be able to have access to the latest versions of packages. To rectify this, you can sync on periodic basis.\n\n\n\nOften there are even further restrictions placed on data scientists which limit what packages can be used for their work. We refer to this as a validated set of packages or a curated CRAN. Packages are often “validated” and through that validation process are promoted to the CRAN repository. The upside to this approach is that teams can be confident in the packages their team are using.\nSome approaches to validating the package environment include selecting the top n packages from CRAN (post on identifying those packages here), having a subject matter expert provide a list of preferred packages, or a ticketing system. The ticketing system is the least scalable, most restrictive, and will likely hinder your work. I don’t recommend it.\nThe limitations with this are rather straight forward: your data scientists do not have too much leeway in utilizing packages that may expedite or even enable their work.\n\n\n\nWith approach 3 there are usually two repositories: 1) a mirror of CRAN and 2) a subset of CRAN. While the subset of CRAN is preferred there is nothing stopping users from using the CRAN repository if they know the URL. To prevent this you can implement strict rules with your proxy to prevent users installing from the CRAN mirror thus forcing users to use the subset. In essence, approach 4 is approach 3 but with an enforcement mechanism."
  },
  {
    "objectID": "posts/2020-12-09-secure-package-environment.html#review",
    "href": "posts/2020-12-09-secure-package-environment.html#review",
    "title": "Secure R Package Environments",
    "section": "Review",
    "text": "Review\nPackage management isn’t easy. It’s even tougher in an offline environment. You’re not going to be able to know exactly what every package does. You’re going to have to make tradeoffs. You can secure your package environment by migrating packages into your own network. You can implement progressively stricter rules to reduce your exposure to potential packages. RStudio Package Manager is a wonderful tool that will make accomplishing all of this a whole lot easier.\nFeel free to reach out to me via twitter or email and we can talk this through."
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html",
    "href": "posts/2019-03-30-plumber-genius-api.html",
    "title": "genius Plumber API",
    "section": "",
    "text": "get started here\nSince I created genius, I’ve wanted to make a version for python. But frankly, that’s a daunting task for me seeing as my python skills are intermediate at best. But recently I’ve been made aware of the package plumber. To put it plainly, plumber takes your R code and makes it accessible via an API.\nI thought this would be difficult. I was so wrong."
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#using-plumber",
    "href": "posts/2019-03-30-plumber-genius-api.html#using-plumber",
    "title": "genius Plumber API",
    "section": "Using plumber",
    "text": "Using plumber\nPlumber works by using roxygen like comments (#*). Using a single comment, you can define the request type and the end point. Following that you define a function. The arguments to the funciton become the query parameters.\nThe main genius functions only require two main arguments artist and album or song. Making these accessible by API is as simple as:\n#* @get /track\nfunction(artist, song) {\n  genius::genius_lyrics(artist, song)\n}\nWith this line of code I created an endpoint called track to retrieve song lyrics. The two parameters as defined by the anonymous function are artist and song. This means that song lyrics are accessible with a query looking like http://hostname/track?artist=artist_name&song=song_name.\nBut as it stands, this isn’t enough to host the API locally. Save your functions with plumber documentation into a file (I named mine plumber.R).\n\nCreating the API\nCreating the API is probably the easiest part. It takes quite literally, two lines of code. The function plumb() takes two arguments, the file which contains your plumber commented code, and the directory that houses it.\nplumb() creates a router which is “responsible for taking an incoming request, submitting it through the appropriate filters.”\nI created a plumber router which would be used to route income queries.\npr &lt;- plumb(\"plumber.R\")\nThe next step is to actually run the router. Again, this is quite simple by calling the run() method of the pr object. All I need to do is specify the port that the API will listen on, and optionally the host address.\npr$run(port = 80, host = \"0.0.0.0\")\nNow I can construct queries in my browser. An example query is http://localhost/track?artist=andrew%20bird&song=proxy%20war. Sending this request produces a very friendly json output.\n[\n{\"track_title\":\"Proxy War\",\"line\":1,\"lyric\":\"He don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":2,\"lyric\":\"She don't to have get over him\"},\n{\"track_title\":\"Proxy War\",\"line\":3,\"lyric\":\"With all their words preserved forevermore\"},\n{\"track_title\":\"Proxy War\",\"line\":4,\"lyric\":\"You don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":5,\"lyric\":\"She don't have to get over you\"},\n{\"track_title\":\"Proxy War\",\"line\":6,\"lyric\":\"It's true these two have never met before\"},\n{\"track_title\":\"Proxy War\",\"line\":7,\"lyric\":\"At least not in real life\"},\n{\"track_title\":\"Proxy War\",\"line\":8,\"lyric\":\"Where your words cut like a knife\"},\n{\"track_title\":\"Proxy War\",\"line\":9,\"lyric\":\"Conjuring blood, biblical floods\"},\n{\"track_title\":\"Proxy War\",\"line\":10,\"lyric\":\"Looks that stop time\"},\n{\"track_title\":\"Proxy War\",\"line\":11,\"lyric\":\"You don't have to remember\"},\n{\"track_title\":\"Proxy War\",\"line\":12,\"lyric\":\"We forget what memories are for\"},\n{\"track_title\":\"Proxy War\",\"line\":13,\"lyric\":\"Now we store them in the atmosphere\"},\n{\"track_title\":\"Proxy War\",\"line\":14,\"lyric\":\"If you don't want to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":15,\"lyric\":\"You don't have to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":16,\"lyric\":\"It's just what we're calling peer-to-peer\"},\n{\"track_title\":\"Proxy War\",\"line\":17,\"lyric\":\"You don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":18,\"lyric\":\"You don't have to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":19,\"lyric\":\"We store them in the atmosphere\"},\n{\"track_title\":\"Proxy War\",\"line\":20,\"lyric\":\"If you don't want to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":21,\"lyric\":\"She don't have to get over you\"},\n{\"track_title\":\"Proxy War\",\"line\":22,\"lyric\":\"It's true these two have never met before\"},\n{\"track_title\":\"Proxy War\",\"line\":23,\"lyric\":\"At least not in real life\"},\n{\"track_title\":\"Proxy War\",\"line\":24,\"lyric\":\"Where your words cut like a knife\"},\n{\"track_title\":\"Proxy War\",\"line\":25,\"lyric\":\"Conjuring blood, biblical floods\"},\n{\"track_title\":\"Proxy War\",\"line\":26,\"lyric\":\"Looks that stop time\"},\n{\"track_title\":\"Proxy War\",\"line\":27,\"lyric\":\"If you don't want to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":28,\"lyric\":\"You don't have to get over her\"},\n{\"track_title\":\"Proxy War\",\"line\":29,\"lyric\":\"If you don't want to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":30,\"lyric\":\"You don't have to get too close\"},\n{\"track_title\":\"Proxy War\",\"line\":31,\"lyric\":\"If you want to remember\"},\n{\"track_title\":\"Proxy War\",\"line\":32,\"lyric\":\"If you don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":33,\"lyric\":\"If you don't want to get over\"},\n{\"track_title\":\"Proxy War\",\"line\":34,\"lyric\":\"If you don't want to get over\"}\n]"
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#writing-a-python-wrapper",
    "href": "posts/2019-03-30-plumber-genius-api.html#writing-a-python-wrapper",
    "title": "genius Plumber API",
    "section": "Writing a Python Wrapper",
    "text": "Writing a Python Wrapper\nOne of the appeals of writing an API is that it can be accessed from any language. This was the inspiriation of creating this API. I want to be able to call R using Python. Creating an API is a great intermediary as writing an API wrapper is much easier for me than recreating all of the code that I wrote in R.\nI want to be able to recreate the three main functions of genius. These are genius_lyrics(), genius_album(), and genius_tracklist(). In doing this there are two steps I have to consider. The first is creating query urls, and the second is parsing json.\nTo create the urls, the requests library is used. Next, I created a template for the urls.\nimport requests\nurl_template = \"http://localhost:80/track?artist={}&song={}\"\nThe idea here is that the {} characters will be filled with provided parameters by using the .format() method.\nFor example, if I wanted to get lyrics for Proxy War by Andrew Bird, I would supply \"Andrew Bird\" and \"Proxy War\" as the arguments to format(). It’s important to note that these arguments are taken positionally. The url is created using this method.\nurl = url_template.format(\"andrew bird\", \"proxy war\")\nNow I am at the point where I can ping the server to receive the json. This is accomplished by using the .get() method from requests.\nresponse = requests.get(url)\nThis returns an object that contains the json response. Next, in order to get this into a format that can be analysed, it needs to be parsed. I prefer a Pandas DataFrame, and fortunately Pandas has a lovely read_json function. I will call the .content attribute of the response objectm and feed that into the read_json() function.\nimport pandas as pd\n\nproxy_war = pd.read_json(response.content)\n\nproxy_war.head()\n\n    line    lyric                                   track_title\n0   1   He don't have to get over her               Proxy War\n1   2   She don't to have get over him              Proxy War\n2   3   With all their words preserved forevermore  Proxy War\n3   4   You don't have to get over her              Proxy War\n4   5   She don't have to get over you              Proxy War\nBeautiful. Song lyrics are available through this API and can easily be accessed via python. The next step is to generalize this and the other two functions. The below is the code to create the genius_lyrics() function in python. It works almost identically as in R. However, at this moment it does not have the ability to set the info argument. But this can be changed easily in the original plumber.R file.\n# Define genius_lyrics()\ndef genius_lyrics(artist, song):\n\n    url_template = \"http://localhost:80/track?artist={}&song={}\"\n    \n    url = url_template.format(artist, song)\n\n    response = requests.get(url)\n    \n    song = pd.read_json(response.content)\n    \n    return(song)\nAt this point I’m feeling extremely stoked on the fact that I can use genius with python. Who says R and python practitioners can’t work together?"
  },
  {
    "objectID": "posts/2019-03-30-plumber-genius-api.html#containerize-with-docker",
    "href": "posts/2019-03-30-plumber-genius-api.html#containerize-with-docker",
    "title": "genius Plumber API",
    "section": "Containerize with Docker",
    "text": "Containerize with Docker\n\nTo make the process of setting up this genius API up easier for those who don’t necessarily interact with R, I created a lightweight-ish Docker container. The idea for this was to be able to pull a Docker image, run a command, and then the API will be available on a local port without having to interact with R at all.\nI’m not the most experience person with creating Docker containers but I can borrow code quite well. Fortunately I came across some wonderful slides from rstudio::conf 2019. Heather Nollis and Jacqueline Nolis presented on “API development with R and TensorFlow at T-Mobile”.\nThis container needs two things: a linux environment and an installation of R with plumber, genius, and its dependencies. An organization called The Rocker Project has created a number of Docker images that are stable and easy to install.\nSince genius relies on many packages from the tidyverse, the rocker/tidyverse image was used. To use their wonderful image, only one line is needed in my Dockerfile.\n# Import existing Docker image\nFROM rocker/tidyverse:3.5.2\nNow, not knowing exactly what I was doing, I copied code from Jacqueline and Heather’s sample Dockerfile in their slides. Their comment says that this is necessary to have the “needed linux libraries for plumber”, I went with it.\n# install needed linux libraries for plumber\nRUN apt-get update -qq && apt-get install -y \\\n  libssl-dev \\\n  libcurl4-gnutls-dev\ngenius and plumber are not part of the tidyverse image and have to be installed manually. The following lines tell Docker to run the listed R commands. For some unknown reason there was an issue with installing genius from CRAN so the repos argument was stated explicitly.\n# Install R packages\nRUN R -e \"install.packages('genius', repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('plumber')\"\nIn addition to the Dockerfile there are two files in my directory which are used to launch the API. The plumber.R and launch_api.R files. These need to be copied into the container. The line COPY / / copies from the location / in my directory to the location / in the container.\nThe Docker image has the libraries and files needed, but it needs to be able to actually launch the API. Since the plumber.R file specifies that the API will be listening on port 80, I need to expose that port in my Docker image using EXPOSE 80.\nThe last part of this is to run the launch_api.R so the API is available. The ENTRYPOINT command tells Docker what to run when the container is launched. In this case ENTRYPOINT [\"Rscript\", \"launch_api.R\"] tells Docker to run the Rscript command with the argument launch_api.R. And with that, the Dockerfile is complete and read to run.\nThe image needs to be built and ran. The simplest way to do this for me was to work from Dockerhub. Thus to run this container only three lines of code are needed!\ndocker pull josiahparry/genius-api:working\n\ndocker build -t josiahparry/genius-api .\n\ndocker run --rm -p 80:80 josiahparry/genius-api\n\nBoom, now you have an API that will be able to use the functionality of genius. If you wish to use Python with the API, I wrote a simple script which creates a nice tidy wrapper around it.\n\nIf anyone is interested in writing a more stable Python library that can call the functionality described above I’d love your help to make genius more readily available to the python community."
  },
  {
    "objectID": "posts/2020-06-12-red-queen.html",
    "href": "posts/2020-06-12-red-queen.html",
    "title": "The Red Queen Effect",
    "section": "",
    "text": "The Red Queen and maintenance of state and society\nIt’s Monday morning. You’re back at work after a few days off. Your inbox is a lot more full than you hoped with 70 emails. Time to get reading and sending. It’s been an hour and you’ve read and sent at least 20 emails but your inbox is still at 70. You’ve been working hard and yet it feels like you’ve gone nowhere. This idea of working really hard but feeling like you’ve gone nowhere is at the center of the Red Queen Effect.\nThere are a number of “Red Queen Effect”s in the scientific literature all of which are inspired from the Red Queen’s race from Lewis Carrols’s Through the Looking Glass.\n\n“Well, in our country,” said Alice, still panting a little, “you’d generally get to somewhere else—if you run very fast for a long time, as we’ve been doing.”\n“A slow sort of country!” said the Queen. “Now, here, you see, it takes all the running you can do, to keep in the same place. If you want to get somewhere else, you must run at least twice as fast as that!” \n\nAlice is running and running and getting nowhere. Much like how as you read emails you get new ones. Daron Acemoglu and James A. Robinson adapt this concept to the evolution of states and connect it to their idea of the Narrow Corridor. The path to a “successful” society is a race between the power of the people and the power of the state. States in which the pace of growth in both the peoples’ power and the power / ability of the state are similar often produce more liberal (in the sense of liberty) nations.\n\nThis graphic is meant to illustrate this race. Getting into the corridor is a game of chase between the power of the state and society. Keeping the balance is delicate act—one that no nation has perfected—which requires society to check the power of the state and the state to provide checks to society. They define the Red Queen as\n\n“The process of competition, struggle and cooperation between state and society”"
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html",
    "href": "posts/2020-01-13-gs4-auth.html",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "",
    "text": "This repository contains an example of an R Markdown document that uses googlesheets4 to read from a private Google Sheet and is deployed to RStudio Connect.\nThe path of least resistance for Google auth is to sit back and respond to some interactive prompts, but this won’t work for something that is deployed to a headless machine. You have to do some advance planning to provide your deployed product with a token.\nThe gargle vignette Non-interactive auth is the definitive document for how to do this. The gargle package handles auth for several packages, such as bigrquery, googledrive, gmailr, and googlesheets4.\nThis repo provides a detailed example for the scenario where you are using an OAuth2 user token for a product deployed on RStudio Connect (see vignette section Project-level OAuth cache from which this was adapted). Note that service account tokens are the preferred strategy for a deployed product, but sometimes there are reasons to use a user token."
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html#authenticating",
    "href": "posts/2020-01-13-gs4-auth.html#authenticating",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "Authenticating",
    "text": "Authenticating\nLoad the googlesheets4 package.\n\nlibrary(googlesheets4)\n\nBy default, gargle uses a central token store, outside of the project, which isn’t going to work for us. Instead we specify a project level directory .secrets which will contain our Google token. We will set the gargle_oauth_cache option to refer to this .secrets directory. We can check where the token will be cached with gargle::gargle_oauth_cache().\n\n# designate project-specific cache\noptions(gargle_oauth_cache = \".secrets\")\n\n# check the value of the option, if you like\ngargle::gargle_oauth_cache()\n\nNext we will have to perform the interactive authentication just once. Doing this will generate the token and store it for us. You will be required to select an email account to authenticate with.\n\n# trigger auth on purpose --&gt; store a token in the specified cache\n# a broswer will be opened\ngooglesheets4::sheets_auth()\n\nNow that you have completed the authentication and returned to R, we can double check that the token was cached in .secrets.\n\n# see your token file in the cache, if you like\nlist.files(\".secrets/\")\n\nVoila! Let’s deauthorize in our session so we can try authenticating once more, but this time without interactivity.\n\n# deauth\nsheets_deauth()\n\nIn sheets_auth() we can specify where the token is cached and which email we used to authenticate.\n\n# sheets reauth with specified token and email address\nsheets_auth(\n  cache = \".secrets\",\n  email = \"josiah@email.com\"\n  )\n\nAlternatively, we can specify these in the options() and run the authentication without an arguments supplied. Let’s first deauth in our session to try authenticating again.\n\n# deauth again\nsheets_deauth()\n\n# set values in options\noptions(\n  gargle_oauth_cache = \".secrets\",\n  gargle_oauth_email = \"josiah@email.com\"\n)\n\n# run sheets auth\nsheets_auth()\n\nNow that we are sure that authorization works without an interactive browser session, we should migrate the options into an .Rprofile file. This way, when an R session is spun up the options will be set from session start. Meaning, if you use sheets_auth() within your R Markdown document it will knit without having to open the browser."
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html#deploying-to-connect",
    "href": "posts/2020-01-13-gs4-auth.html#deploying-to-connect",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "Deploying to Connect",
    "text": "Deploying to Connect\nIn order for the deployment to RStudio Connect to work, the .secrets directory and .Rprofile files need to be in the bundle. Be sure to do this from the Add Files button. If you cannot see the files because they are hidden from Finder you cran press cmnd + shift + .. Then publish!"
  },
  {
    "objectID": "posts/2020-01-13-gs4-auth.html#other-google-platforms",
    "href": "posts/2020-01-13-gs4-auth.html#other-google-platforms",
    "title": "Non-interactive user tokens with googlesheets4",
    "section": "Other Google Platforms",
    "text": "Other Google Platforms\nThis same process can be replicated for other packages that use gargle authentication. By virtue of having gargle as the central auth package for Google APIs, the workflow outlined here, and the others in the non-interactive auth vignette, can can be utilized for other google API packages (i.e. googledrive).\n\n# authenticate with googledrive and create a token\ngoogledrive::drive_auth()\n\nThank you to Jenny Bryan for her help editing this!"
  },
  {
    "objectID": "posts/2022-11-23-youtube-videos.html",
    "href": "posts/2022-11-23-youtube-videos.html",
    "title": "YouTube Videos & what not",
    "section": "",
    "text": "Please vote or post a comment in this discuss on what would be helpful for you.\n\nI first made R programming videos when I had the opportunity to teach a remote and asynchronous course called Big Data for Cities. I used the videos as alternative learning material besides a text-book and other required readings.\nI just recently noticed that my video Making your R Markdown Pretty has 16 thousand views. I never thought it would reach that many people! So, if I’ve helped even 0.1% of those people it will have been worth it.\nI’ve started to record some more videos on spatial analaysis in R. I think the spatial anlaysis / statistics videos on youtube are lacking in diversity—with the noteable exception of @GeostatsGuyLectures but he’s more of a environmental scientist :) I will note, though, that the lectures of Luc Anselin are one of the only reason why I am where I am in my knowledge and abilities.\n\n\nThe thing about Anselin’s videos, though, is that they are not about the how of doing it. But they focus more on the theory and the math that sits behind the statistics themselves."
  },
  {
    "objectID": "posts/2022-11-23-youtube-videos.html#i-ask-you",
    "href": "posts/2022-11-23-youtube-videos.html#i-ask-you",
    "title": "YouTube Videos & what not",
    "section": "I ask you!",
    "text": "I ask you!\nWhat do you want to see? What is actually helpful? I’m sure me stumbling and mumbling for 18 minutes on spatial lags can’t be too helpful.\nPlease vote or leave a comment in this discussion."
  },
  {
    "objectID": "posts/2023-01-19-raw-strings-in-r.html",
    "href": "posts/2023-01-19-raw-strings-in-r.html",
    "title": "Raw strings in R",
    "section": "",
    "text": "The one thing about Python I actually really like is the ability to use raw strings. Raw strings are super helpful for me because at work I use a windows machine. And windows machines use a silly file path convention. The \\ back slack character is used as the file separator as opposed to the linux / unix / forward slash.\nUsing the backslash is so annoying because it’s also an escape character. In python I can write the following to hard code a file path.\nWhereas in R typically you would have to write:\nSince \\ is an escape character you have to escape it first using itself. So, its annoying. And file.path(\"nav\", \"to\", \"file\", \"path.ext\", fsep = \"\\\\\") is a wee bit cumbersome sometimes."
  },
  {
    "objectID": "posts/2023-01-19-raw-strings-in-r.html#aight.",
    "href": "posts/2023-01-19-raw-strings-in-r.html#aight.",
    "title": "Raw strings in R",
    "section": "Aight.",
    "text": "Aight.\nSo like, you can use raw strings today.\nHow can I get the R-devel news? I’m on the mailing list and get it once a week and it’s like “Re: memory leak in png() ` not this stuff. Tips?\nIt was announced in the news for version 4.0.0.\nThey write:\n\nThere is a new syntax for specifying raw character constants similar to the one used in C++: r”(…)” with … any character sequence not containing the sequence ‘⁠)“⁠’. This makes it easier to write strings that contain backslashes or both single and double quotes. For more details see ?Quotes.\n\nYou can write raw strings using the following formats:\n\nr\"( ... )\"\nr\"{ ... }\"\nr\"[ ... ]\"\nR\"( ... )\"\nR\"{ ... }\"\nR\"[ ... ]\"\n\nYou can even trickier by adding dashes between the quote and the delimter. The dashes need to be symmetrical though. So the following is also valid.\n\nr\"-{ ... }\"-\nr\"--{ ... }--\"\nr\"--{ * _ * }--\"\n\nIt kinda looks like a crab\nAlright so back to the example\n\n r\"{nav\\to\\file\\path.ext}\"\n\n[1] \"nav\\\\to\\\\file\\\\path.ext\"\n\n\nHot damn. Thats nice.\nI freaked out at first though because R prints two backslashes. But if you cat the result they go away. So do not worry.\n\n r\"{nav\\to\\file\\path.ext}\" |&gt; \n  cat()\n\nnav\\to\\file\\path.ext"
  },
  {
    "objectID": "posts/2022-04-05-an-open-system-requirements-database.html",
    "href": "posts/2022-04-05-an-open-system-requirements-database.html",
    "title": "Actually identifying R package System Requirements",
    "section": "",
    "text": "During my approximately three years at RStudio there were two things that stumped system admins more than anything: proxied authentication and system dependencies for R package (god help anyone trying to install rgdal on RHEL 7). When RStudio Package Manager (RSPM) v1.0.8 was released there was finally an answer. RSPM can help you identify system requirements via the GUI. Also, there’s a restful API that isn’t fully supported but can provide these system requirements programatically if needed. As such, I think it is still a little used feature for most RSPM users and admins.\n{pak} did a great job of providing an interface to the public installation of RSPM. Back in May 2021 I suggested that the API calls to RSPM be made publicly available. Since then pak::pkg_system_requirements() has become an exported function. It is exceptionally useful. I use it in my current work to create a bash script which installs system requirements into a fresh Databricks compute cluster and then install R packages from RSPM.\nOne of my qualms about the RSPM API and thus the output of pak is that it always includes the installation commands for the provided OS–e.g. apt-get install -y which I suppose could be easily stringr::str_remove()d.\nThe second qualm has been that this relies on RSPM. The logic has been semi-hidden behind a closed-source tool. However, RStudio maintains a repository r-system-requirements which is used by RSPM to identify system requirements from a packages DESCRIPTION file.\nAll of the logic for RSPM is in that repository. And that’s why I made https://r-sysreqs.josiahparry.com. It’s a way to provide the REST API functionality from RSPM without having to rely strictly on the public installation.\nUsers can use the functionality from {sysreqs} to make this api available on their own machines.\n\nsysreqs::get_pkgs_sysreqs(c(\"rgdal\", \"igraph\"),\n                          \"ubuntu\", \"18.04\") |&gt; \n  tidyr::unnest(dependencies)\n#&gt; # A tibble: 6 × 5\n#&gt;   pkg    name    dependencies pre_installs post_installs\n#&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;        \n#&gt; 1 rgdal  gdal    libgdal-dev  NA           NA           \n#&gt; 2 rgdal  gdal    gdal-bin     NA           NA           \n#&gt; 3 rgdal  proj    libproj-dev  NA           NA           \n#&gt; 4 igraph glpk    libglpk-dev  NA           NA           \n#&gt; 5 igraph gmp     libgmp3-dev  NA           NA           \n#&gt; 6 igraph libxml2 libxml2-dev  NA           NA"
  },
  {
    "objectID": "posts/2019-06-11-scraping-4-campaigns.html",
    "href": "posts/2019-06-11-scraping-4-campaigns.html",
    "title": "Web-scraping for Campaigns",
    "section": "",
    "text": "Note: 2022-11-14 I’m migrating my website and this can no longer be reproduced. This blog post is based on the short guide I wrote back in 2019. Please see the old bookdown here.\nAs the primaries approach, I am experiencing a mix of angst, FOMO, and excitement. One of my largest concerns is that progressive campaigns are stuck in a sort of antiquated but nonetheless entrenched workflow. Google Sheets reign in metric reporting. Here I want to present one use case (of a few more to come) where R can be leveraged by your data team.\nIn this post I show you how to scrape the most recent polling data from FiveThirtyEight. FiveThirtyEight aggregates this data in an available way. This can allow you as a Data Manager to provide a useful report to your Media Manager.\nAs always, please feel free to contact me on Twitter @josiahparry if you have any questions or want to discuss this further."
  },
  {
    "objectID": "posts/2019-06-11-scraping-4-campaigns.html#understanding-rvest",
    "href": "posts/2019-06-11-scraping-4-campaigns.html#understanding-rvest",
    "title": "Web-scraping for Campaigns",
    "section": "Understanding rvest",
    "text": "Understanding rvest\nThis use case will provide a cursory overview of the package rvest. To learn more go here.\nWeb scraping is the process of extracting data from a website. Websites are written in HTML and CSS. There are a few aspects of these languages that are used in web scraping that is important to know. HTML is written in a series of what are call tags. A tag is a set of characters wrapped in angle brackets—i.e. &lt;img&gt;.\nWith CSS (cascading style sheets), web developers can give unique identifiers to a tag. Classes can also be assigned to a tag. Think of these as group. With web scraping we can specify a particular part of a website by it’s HTML tag and perhaps it’s class or ID. rvest provides a large set of functions to make this simpler."
  },
  {
    "objectID": "posts/2019-06-11-scraping-4-campaigns.html#example",
    "href": "posts/2019-06-11-scraping-4-campaigns.html#example",
    "title": "Web-scraping for Campaigns",
    "section": "Example",
    "text": "Example\nFor this example we will be scraping FiveThirtyEight’s aggregated poll table. The table can be found at https://projects.fivethirtyeight.com/2020-primaries/democratic/national/.\nBefore we begin, we must always prepare our workspace. Mise en place.\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nThe first thing we will have to do is specify what page we will be scraping from. html_session() will simulate a session in an html browser. By providing a URL to html_session() we will then be able to access the underlying code of that page. Create an object called session by providing the FiveThirtyEight URL to html_session().\n\nsession &lt;- html_session(\"https://projects.fivethirtyeight.com/2020-primaries/democratic/national/\")\n\nThe next and most important step is to identify which piece of HTML code contains the table. The easiest way to do this is to open up the webpage in Chrome and open up the Inspect Elements view (on Mac - ⌘ + Shift + C). Now that this is open, click the select element button at the top left corner of the inspection pane. Now hover over the table.\nYou will see that the HTML element is highlighted. We can see that it is a table tag. Additionally we see that there are two different classes polls-table and tracker. To specify a class we put a preceding . to the class name—i.e. .class-name. If there are multiple classes we just append the second class name to it—i.e. .first-class.second-class. Be aware that these selectors can be quite finicky and be a bit difficult to figure out. You might need to do some googling or playing around with the selector.\nTo actually access the content of this HTML element, we must specify the element using the proper selector. html_node() will be used to do this. Provide the html session and the CSS selector to html_node() to extract the HTML element.\n\nsession %&gt;% \n  html_node(\".polls-table.tracker\")\n\nHere we see that this returns on object of class xml_node. This object returns some HTML code but it is still not entirely workable. Since this is an HTML table we want to extract we can use the handy html_table(). Note that if this wasn’t a table but rather text, you can use html_text().\n\nsession %&gt;% \n  html_node(\".polls-table.tracker\") %&gt;% \n  html_table()\n\nTake note of the extremely informative error. It appears we might have to deal with mismatching columns.\n\nsession %&gt;% \n  html_node(\".polls-table.tracker\") %&gt;% \n  html_table(fill = TRUE) %&gt;% \n  head()\n\nThis is much better! But based on visual inspection the column headers are not properly matched. There are a few things that need to be sorted out: there are two date columns, there are commas and percents where numeric columns should be, the column headers are a little messy, and the table isn’t a tibble (this is just personal preference).\nWe will handle the final two issues first as they are easiest to deal with. The function clean_names() from janitor will handle the column headers, and as_tibble() will coerce the data.frame into a proper tibble. Save this semi-clean tibble into an object called polls.\n\npolls &lt;- session %&gt;% \n  html_node(\".polls-table.tracker\") %&gt;% \n  html_table(fill = TRUE) %&gt;% \n  janitor::clean_names() %&gt;% \n  as_tibble()\n\npolls\n\nWe want to shift over the column names to the right just once. Unfortunately there is no elegant way to do this (that I am aware of). We can see that the first column is completely useless so that can be removed. Once that column is removed we can reset the names this way they will be well aligned.\nWe will start by creating a vector of the original column names.\n\ncol_names &lt;- names(polls)\ncol_names\n\nUnfortunately this also presents another issue. Once a column is deselected, there will be one more column name than column. So we will need to select all but the last element of the original names. We will create a vector called new_names.\n\n# identify the integer number of the last column\nlast_col &lt;- length(col_names) - 1\n\n# create a vector which will be used for the new names\nnew_names &lt;- col_names[1:last_col]\n\nNow we can try implementing the hacky solution. Here we will deselect the first column and reset the names using setNames(). Following, we will use the mutate_at() variant to remove the percent sign from every candidate column and coerce them into integer columns. Here we will specify which variables to not mutate at within vars().\n\npolls %&gt;% \n  select(-1) %&gt;%  \n  setNames(new_names)%&gt;%\n  select(-1) %&gt;%\n  mutate_at(vars(-c(\"dates\", \"pollster\", \"sample\", \"sample_2\")), \n            ~as.integer(str_remove(., \"%\")))\n\n\n\nNow we must tidy the data. We will use tidyr::gather() to transform the data from wide to long. In short, gather takes the column headers (the key argument) and creates a new variable from the values of the columns (the value argument). In this case, we will create a new column called candidate from the column headers and a second column called points which are a candidates polling percentage. Next we deselect any columns that we do not want to be gathered.\n\npolls %&gt;% \n  select(-1) %&gt;% \n  setNames(new_names)%&gt;%\n  select(-1) %&gt;%\n  mutate_at(vars(-c(\"dates\", \"pollster\", \"sample\", \"sample_2\")),\n            ~as.integer(str_remove(., \"%\"))) %&gt;% \n  gather(candidate, points, -dates, -pollster, -sample, -sample_2)\n\nThere are a few more house-keeping things that need to be done to improve this data set. sample_2 is rather uninformative. On the FiveThirtyEight website there is a key which describes what these values represent (A = ADULTS, RV = REGISTERED VOTERS, V = VOTERS, LV = LIKELY VOTERS). This should be specified in our data set. In addition the sample column ought to be cast into an integer column. And finally, those messy dates will need to be cleaned. My approach to this requires creating a function to handle this cleaning. First, the simple stuff.\nTo do the first two above steps, we will continue our function chain and save it to a new variable polls_tidy.\n\npolls_tidy &lt;- polls %&gt;% \n  select(-1) %&gt;% \n  setNames(new_names)%&gt;%\n  select(-1) %&gt;%\n  mutate_at(vars(-c(\"dates\", \"pollster\", \"sample\", \"sample_2\")), \n            ~as.integer(str_remove(., \"%\"))) %&gt;% \n  gather(candidate, points, -dates, -pollster, -sample, -sample_2) %&gt;% \n  mutate(sample_2 = case_when(\n    sample_2 == \"RV\" ~ \"Registered Voters\",\n    sample_2 == \"LV\" ~ \"Likely Voters\",\n    sample_2 == \"A\" ~ \"Adults\",\n    sample_2 == \"V\" ~ \"Voters\"\n  ),\n  sample = as.integer(str_remove(sample, \",\")))\n\npolls_tidy\n\n\nDate cleaning\nNext we must work to clean the date field. I find that when working with a messy column, creating a single function which handles the cleaning is one of the most effective approaches. Here we will create a function which takes a value provided from the dates field and return a cleaned date. There are two unique cases I identified. There are poll dates which occurred during a single month, or a poll that spanned two months. The dates are separated by a single hyphen -. If we split the date at - we will either receive two elements with a month indicated or one month with a day and a day number. In the latter case we will have to carry over the month. Then the year can be appended to it and parsed as a date using the lubridate package. For more on lubridate visit here.\nThe function will only return one date at a time. The two arguments will be date and .return to indicate whether the first or second date should be provided. The internals of this function rely heavily on the stringr package (see R for Data Science Chapter 14). switch() at the end of the function determines which date should be returned (see Advanced R Chapter 5).\n\nclean_date &lt;- function(date, .return = \"first\") {\n  # take date and split at the comma to get the year and the month-day combo\n  date_split &lt;- str_split(date, \",\") %&gt;% \n    # remove from list / coerce to vector\n    unlist() %&gt;% \n    # remove extra white space\n    str_trim()\n  \n  # extract the year\n  date_year &lt;- date_split[2]\n  \n  # split the month day portion and coerce to vector\n  dates &lt;- unlist(str_split(date_split[1],  \"-\"))\n  \n  # paste the month day and year together then parse as date using `mdy()`\n  first_date &lt;- paste(dates[1], date_year) %&gt;% \n    lubridate::mdy()\n  \n  second_date &lt;- ifelse(!str_detect(dates[2], \"[A-z]+\"),\n                        yes = paste(str_extract(dates[1], \"[A-z]+\"), \n                              dates[2], \n                              date_year), \n                        no = paste(dates[2], date_year)) %&gt;% \n    lubridate::mdy()\n  \n  switch(.return, \n         first = return(first_date),\n         second = return(second_date)\n         )\n  \n}\n\n# test on a date\nclean_date(polls_tidy$dates[10], .return = \"first\")\nclean_date(polls_tidy$dates[10], .return = \"second\")\n\nWe can use this new function to create two new columns poll_start and poll_end using mutate(). Following this we can deselect the original dates column, remove any observations missing a points value, remove duplicates using distinct(), and save this to polls_clean.\n\npolls_clean &lt;- polls_tidy %&gt;% \n  mutate(poll_start = clean_date(dates, \"first\"),\n         poll_end = clean_date(dates, \"second\")) %&gt;% \n  select(-dates) %&gt;% \n  filter(!is.na(points)) %&gt;% \n  distinct()\n\npolls_clean\n\n\n\nVisualization\nThe cleaned data can be aggregated and visualized.\n\navg_polls &lt;- polls_clean %&gt;% \n  group_by(candidate) %&gt;% \n  summarise(avg_points = mean(points, na.rm = TRUE),\n            min_points = min(points, na.rm = TRUE),\n            max_points = max(points, na.rm = TRUE),\n            n_polls = n() - sum(is.na(points))) %&gt;% # identify how many polls candidate is in\n  # remove candidates who appear in 50 or fewer polls: i.e. HRC\n  filter(n_polls &gt; 50) %&gt;% \n  arrange(-avg_points)\n\navg_polls\n\n\navg_polls %&gt;% \n  mutate(candidate = fct_reorder(candidate, avg_points)) %&gt;% \n  ggplot(aes(candidate, avg_points)) +\n  geom_col() + \n  theme_minimal() +\n  coord_flip() +\n  labs(title = \"Polls Standings\", x = \"\", y = \"%\")"
  },
  {
    "objectID": "posts/2019-06-11-scraping-4-campaigns.html#creating-historic-polling-data",
    "href": "posts/2019-06-11-scraping-4-campaigns.html#creating-historic-polling-data",
    "title": "Web-scraping for Campaigns",
    "section": "Creating historic polling data",
    "text": "Creating historic polling data\nIt may become useful to have a running history of how candidates have been polling. We can use R to write a csv file of the data from FiveThirtyEight. However, what happens when the polls update? How we can we keep the previous data and the new data? We will work through an example using a combination of bind_rows() and distinct(). I want to emphasize that this is not a good practice if you need to scale to hundreds of thousand of rows. This works in this case as the data are inherently small.\nTo start, I have created a sample dataset which contains 80% of these polls (maybe less by the time you do this!). Note that is probably best to version control this or have multiple copies as a failsafe.\nThe approach we will take is to read in the historic polls data set and bind rows with the polls_clean data we have scraped. Next we remove duplicate rows using distinct().\n\nold_polls &lt;- read_csv(\"https://raw.githubusercontent.com/JosiahParry/r-4-campaigns/master/data/polls.csv\")\n\nold_polls\n\nupdated_polls &lt;- bind_rows(old_polls, polls_clean) %&gt;% \n  distinct()\n\nupdated_polls\n\nNow you have a cleaned data set which has been integrated with the recently scraped data. Write this to a csv using write_csv() for later use."
  },
  {
    "objectID": "posts/2021-06-18-sliced-a-brief-reflection.html",
    "href": "posts/2021-06-18-sliced-a-brief-reflection.html",
    "title": "SLICED! a brief reflection",
    "section": "",
    "text": "A few weeks ago I was a contestant on the machine learning game show #SLICED! The format of the challenge is as follows:\nMy stream is uploaded to youtube so you can catch it in all of its glory."
  },
  {
    "objectID": "posts/2021-06-18-sliced-a-brief-reflection.html#how-i-got-roped-in",
    "href": "posts/2021-06-18-sliced-a-brief-reflection.html#how-i-got-roped-in",
    "title": "SLICED! a brief reflection",
    "section": "How I got roped in",
    "text": "How I got roped in\nAbout a month ago I saw the below tweet from Jesse Mostipak. Naturally, it piqued my interest.\n\n\nCome play with meeeeee! I promise to make you look good. https://t.co/U7DRXM8deP\n\n— Jesse Mostipak is making mirepoix for #SLICED (@kierisi) May 8, 2021\n\n\nEveryone’s favorite Tidy-Tuesday-Tom essentially voluntold me. I decided to put my name in the hat and see if I can compete. The challenge, though, is that I have only ever dabbled in machine learning. In May it was something that I had only done a handful of times and with a much older toolset-e.g. caret. If there is one thing I know about myself, it’s that there is nothing like a deadline and a concrete objective to get me to learn something.\nI am a strong believer in Parkinson’s Law—you can thank my father for that—which is characterized by the saying “If you wait until the last minute, it only takes a minute to do.” Or, more formally, “work expands so as to fill the time available for its completion.”\nIn essence, the best way for me to get better at machine learning would be to put myself in a situation—as uncomfortable it may be—where I would have to do machine learning. Alternatively, I could just faily miserably but I don’t like that."
  },
  {
    "objectID": "posts/2021-06-18-sliced-a-brief-reflection.html#getting-a-grip-on-tidymodels",
    "href": "posts/2021-06-18-sliced-a-brief-reflection.html#getting-a-grip-on-tidymodels",
    "title": "SLICED! a brief reflection",
    "section": "Getting a grip on {tidymodels}",
    "text": "Getting a grip on {tidymodels}\nI have been loosely following the tidymodels ecosystem since the beginning. Previously my understanding of tidymodels only included, recipes, rsample, and parsnip. These three packages can get you exceptionally far, but there are so many additional packages that are instrumental to improving the ML workflow for useRs. These are tune, workflows, and workflowsets.\nThe most challenging part of getting started with tidymodels was understanding where each package fits in during the process. The challenging task was to figure out which packages were low level APIs and which were abstractions."
  },
  {
    "objectID": "posts/2021-06-18-sliced-a-brief-reflection.html#understanding-tidymodels-libraries",
    "href": "posts/2021-06-18-sliced-a-brief-reflection.html#understanding-tidymodels-libraries",
    "title": "SLICED! a brief reflection",
    "section": "Understanding tidymodels libraries",
    "text": "Understanding tidymodels libraries\nThe most basic component of a tidymodel ML process is a recipe ({recipes}) and a model specification ({parsnip}). The recipe determines the features used and any preprocessing steps. The model specification determines which model will be trained. Additionally, we often want to include resampling—e.g. bootstrap or cross validation (called a resamples ({rsample}) object in tidymodels). With these three components we can then utilize the {tune} package to train our model on our resamples. We can build a layer of abstraction from these four components which is called a workflow.\nIn the ML process we want to train many models. And rather than just repeating the steps manually for each model, the package workflowsets will create many workflows for you and help you train all of those models quickly. Workflowsets were essential in my approach to sliced."
  },
  {
    "objectID": "posts/2021-06-18-sliced-a-brief-reflection.html#tidymodels-resources",
    "href": "posts/2021-06-18-sliced-a-brief-reflection.html#tidymodels-resources",
    "title": "SLICED! a brief reflection",
    "section": "Tidymodels resources",
    "text": "Tidymodels resources\nGetting up to speed with tidymodels was a bit of a challenge. The packages are still actively under development so building corresponding documentation must be a bit of a challenge for the team! With that said, there are so many resources that you can use to get started. Below are some of the ones that I utilized and found helpful.\n\nTidymodeling with R\nWorkflowsets vignette\nJulia Silge’s blog\nAnd a bunch of the help docs"
  },
  {
    "objectID": "posts/2020-02-19-r-history.html",
    "href": "posts/2020-02-19-r-history.html",
    "title": "Design Paradigms in R",
    "section": "",
    "text": "Lately I have been developing a deep curiosity of the origins of the R language. I have since read a more from the WayBack Machine than a Master’s student probably should. There are four documents that I believe to be extremely foundational and most clearly outline the original philosophies underpinning both R and its predecessor S. These are Evolution of the S Language (Chambers, 1996), A Brief History of S (Becker), Stages in the Evolution of S (Chambers, 200), and R: Past and Future History by Ross Ihaka (1998). The readings have elicited many lines of thought and potential inquiry. What interests me the most at present is the question “how have the design principles of S and R manifested themselves today?”\nThere are a number of design principles that I believe still exist in the R language today. Two of these find their origin in the development of S and the third was most clearly emphasized in the development of R. These are, in no particular order, what I believe to be the design principles that are most prominent in the modern iteration of R. The software should:\n\nbe an interface language;\nbe performant;\nand make users feel like they are performing analyses.\n\nThe first design philosophy I think is quite apparent in R’s continual development as an interface language. I believe part of R’s success is because of the immense community development focusing on interfaces to other tools and languages. Most prominently I would argue is the development of interfaces to SQL, Stan, JavaScript, and Python. Each enables R users to interact with existing infrastructure which vastly increases the breadth of technologies and tools available to useRs.\nIdentifying R’s evolution as an interface language is a rather objective task. Assessing these latter two principles is a subjective task, but one I will endeavor nonetheless.\nThe second principle is stated clearly by Ihaka (1998).\n\n“My own conclusion has been that it is important to pursue efficiency issues, and in particular, speed.”\n\nIn my view of what is the prominent landscape of R today, few packages have paid as much attention to this principle as data.table. data.table’s performance is beyond reproach. The speed at which one can subset, manipulate, order, etc. using data.table is remarkable. Recent speed tests by package author Matt Dowle illustrate this. In performance testing, data.table regularly outperforms other existing tools such as Spark, dplyr, and pandas—spark being an exceptionally notable one.\nThis brings us to the third point which actually finds its origination in the development of S. John Chambers in Stages in the Evolution of S wrote that\n\n“The ambiguity is real and goes to a key objective: we wanted users to be able to begin in an interactive environment, where they did not consciously think of themselves as programming.”\n\nThis is a helpful reminder that S was developed with the intentions of creating a tool for statistical analysis. In the design and implementation of S, the emphasis was doing analysis not programming. From this paradigm is where I think the tidyverse came—perhaps unintentionally, but consequently nonetheless.\nThe development of the tidyverse has, I believe, adhered to this principle. There is what seems to be a constant and conscious consideration to the useR experience. dplyr, for example, has developed a way to perform an analysis that is clear in intent and, to an extent, that can be read in a linguistically cogent manner.\nFrom this view, it clear that the R community has done a great job adhering to these principles. The various odbc packages, numerous javascript packages (particularly in the Shiny space), reticulate, rstan, JuliaCall, and rcpp, among many others are an immense testament to this. Moreover, data.table’s “relentless focus on performance across the entire package” is reason for its success (Wickham, 2019). Similarly, I believe the relentless focus on user experience in the tidyverse is reason for its success. When viewing these two latter toolkits, they should be viewed at two sides to the same coin with each approaching the same end goal from a different perspective."
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html",
    "title": "genius tutorial",
    "section": "",
    "text": "knitr::opts_chunk$set(eval = FALSE)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#introducing-genius",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#introducing-genius",
    "title": "genius tutorial",
    "section": "Introducing genius",
    "text": "Introducing genius\nYou want to start analysing song lyrics, where do you go? There have been music information retrieval papers written on the topic of programmatically extracting lyrics from the web. Dozens of people have gone through the laborious task of scraping song lyrics from websites. Even a recent winner of the Shiny competition scraped lyrics from Genius.com.\nI too have been there. Scraping websites is not always the best use of your time. genius is an R package that will enable you to programatically download song lyrics in a tidy format ready for analysis. To begin using the package, it first must be installed, and loaded. In addition to genius, we will need our standard data manipulation tools from the tidyverse.\n\ninstall.packages(\"genius\")\n\n\nlibrary(genius)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#single-song-lyrics",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#single-song-lyrics",
    "title": "genius tutorial",
    "section": "Single song lyrics",
    "text": "Single song lyrics\nThe simplest method of extracting song lyrics is to get just a single song at a time. This is done with the genius_lyrics() function. It takes two main arguments: artist and song. These are the quoted name of the artist and song. Additionally there is a third argument info which determines what extra metadata you can get. The possible values are title, simple, artist, features, and all. I recommend trying them all to see how they work.\nIn this example we will work to retrieve the song lyrics for the upcoming musician Renny Conti.\n\nfloating &lt;- genius_lyrics(\"renny conti\", \"people floating\")\nfloating"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#album-lyrics",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#album-lyrics",
    "title": "genius tutorial",
    "section": "Album Lyrics",
    "text": "Album Lyrics\nNow that you have the intuition for obtaining lyrics for a single song, we can now create a larger dataset for the lyrics of an entire album using genius_album(). Similar to genius_lyrics(), the arguments are artist, album, and info.\nIn the exercise below the lyrics for Snail Mail’s album Lush. Try retrieving the lyrics for an album of your own choosing.\n\nlush &lt;- genius_album(\"Snail Mail\", \"Lush\")\nlush"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#adding-lyrics-to-a-data-frame",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#adding-lyrics-to-a-data-frame",
    "title": "genius tutorial",
    "section": "Adding Lyrics to a data frame",
    "text": "Adding Lyrics to a data frame\n\nMultiple songs\nA common use for lyric analysis is to compare the lyrics of one artist to another. In order to do that, you could potentially retrieve the lyrics for multiple songs and albums and then join them together. This has one major issue in my mind, it makes you create multiple object taking up precious memory. For this reason, the function add_genius() was developed. This enables you to create a tibble with a column for an artists name and their album or song title. add_genius() will then go through the entire tibble and add song lyrics for the tracks and albums that are available.\nLet’s try this with a tibble of three songs.\n\nthree_songs &lt;- tribble(\n  ~ artist, ~ title,\n  \"Big Thief\", \"UFOF\",\n  \"Andrew Bird\", \"Imitosis\",\n  \"Sylvan Esso\", \"Slack Jaw\"\n)\n\nsong_lyrics &lt;- three_songs %&gt;% \n  add_genius(artist, title, type = \"lyrics\")\n\nsong_lyrics %&gt;% \n  count(artist)\n\n\n\n\nMultiple albums\nadd_genius() also extends this functionality to albums.\n\nalbums &lt;- tribble(\n  ~ artist, ~ title,\n  \"Andrew Bird\", \"Armchair Apocrypha\",\n  \"Andrew Bird\", \"Things are really great here sort of\"\n)\n\nalbum_lyrics &lt;- albums %&gt;% \n  add_genius(artist, title, type = \"album\")\n\nalbum_lyrics\n\nWhat is important to note here is that the warnings for this function are somewhat informative. When a 404 error occurs, this may be because that the song does not exist in Genius. Or, that the song is actually an instrumental which is the case here with Andrew Bird.\n\n\nAlbums and Songs\nIn the scenario that you want to mix single songs and lyrics, you can supply a column with the type value of each row. The example below illustrates this. First a tibble with artist, track or album title, and type columns are created. Next, the tibble is piped to add_genius() with the unquote column names for the artist, title, and type columns. This will then iterate over each row and fetch the appropriate song lyrics.\n\nsong_album &lt;- tribble(\n  ~ artist, ~ title, ~ type,\n  \"Big Thief\", \"UFOF\", \"lyrics\",\n  \"Andrew Bird\", \"Imitosis\", \"lyrics\",\n  \"Sylvan Esso\", \"Slack Jaw\", \"lyrics\",\n  \"Movements\", \"Feel Something\", \"album\"\n)\n\nmixed_lyrics &lt;- song_album %&gt;% \n  add_genius(artist, title, type)"
  },
  {
    "objectID": "posts/2019-05-08-genius-learnr-tutorial.html#self-similarity",
    "href": "posts/2019-05-08-genius-learnr-tutorial.html#self-similarity",
    "title": "genius tutorial",
    "section": "Self-similarity",
    "text": "Self-similarity\nAnother feature of genius is the ability to create self-similarity matrices to visualize lyrical patterns within a song. This idea was taken from Colin Morris’ wonderful javascript based Song Sim project. Colin explains the interpretation of a self-similarity matrix in their TEDx talk. An even better description of the interpretation is available in this post.\nTo use Colin’s example we will look at the structure of Ke$ha’s Tik Tok.\nThe function calc_self_sim() will create a self-similarity matrix of a given song. The main arguments for this function are the tibble (df), and the column containing the lyrics (lyric_col). Ideally this is one line per observation as is default from the output of genius_*(). The tidy output compares every ith word with every word in the song. This measures repetition of words and will show us the structure of the lyrics.\n\ntik_tok &lt;- genius_lyrics(\"Ke$ha\", \"Tik Tok\")\n\ntt_self_sim &lt;- calc_self_sim(tik_tok, lyric, output = \"tidy\")\n\ntt_self_sim\n\ntt_self_sim %&gt;% \n  ggplot(aes(x = x_id, y = y_id, fill = identical)) +\n  geom_tile() +\n  scale_fill_manual(values = c(\"white\", \"black\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        axis.text = element_blank()) +\n  scale_y_continuous(trans = \"reverse\") +\n  labs(title = \"Tik Tok\", subtitle = \"Self-similarity matrix\", x = \"\", y = \"\", \n       caption = \"The matrix displays that there are three choruses with a bridge between the last two. The bridge displays internal repetition.\")"
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "",
    "text": "Installing python has never been an easy task for me. I remember back in 2016 I wanted to learn how to use pyspark and thus python, I couldn’t figure out how to install python so I gave up. In graduate school I couldn’t install python so I used a docker container my professor created and never changed a thing. When working at RStudio I used the Jupyter Lab instance in RStudio Workbench when I couldn’t install it locally.\nNow, I want to compare pysal results to some functionality I’ve written in R. To do that, I need a python installation. I’ve heard extra horror stories about installing Python on the new Mac M1 chip—which I have.\nPrior to installing, I took to twitter for suggestions. I received the phenomenal tweet below encouraging me to install with {reticulate}1 which was absolutely phenomenal advice."
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#installing-python",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#installing-python",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Installing Python",
    "text": "Installing Python\nThe steps to install python, at least for me, was very simple.\n\nInstall reticulate\nInstall miniconda\n\ninstall.packages(\"reticulate\")\nreticulate::install_miniconda()\nThat’s it. That’s all it took."
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#creating-my-first-conda-environment",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#creating-my-first-conda-environment",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Creating my first conda environment",
    "text": "Creating my first conda environment\nAfter installing python, I restarted R, and began building my first conda environment. I created a conda environment called geo for my geospatial work. I installed libpysal, geopandas, and esda. These installed every other dependency I needed–e.g. pandas, and numpy.\nreticulate::conda_create(\"geo\")\nreticulate::use_condaenv(\"geo\")\nreticulate::conda_install(\"geo\", c(\"libpysal\", \"geopandas\", \"esda\"))"
  },
  {
    "objectID": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#using-my-conda-environment",
    "href": "posts/2022-02-12-installing-python-on-my-m1-in-under-10-minutes.html#using-my-conda-environment",
    "title": "Installing Python on my M1 in under 10 minutes",
    "section": "Using my conda environment",
    "text": "Using my conda environment\nTo begin using my new conda environment, I opened up a fresh R session and a fresh R Markdown document. In my first code chunk I told reticulate which conda environment to use. Then my following code chunks were python which opened up the python repl. Make sure that you start your code chunk with ```{python}\n\nreticulate::use_condaenv(\"geo\")\n\nIn the following example I utilize esda to calculate a local join count.\n\nimport libpysal\nimport geopandas as gpd\nfrom esda.join_counts_local import Join_Counts_Local\n\nfp = libpysal.examples.root + \"/guerry/\" + \"Guerry.shp\" \n\nguerry_ds = gpd.read_file(fp)\nguerry_ds['SELECTED'] = 0\nguerry_ds.loc[(guerry_ds['Donatns'] &gt; 10997), 'SELECTED'] = 1\n\nw = libpysal.weights.Queen.from_dataframe(guerry_ds)\n\nLJC_uni = Join_Counts_Local(connectivity=w).fit(guerry_ds['SELECTED'])\n\nLJC_uni.p_sim\n\n## array([  nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan, 0.435,   nan, 0.025, 0.025,   nan, 0.328,\n##          nan,   nan,   nan,   nan,   nan,   nan, 0.342,   nan, 0.334,\n##          nan,   nan,   nan,   nan,   nan,   nan, 0.329,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan, 0.481,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,   nan,\n##          nan,   nan, 0.02 ,   nan,   nan,   nan,   nan,   nan, 0.125,\n##          nan, 0.043,   nan,   nan])"
  },
  {
    "objectID": "projects/pkgs/sysreqs.html",
    "href": "projects/pkgs/sysreqs.html",
    "title": "R package system requirements",
    "section": "",
    "text": "GitHub repo\nRelated blog post\n\nThe goal of sysreqs is to make it easy to identify R package system dependencies. There are two components to this package: an “API” and a wrapper package.\nThis API and package is based on rstudio/r-system-requirements and the API client for RStudio Package Manager. The functionality is inspired by pak::pkg_system_requirements()."
  },
  {
    "objectID": "projects/writing/uitk.html",
    "href": "projects/writing/uitk.html",
    "title": "Urban Informatics Toolkit",
    "section": "",
    "text": "The Urban Informatics Toolkit (uitk) is an open text book I wrote with the intention of teaching first year graduate students in Urban Informatics the R programming language.\nWithin it are two years of study in the Urban Informatics Program at Northeastern University, five years of self-directed education in R, two years of teaching R, and innumerable hours learning R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Josiah Parry",
    "section": "",
    "text": "Welcome to my personal website. I always aspire to keep pumping out content, but I don’t.\nHere you will find my “recent” blog posts, possibly outdated biographical information, my socials, and links to projects I have made or worked on.\n\n\n\n\n\n\n\n\n\na note to self for later\n\n\n\npkg-dev\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n\n\n\n\n\n\nextending R with Arrow and Rust\n\n\n\nrust\n\n\npkg-dev\n\n\nextendr\n\n\narrow\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\n\n\n\n\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\n\n\n\n\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\n\n\n\n\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\n\n\n\n\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\n\n\n\n\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\n\n\n\n\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\n\n\n\n\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\n\n\n\n\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\n\n\n\n\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "index.html#letters-to-a-layperson-myself",
    "href": "index.html#letters-to-a-layperson-myself",
    "title": "Josiah Parry",
    "section": "",
    "text": "Welcome to my personal website. I always aspire to keep pumping out content, but I don’t.\nHere you will find my “recent” blog posts, possibly outdated biographical information, my socials, and links to projects I have made or worked on.\n\n\n\n\n\n\n\n\n\na note to self for later\n\n\n\npkg-dev\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n\n\n\n\n\n\nextending R with Arrow and Rust\n\n\n\nrust\n\n\npkg-dev\n\n\nextendr\n\n\narrow\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npackage-dev\n\n\nrust\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\npython\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\nsome key takeaways\n\n\n\nr\n\n\nr-spatial\n\n\nspatial\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\nproduction\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\nr\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\nand how they’ll make your package better\n\n\n\nrust\n\n\nr\n\n\npackage-development\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrust\n\n\npersonal\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npackage-dev\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nspatial\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\nrstats\n\n\nsfdep\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nsfdep\n\n\nrstats\n\n\nspacetime\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ndatabricks\n\n\nr\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nesda\n\n\nsfdep\n\n\nr-spatial\n\n\ntalk\n\n\n\n\n\n\n\n\n\nApr 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nr\n\n\nreticulate\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nspatial\n\n\n\n\n\n\n\n\n\nFeb 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\n\n\n\n\n\n\nMoving Beyond the R & Python love story\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nr-admin\n\n\nsecurity\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\n\n\n\n\n\n\nCompress your JSON with gzip to increase API speed and reduce memory utilization in R\n\n\n\nproduction\n\n\napi\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\n\n\n\n\n\n\nUnderstanding the Narrow Corridor\n\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 4, 2020\n\n\n\n\n\n\n\n\nDeploying to RStudio Connect\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n\n\nHow to deal with .sav files from SPSS\n\n\n\n\n\n\n\n\nDec 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\n\nResource created for training at EPA\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nAug 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n\n\nA tidy wrapper for gtrendsR\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nDocker\n\n\nPython\n\n\n\n\n\n\n\n\n\nMar 30, 2019\n\n\n\n\n\n\n\n\nQuantifying constituency representation\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\nShutdown Politics and Their Impact on Greater Boston\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\n\n\n\n\n\n\nExtracting and plotting feature importance\n\n\n\nR\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nDec 1, 2018\n\n\n\n\n\n\n\n\nA simple explanation because I struggled to understand any documentation\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 28, 2018\n\n\n\n\n\n\n\n\nThe Connecticut Compromise and it’s lasting effects\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nletters\n\n\n\n\n\n\n\n\n\nNov 15, 2018\n\n\n\n\n\n\n\n\nWriting data subsets\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nReading chunked csv files\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nOct 27, 2018\n\n\n\n\n\n\n\n\nThe Tidy Approach\n\n\n\nR\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nApr 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ngeniusR\n\n\n\n\n\n\n\n\n\nJan 27, 2018\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "employment: Senior Product Engineer @ Esri\neducation:\n\nMS Urban Informatic, Northeastern University (2020)\nBA Sociology, Plymouth State University\n\nMinor, General Mathematics\nProfessional Certificate GIS\nI am a Senior Product Engineer on the Spatial Analysis team at Esri. Previously, I was at The NPD Group as a Research Analyst where I worked to modernize our data science infrastructure to use Databricks, Docker, and Spark. Before that, I was at RStudio, PBC on the customer success team enabling public sector adoption of data science tools. In 2020 I received my master’s degree in Urban Informatics from Northeastern University following my bachelor’s degree in sociology with focuses in geographic information systems and general mathematics from Plymouth State University in 2018.",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "about",
    "section": "Contact me",
    "text": "Contact me\nIf you want to get in contact with me please send me an email at josiah.parry at gmail dot com.\n\n\ntalks i’ve given\n\n\nExploratory Spatal Data Analysis in the tidyverse\n\nJuly 28th, 2022 rstudio::conf(2022L)\n\nExploratory Spatial Data Analysis in R\n\nRecording\nApril 28th, 2022\n\nAPIs: you’re probably not using them and why you probably should\n\nGovernment Advances in Statistical Programming\nNovember 6th, 2020\n\n“Old Town Road” Rap or Country?: Putting R in Production with Tidymodels, Plumber, and Shiny\n\nBoston useR group\nDecember 10th, 2019\n\nTidy Lyrical Analysis\n\nBoston useR group\nJuly 17th, 2018\n\nNewfound Lake Landscape Value Analysis: Exploring the efficacy of PPGIS, NESTVAL 2016\n\nNew England St. Lawrence River Valley regional American Associations of Geographers Conference\n2016",
    "crumbs": [
      "about"
    ]
  },
  {
    "objectID": "posts/2023-11-27-custom-head-method/index.html",
    "href": "posts/2023-11-27-custom-head-method/index.html",
    "title": "Writing S3 head() methods",
    "section": "",
    "text": "I’ve been struggling for the past 15-20 minutes trying to fix the following R CMD check greivances.\nchecking whether the namespace can be loaded with stated dependencies ... WARNING\n  Error: object 'head' not found whilst loading namespace 'arcgislayers'\n  Execution halted\n\nchecking dependencies in R code ... NOTE\n  Error: object 'head' not found whilst loading namespace 'arcgislayers'\nIt feels like something that shouldn’t be difficult? You write the method and you export it right? Well, that’s true if the function is exported in base. But there are a lot of handy functions that are in base R that are not in the package {base}.\nhead(), the function I’m fighting with, is actually an export of the base R package {utils}.\nHere’s some code I have that I couldn’t get to export head() properly.\n#' @export\nhead.FeatureLayer &lt;- function(x, n = 6, token = Sys.getenv(\"ARCGIS_TOKEN\"), ...) {\n  collect_layer(x, n_max, token)\n}\nTo fix this we need to do the following:\n\nAdd utils as an imported package with usethis::use_package(\"utils\")\nThen we need to specifically import head by adding #' @importFrom utils head\nRedocument with devtools::document() (or cmd + shift + d)\n\nThe whole shebang:\n#' @importFrom utils head\n#' @export\nhead.FeatureLayer &lt;- function(x, n = 6, token = Sys.getenv(\"ARCGIS_TOKEN\"), ...) {\n  collect_layer(x, n, token)\n}\nNow R CMD check won’t complain about it."
  },
  {
    "objectID": "posts/csr.html",
    "href": "posts/csr.html",
    "title": "Complete spatial randomness",
    "section": "",
    "text": "&lt; this is a cliche about Tobler’s fist law and things being related in space&gt;. Because of Tobler’s first law, spatial data tend to not follow any specific distribution. So, p-values are sort of…not all that accurate most of the time. P-values in spatial statistics often take a “non-parametric” approach instead of an “analytical” one.\nConsider the t-test. T-tests make the assumption that data are coming from a normal distribution. Then p-values are derived from the cumulative distribution function. The alternative hypothesis, then, is that the true difference in means is not 0.\nIn the spatial case, our alternative hypothesis is generally “the observed statistic different than what we would expect under complete spatial randomness?” But what really does that mean? To know, we have to simulate spatial randomness.\nThere are two approaches to simulating spatial randomness that I’ll go over. One is better than the other. First, I’m going to describe the less good one: bootstrap sampling.\nLoad the super duper cool packages. We create queen contiguity neighbors and row-standardized weights.\nlibrary(sf)\nlibrary(sfdep)\nlibrary(tidyverse)\n\ngrid &lt;- st_make_grid(cellsize = c(1, 1), n = 12, offset = c(0, 0)) |&gt; \n  as_tibble() |&gt; \n  st_as_sf() |&gt; \n  mutate(\n    id = row_number(),\n    nb = st_contiguity(geometry),\n    wt = st_weights(nb)\n    )\nLet’s generate some spatially autocorrelated data. This function is a little slow, but it works.\nnb &lt;- grid[[\"nb\"]]\nwt &lt;- grid[[\"wt\"]]\n\nx &lt;-  geostan::sim_sar(w = wt_as_matrix(nb, wt), rho = 0.78)"
  },
  {
    "objectID": "posts/csr.html#bootstrap-sampling",
    "href": "posts/csr.html#bootstrap-sampling",
    "title": "Complete spatial randomness",
    "section": "Bootstrap sampling",
    "text": "Bootstrap sampling\nUnder the bootstrap approach we are sampling from existing spatial configurations. In our case there are 144 existing neighborhoods. For our simulations, we will randomly sample from existing neighborhoods and then recalculate our statistic. It helps us by imposing randomness into our statistic. We can then repeat the process nsim times. There is a limitation, however. It is that there are only n - 1 possible neighborhood configurations per location.\nHere we visualize a random point and it’s neighbors.\n\n# for a given location create vector indicating position of\n# neighbors and self\ncolor_block &lt;- function(i, nb) {\n  res &lt;- ifelse(1:length(nb) %in% nb[[i]], \"neighbors\", NA)\n  res[i] &lt;- \"self\"\n  res\n}\n\nPlot a point and its neighbors\n\ngrid |&gt; \n  mutate(block = color_block(sample(1:n(), 1), nb)) |&gt; \n  ggplot(aes(fill = block)) +\n  geom_sf() +\n  labs(title = \"Point and it's neighbors\")\n\n\n\n\nFor bootstrap we grab a point and then the neighbors from another point. This function will randomize a nb list object.\n\ncolor_sample_block &lt;- function(i, nb) {\n  index &lt;- 1:length(nb)\n  not_i &lt;- index[-i]\n  \n  sample_block_focal &lt;- sample(not_i, 1)\n  \n  res &lt;- rep(NA, length(index))\n  \n  res[nb[[sample_block_focal]]] &lt;- \"neighbors\"\n  res[i] &lt;- \"self\"\n  res\n}\n\n# visualize it\ngrid |&gt; \n  mutate(block = color_sample_block(sample(1:n(), 1), nb)) |&gt; \n  ggplot(aes(fill = block)) +\n  geom_sf() +\n  labs(title = \"Point and random point's neighbors\")\n\n\n\n\nOften, we will want to create a reference distribution by creating a large number of simulations—typically 999. As the simulations increase in size, we are limited in the amount of samples we can draw. The number of neighborhoods becomes limiting!\nSay we want to look at income distribution in Boston and the only data we have is at the census tract level. I happen to know that Boston has 207 tracts. If we want to do 999 simulations, after the 206th simulation, we will likely have gone through all over the neighborhood configurations!\nHow can we do this sampling? For each observation, we can sample another location, grab their neighbors, and assign them as the observed location’s neighbors."
  },
  {
    "objectID": "posts/csr.html#bootstrap-simulations",
    "href": "posts/csr.html#bootstrap-simulations",
    "title": "Complete spatial randomness",
    "section": "Bootstrap simulations",
    "text": "Bootstrap simulations\nIn sfdep, we use spdep’s nb object. These are lists that store the row position of the neighbors as integer vectors at each element.\n\nIf you want to learn more about neighbors I gave a talk at NY Hackr MeetUp a few months ago that might help.\n\nHere I define a function that samples from the positions (index), then uses that sample to shuffle up the existing neighborhoods and return a shuffled nb object. Note that I add the nb class back to the list.\n\nbootstrap_nbs &lt;- function(nb) {\n  # create index\n  index &lt;- 1:length(nb)\n  # create a resampled index\n  resampled_index &lt;- sample(index, replace = TRUE)\n  # shuffle the neighbors and reassign class\n  structure(nb[resampled_index], class = c(\"nb\", \"list\"))\n}\n\nLet’s compare some observations\n\nnb[1:3]\n\n[[1]]\n[1]  2 13 14\n\n[[2]]\n[1]  1  3 13 14 15\n\n[[3]]\n[1]  2  4 14 15 16\n\nbootstrap_nbs(nb)[1:3]\n\n[[1]]\n[1]  90  91  92 102 104 114 115 116\n\n[[2]]\n[1] 29 30 31 41 43 53 54 55\n\n[[3]]\n[1] 15 16 17 27 29 39 40 41\n\n\nHere we can see the random pattern. Look’s like there is fair amount of clustering of like values.\n\ngrid |&gt; \n  mutate(x = classInt::classify_intervals(x, 7)) |&gt; \n  ggplot(aes(fill = x)) +\n  geom_sf(color = NA, lwd = 0) +\n  scale_fill_brewer(type = \"div\", palette = 5, direction = -1) +\n  theme_void() \n\n\n\n\nWith the weights and the neighbors we can calculate the global Moran. I’ll refer to this as the “observed.” Store it into an object called obs. We’ll need this to calculate a simulated p-value later.\n\nobs &lt;- global_moran(x, nb, wt)\nobs[[\"I\"]]\n\n[1] 0.465776\n\n\n0.47 is a fair amount of positive spatial autocorrelation indicating that like values tend to cluster. But is this due to random chance, or does it depend on where these locations are? Now that we have the observed value of Moran’s I, we can simulate the value under spatial randomness using the bootstrapped sampling. To do so, we bootstrap sample our neighbors, recalculate the weights and then the global Moran. Now, if you’ve read my vignette on conditional permutation, you know what is coming next. We need to create a reference distribution of the global Moran under spatial randomness. To do that, we apply our boot strap nsim times and recalculate the global Moran with each new neighbor list. I love the function replicate() for these purposes.\n\nnsim = 499 \n\n\nAlso, a thing I’ve started doing is assigning scalars / constants with an equals sign because they typically end up becoming function arguments.\n\n\nreps &lt;- replicate(\n  nsim, {\n    nb_sim &lt;- bootstrap_nbs(nb)\n    wt_sim &lt;- st_weights(nb_sim)\n    global_moran(x, nb_sim, wt_sim)[[\"I\"]]\n  }\n)\n\nhist(reps, xlim = c(min(reps), obs[[\"I\"]]))\nabline(v = obs[[\"I\"]], lty = 2)\n\n\n\n\nBootstrap limitations\nThat’s all well and good, but let’s look at this a bit more. Since we’re using the bootstrap approach, we’re limited in the number of unique combinations that are possible. Let’s try something. Let’s calculate the spatial lag nsim times and find the number of unique values that we get.\n\nlags &lt;- replicate(\n  nsim, {\n    # resample the neighbors list\n    nb_sim &lt;- bootstrap_nbs(nb)\n    # recalculate the weights\n    wt_sim &lt;- st_weights(nb_sim)\n    # calculate the lag\n    st_lag(x, nb_sim, wt_sim)\n  }\n)\n\n# cast from matrix to vector\nlags_vec &lt;- as.numeric(lags)\n\n# how many are there?\nlength(lags_vec)\n\n[1] 71856\n\n# how many unique?\nlength(unique(lags_vec))\n\n[1] 144\n\n\nSee this? There are only 144 unique value! That isn’t much! Don’t believe me? Run table(lags_vec). For each location there are only a limited number of combinations that can occur."
  },
  {
    "objectID": "posts/csr.html#conditional-permutation",
    "href": "posts/csr.html#conditional-permutation",
    "title": "Complete spatial randomness",
    "section": "Conditional Permutation",
    "text": "Conditional Permutation\nNow, here is where I want to introduce what I view to be the superior alternative: conditional permutation. Conditional permutation was described by Luc Anselin in his seminal 1995 paper. The idea is that we hold an observation constant, then we randomly assign neighbors. This is like the bootstrap approach but instead of grabbing a random observation’s neighborhood we create a totally new one. We do this be assigning the neighbors randomly from all possible locations.\nLet’s look at how we can program this. For each location we need to sample from an index that excludes the observation’s position. Further we need to ensure that there are the same number of neighbors in each location (cardinality).\n\npermute_nb &lt;- function(nb) {\n  # first get the cardinality\n  cards &lt;- st_cardinalties(nb)\n  # instantiate empty list to fill\n  nb_perm &lt;- vector(mode = \"list\", length = length(nb))\n  \n  # instantiate an index\n  index &lt;- seq_along(nb)\n  \n  # iterate through and full nb_perm\n  for (i in index) {\n    # remove i from the index, then sample and assign\n    nb_perm[[i]] &lt;- sample(index[-i], cards[i])\n  }\n  \n  structure(nb_perm, class = \"nb\")\n}\n\n\nnb[1:3]\n\n[[1]]\n[1]  2 13 14\n\n[[2]]\n[1]  1  3 13 14 15\n\n[[3]]\n[1]  2  4 14 15 16\n\npermute_nb(nb)[1:3]\n\n[[1]]\n[1]  83  98 120\n\n[[2]]\n[1] 124  54 111  69  93\n\n[[3]]\n[1]  49 108  28   4  18\n\n\nNow, let’s repeat the same exercise using conditional permutation.\n\nlags2 &lt;- replicate(\n  nsim, {\n    nb_perm &lt;- permute_nb(nb)\n    st_lag(x, nb_perm, st_weights(nb_perm))\n  }\n)\n\nlags2_vec &lt;- as.numeric(lags2)\n  \nlength(unique(lags2_vec))\n\n[1] 71855\n\n\nThere are farrrrr more unique values. In fact, there is a unique value for each simulation - location pair. If we look at the histograms, the difference is even more stark. The conditional permutation approach actually begins to represent a real distribution.\n\npar(mfrow = c(1, 2))\nhist(lags_vec, breaks = 20) \nhist(lags2_vec, breaks = 20)\n\n\n\n\nSo, this is all for me to say that bootstrapping isn’t it for creating simulated distributions for which to calculate your p-values."
  }
]