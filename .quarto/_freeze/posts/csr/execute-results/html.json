{
  "hash": "43d88153167361be97e82f5c17724944",
  "result": {
    "markdown": "---\ntitle: Complete spatial randomness\nauthor: Josiah Parry\ndate: '2022-11-08'\nslug: []\ncategories: [spatial, rstats, sfdep]\ntags: [rspatial]\ndraft: no\n---\n\n\n< this is a cliche about Tobler's fist law and things being related\nin space>. Because of Tobler's first law, spatial data tend to not follow any\nspecific distribution. So, p-values are sort of...not all that accurate most of the time. \nP-values in spatial statistics often take a \"non-parametric\" approach instead\nof an \"analytical\" one. \n\nConsider the t-test. T-tests make the assumption that data are \ncoming from a normal distribution. Then p-values are derived from the \ncumulative distribution function. The alternative hypothesis, then, is \nthat the true difference in means is not 0.  \n\nIn the spatial case, our alternative hypothesis is generally \"the observed\nstatistic different than what we would expect under complete spatial\nrandomness?\" But what really does that mean? To know, we have to \nsimulate spatial randomness. \n\nThere are two approaches to simulating spatial randomness that I'll go\nover. One is better than the other. First, I'm going to describe the \nless good one: bootstrap sampling.\n\nLoad the super duper cool packages. We create queen contiguity neighbors and row-standardized weights.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\nlibrary(sfdep)\nlibrary(tidyverse)\n\ngrid <- st_make_grid(cellsize = c(1, 1), n = 12, offset = c(0, 0)) |> \n  as_tibble() |> \n  st_as_sf() |> \n  mutate(\n    id = row_number(),\n    nb = st_contiguity(geometry),\n    wt = st_weights(nb)\n    )\n```\n:::\n\n\nLet's generate some spatially autocorrelated data. This function is a little slow, but it works.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb <- grid[[\"nb\"]]\nwt <- grid[[\"wt\"]]\n\nx <-  geostan::sim_sar(w = wt_as_matrix(nb, wt), rho = 0.78)\n```\n:::\n\n\n## Bootstrap sampling\n\nUnder the bootstrap approach we are sampling from existing spatial \nconfigurations. In our case there are 144 existing neighborhoods.\nFor our simulations, we will randomly sample from existing neighborhoods\nand then recalculate our statistic. It helps us by imposing randomness\ninto our statistic. We can then repeat the process `nsim` times. There is \na limitation, however. It is that there are only `n - 1` \npossible neighborhood configurations per location.\n\nHere we visualize a random point and it's neighbors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# for a given location create vector indicating position of\n# neighbors and self\ncolor_block <- function(i, nb) {\n  res <- ifelse(1:length(nb) %in% nb[[i]], \"neighbors\", NA)\n  res[i] <- \"self\"\n  res\n}\n```\n:::\n\n\nPlot a point and its neighbors\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid |> \n  mutate(block = color_block(sample(1:n(), 1), nb)) |> \n  ggplot(aes(fill = block)) +\n  geom_sf() +\n  labs(title = \"Point and it's neighbors\")\n```\n\n::: {.cell-output-display}\n![](csr_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nFor bootstrap we grab a point and then the neighbors from another point. \nThis function will randomize a `nb` list object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolor_sample_block <- function(i, nb) {\n  index <- 1:length(nb)\n  not_i <- index[-i]\n  \n  sample_block_focal <- sample(not_i, 1)\n  \n  res <- rep(NA, length(index))\n  \n  res[nb[[sample_block_focal]]] <- \"neighbors\"\n  res[i] <- \"self\"\n  res\n}\n\n# visualize it\ngrid |> \n  mutate(block = color_sample_block(sample(1:n(), 1), nb)) |> \n  ggplot(aes(fill = block)) +\n  geom_sf() +\n  labs(title = \"Point and random point's neighbors\")\n```\n\n::: {.cell-output-display}\n![](csr_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nOften, we will want to create a reference distribution by creating a large\nnumber of simulationsâ€”typically 999. As the simulations increase in size, we are limited in the amount of \nsamples we can draw. The number of neighborhoods becomes limiting! \n\nSay we want to look at income distribution in Boston and the only data we \nhave is at the census tract level. I happen to know that Boston has 207 \ntracts. If we want to do 999 simulations, after the 206th simulation, we \nwill likely have gone through all over the neighborhood configurations! \n\nHow can we do this sampling? For each observation, we can sample another\nlocation, grab their neighbors, and assign them as the observed location's\nneighbors. \n\n## Bootstrap simulations\n\nIn sfdep, we use spdep's `nb` object. These are lists that store the row\nposition of the neighbors as integer vectors at each element. \n\n> If you want to learn more about neighbors [I gave a talk](https://youtu.be/i_MA1U6SJ1Y?t=1301) at NY Hackr MeetUp a few months ago that might help.\n\nHere I define a function that samples from the positions (`index`), \nthen uses that sample to shuffle up the existing neighborhoods and \nreturn a shuffled `nb` object. Note that I add the `nb` class back\nto the list. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_nbs <- function(nb) {\n  # create index\n  index <- 1:length(nb)\n  # create a resampled index\n  resampled_index <- sample(index, replace = TRUE)\n  # shuffle the neighbors and reassign class\n  structure(nb[resampled_index], class = c(\"nb\", \"list\"))\n}\n```\n:::\n\n\nLet's compare some observations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb[1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1]  2 13 14\n\n[[2]]\n[1]  1  3 13 14 15\n\n[[3]]\n[1]  2  4 14 15 16\n```\n:::\n\n```{.r .cell-code}\nbootstrap_nbs(nb)[1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1]  7  9 19 20 21\n\n[[2]]\n[1]  3  4  5 15 17 27 28 29\n\n[[3]]\n[1]  92  93  94 104 106 116 117 118\n```\n:::\n:::\n\n\nHere we can see the random pattern. Look's like there is fair amount of \nclustering of like values. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid |> \n  mutate(x = classInt::classify_intervals(x, 7)) |> \n  ggplot(aes(fill = x)) +\n  geom_sf(color = NA, lwd = 0) +\n  scale_fill_brewer(type = \"div\", palette = 5, direction = -1) +\n  theme_void() \n```\n\n::: {.cell-output-display}\n![](csr_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nWith the weights and the neighbors we can calculate the global Moran.\nI'll refer to this as the \"observed.\" Store it into an object called \n`obs`. We'll need this to calculate a simulated p-value later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs <- global_moran(x, nb, wt)\nobs[[\"I\"]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.346877\n```\n:::\n:::\n\n\n0.35 is a fair amount of positive spatial autocorrelation indicating \nthat like values tend to cluster. But is this due to random chance, or\ndoes it depend on where these locations are?\nNow that we have the observed value of Moran's I, we can simulate the \nvalue under spatial randomness using the bootstrapped sampling. To do so,\nwe bootstrap sample our neighbors, recalculate the weights and then \nthe global Moran.\nNow, if you've read my [vignette on conditional permutation](https://sfdep.josiahparry.com/articles/conditional-permutation.html), you know what\nis coming next. We need to create a reference distribution of the global\nMoran under spatial randomness. To do that, we apply our boot strap `nsim`\ntimes and recalculate the global Moran with each new neighbor list. I love\nthe function `replicate()` for these purposes. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsim = 499 \n```\n:::\n\n\n> _Also, a thing I've started doing is assigning scalars / constants with an equals sign because they typically end up becoming function arguments._\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreps <- replicate(\n  nsim, {\n    nb_sim <- bootstrap_nbs(nb)\n    wt_sim <- st_weights(nb_sim)\n    global_moran(x, nb_sim, wt_sim)[[\"I\"]]\n  }\n)\n\nhist(reps, xlim = c(min(reps), obs[[\"I\"]]))\nabline(v = obs[[\"I\"]], lty = 2)\n```\n\n::: {.cell-output-display}\n![](csr_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n### Bootstrap limitations\n\nThat's all well and good, but let's look at this a bit more. Since we're\nusing the bootstrap approach, we're limited in the number of unique \ncombinations that are possible. Let's try something. Let's calculate\nthe spatial lag `nsim` times and find the number of unique values that\nwe get. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlags <- replicate(\n  nsim, {\n    # resample the neighbors list\n    nb_sim <- bootstrap_nbs(nb)\n    # recalculate the weights\n    wt_sim <- st_weights(nb_sim)\n    # calculate the lag\n    st_lag(x, nb_sim, wt_sim)\n  }\n)\n\n# cast from matrix to vector\nlags_vec <- as.numeric(lags)\n\n# how many are there?\nlength(lags_vec)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 71856\n```\n:::\n\n```{.r .cell-code}\n# how many unique?\nlength(unique(lags_vec))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 144\n```\n:::\n:::\n\n\nSee this? There are only 144 unique value! That isn't much! Don't believe me? Run `table(lags_vec)`.\nFor each location there are only a limited number of combinations that\ncan occur. \n\n## Conditional Permutation\n\nNow, here is where I want to introduce what I view to be the superior\nalternative: conditional permutation. Conditional permutation was \ndescribed by Luc Anselin in his seminal 1995 paper. The idea is that we\nhold an observation constant, then we randomly assign neighbors. This \nis like the bootstrap approach but instead of grabbing a random \nobservation's neighborhood we create a totally new one. We do this be\nassigning the neighbors randomly from all possible locations.\n\nLet's look at how we can program this. For each location we need to\nsample from an index that excludes the observation's position. Further\nwe need to ensure that there are the same number of neighbors in each\nlocation (cardinality).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npermute_nb <- function(nb) {\n  # first get the cardinality\n  cards <- st_cardinalties(nb)\n  # instantiate empty list to fill\n  nb_perm <- vector(mode = \"list\", length = length(nb))\n  \n  # instantiate an index\n  index <- seq_along(nb)\n  \n  # iterate through and full nb_perm\n  for (i in index) {\n    # remove i from the index, then sample and assign\n    nb_perm[[i]] <- sample(index[-i], cards[i])\n  }\n  \n  structure(nb_perm, class = \"nb\")\n}\n\n\nnb[1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1]  2 13 14\n\n[[2]]\n[1]  1  3 13 14 15\n\n[[3]]\n[1]  2  4 14 15 16\n```\n:::\n\n```{.r .cell-code}\npermute_nb(nb)[1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] 68 71 51\n\n[[2]]\n[1]  27  44  26  35 127\n\n[[3]]\n[1]   6 134  48 136  70\n```\n:::\n:::\n\n\nNow, let's repeat the same exercise using conditional permutation. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlags2 <- replicate(\n  nsim, {\n    nb_perm <- permute_nb(nb)\n    st_lag(x, nb_perm, st_weights(nb_perm))\n  }\n)\n\nlags2_vec <- as.numeric(lags2)\n  \nlength(unique(lags2_vec))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 71853\n```\n:::\n:::\n\n\nThere are farrrrr more unique values. In fact, there is a unique value\nfor each simulation - location pair. If we look at the histograms, \nthe difference is even more stark. The conditional permutation approach actually begins to represent a real distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nhist(lags_vec, breaks = 20) \nhist(lags2_vec, breaks = 20)\n```\n\n::: {.cell-output-display}\n![](csr_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nSo, this is all for me to say that bootstrapping isn't it for creating\nsimulated distributions for which to calculate your p-values. \n",
    "supporting": [
      "csr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}