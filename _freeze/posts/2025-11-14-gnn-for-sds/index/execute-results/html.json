{
  "hash": "686924bf16e8c371111f53895e73a94b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Graph Neural Nets for Spatial Data Science \nsubtitle: Spatial lags all the way down\ndate: \"2025-11-16\"\nauthor: Josiah Parry\ncategories: [spatial, r, deep-learning, gnn]\ncode-fold: true\n---\n\nI've been saying this for years and I'll say it again:\n\n> *Spatial data is just a graph*\n\nI've been diving *deep* into **Graph Neural Networks** (GNN) to understand how we can use them for \"GeoAI\" and spatial machine learning.\n\nThis post is going to build the intuition for why Graph Neural Networks are perfect for spatial data (non-raster).\n\n## Spatial Weights Matrices are graphs\n\nA graph is defined as a set **nodes** connected by **edges**. Nodes can be thought of as \"things\" or \"entities\" and then the connections between them are the edges. The nodes can be thought of as **rows** in a dataset. Then the edges are an additional \"edge list\" that informs us of connectivity between them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(igraph)\n\nset.seed(0)\ng <- sample_gnm(6, 11)\nplot(g)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nWhen we do spatial statistics we need to create the concept of a **neighborhood**. For example with polygons, we typically use **contiguity** to define our neighborhood.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\ndata(guerry, package = \"sfdep\")\nplot(guerry$geometry)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIf we identify *neighborhoods* based on contiguity we then identify our neighbors based on that which we use to create a **spatial weights matrix**.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(spdep)\nnb <- poly2nb(guerry)\nlw <- nb2listw(nb)\nlw\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 85 \nNumber of nonzero links: 420 \nPercentage nonzero weights: 5.813149 \nAverage number of links: 4.941176 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0      S1       S2\nW 85 7225 85 37.2761 347.6683\n```\n\n\n:::\n:::\n\n\n::: aside\nThe `listw` object from spdep is the representation of the spatial weights matrix.\n:::\n\nThe spatial weights matrix is a **sparse matrix**. For every location in our data set there is a row and a columnâ€”`n x n` matrix. A non-zero value in a row indicates a neighbor. The connections between locations (or the **spatial weights matrix** (SWM)) can be viewed as a network.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(nb, guerry$geometry)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nAs you can see a spatial weight matrix **is a graph**! \n\n::: aside \nTo be exceptionally pedantic (as this whole post is) a SWM is a graph $G = (V , E)$ where each node, $v_i$, is a location in a set of nodes $(i = 1, ..., n)$. The edges in the graph $e_{ij} = (v_i, v_j, w_{ij})$ are the spatial weights between neighbors.\n:::\n\nOkay, moving on.\n\n## Spatial Lags and Message Passing\n\nThe crux of spatial econometrics is the spatial lag. For all intents and purposes, the spatial lag is \"just\" an average of a variable $X$ over a neighborhood.\n\n::: callout-note\nSee my [YouTube video](https://www.youtube.com/watch?v=abrQBSdTk7E) on this in more depth.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(sfdep)\nlibrary(ggplot2)\nlibrary(patchwork)\n\ndf <- guerry_nb |> \n  mutate(crime_lag = st_lag(crime_pers, nb, wt))\n\nobs <- ggplot(df) +\n  geom_sf(aes(fill = crime_pers)) +\n  scale_fill_viridis_c() +\n  theme_void() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Observed\")\n\nlagged <- ggplot(df) +\n  geom_sf(aes(fill = crime_lag)) +\n  scale_fill_viridis_c() +\n  theme_void() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Spatial Lag\")\n\nobs | lagged\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n::: callout-important\n## ðŸ‘‡ðŸ½ Don't miss this\n\nðŸ‘‰ðŸ¼ The spatial lag is a way to **aggregate information** about a location's neighborhood. ðŸ‘ˆðŸ¼\n:::\n\nGNNs are based on the concept of **message passing**. Message passing is how we propagate information from a node's neighbors (edge connections) to the node itself.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a simple 4-node network: central node with 3 neighbors\nedges <- matrix(c(\n  2, 1,  # node 2 -> central node\n  3, 1,  # node 3 -> central node\n  4, 1   # node 4 -> central node\n), ncol = 2, byrow = TRUE)\n\ng <- graph_from_edgelist(edges, directed = TRUE)\n\n# Assign values to each node\nnode_values <- c(0.2, 0.8, 0.5, 0.9)\n\n# Create layout: central node in middle, others in circle\nlayout <- matrix(c(\n  0, 0,           # central node\n  -1, 1,          # node 2\n  -1, -1,         # node 3\n  1, 0            # node 4\n), ncol = 2, byrow = TRUE)\n\n# Plot\nplot(g,\n  vertex.size = 30,\n  vertex.color = rgb(node_values, 0, 1 - node_values),\n  vertex.label = paste0(\"x\", 1:4),\n  vertex.label.cex = 1.2,\n  vertex.label.font = 2,\n  vertex.label.color = \"white\",\n  edge.width = 2,\n  edge.arrow.size = 1,\n  edge.color = \"gray50\",\n  layout = layout,\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nIn this graph, we have nodes x1 through x4. The nodes x2, x3, and x4 are \"passing\" their message to the focal node. These node values have to be summarized in some way. This is typically called an **aggregation function**. The most common one? The average!\n\n### The takeaway\n\nThe spatial lag operator **is message passing**! This means that most of our spatial econometric modelsâ€”spatial lag and spatial error models and many others are actually utilizing graph message passing in some way.\n\n## Graph Convolution Networks\n\nThe most common type of GNN is the **Graph Convolution Network** (GCN).\n\n::: aside\nThe term \"Convolution\" is often used for deep learning models that perform image detectionâ€”typically a **Convolution Neural Network**. These apply a \"filter\" over each pixel in an image and typically take the weighted sum (or average) of the neighboring pixels to summarize nearby pixels.\n\nThis is literally the same thing! Except pixels have a fixed grid whereas we're generalizing message passing and spatial lags to *any* network with a node and neighbors.\n:::\n\nThe GCN is defined from a famous paper [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) as:\n\n$$\n\\mathbf{H}^{(k)} = \\sigma\\left(\\tilde{\\mathbf{A}}\\mathbf{H}^{(k-1)}\\mathbf{W}^{(k)}\\right) \n$$\n\nThe single layer version of this can be written as:\n\n$$\n\\mathbf{H} = \\sigma\\left(\\tilde{\\mathbf{A}} \\mathbf{X} \\mathbf{W}\\right)\n$$\n\nThis is actually a lot less scary once you think of everything as a spatial lag and locations on a map. Let's start with $\\tilde{A}$.\n\n### The adjacency matrix\n\n$$\n\\tilde{\\mathbf{A}} = (\\mathbf{D} + \\mathbf{I})^{-\\frac{1}{2}}(\\mathbf{I} + \\mathbf{A})(\\mathbf{D} + \\mathbf{I})^{-\\frac{1}{2}} \n$$\n\nThis equation is saying that we have an adjacency matrix $A$ which is \"normalized\" (scaled by) the **degree matrix** $D$. The degree matrix counts the number of neighbors per location.\n\nThe matrix D is equivalent to `spdep::card(nb)`.\n\nThe biggest difference here between spatial econometrics and the GCN is that the adjacency matrix $A$ must have \"self-loops.\" The $I + A$ is equivalent to `spdep::include.self()`. This ensures that the neighborhood includes the observed location.\n\n::: aside\n$I$ is called the \"identity matrix.\" It's literally just the diagonal of a matrix set to 1.\n:::\n\nThe point of this matrix is to ensure that when we multiply values together the scale is roughly maintained and that the contribution from neighbors **and** the focal node (location) are roughly balanced.\n\n### The weights matrix $W$\n\nThe strength of neural networks is their ability to learn representations of our variablesâ€”typically using a higher dimension representation. For example, taking 3 variables and mapping them onto, say, 3 \"hidden\" dimension would create a total of 9 values of $W$ that would be learned on. These embeddings can capture more patterns than might be possible from the values themselves.\n\n![](images/paste-3.png)\n\n::: callout-note\nThe rows of $W$ correspond to the input variables. The columns indicate the \"hidden dimensions.\" Multiplying by the $W$ creates \"node embeddings.\" The more hidden dimensions the more embedding variables there are.\n:::\n\nThe matrix $W$ is **learnable** which means that the network adjusts the values to best predict the target variable $Y$. The $W$ is akin to the $\\beta$ coefficients of a regression model.\n\n## Single layer GCN is just regression\n\nIn a regression we have a $\\beta$ for each variable in our model. If we have 3 variables in our $X$ matrix (the predictor variables) and we specify that our single layer model only have 1 hidden dimension then the $W$ only has 3 valuesâ€”one per variable. This is essentially a learned version of the $\\beta$!\n\n![](images/paste-4.png)\n\nIn this example we only have 1 hidden dimension which means we only have 1 weight per variableâ€”akin to the actual $\\beta$ of a linear regression. Now, this is actually ignoring the message passing aspect of this. To be a GCN we need to calculate the spatial lag of each X before we multiply by the learned weights. The actual GCN looks like\n\n![](images/paste-5.png)\n\nFor the spatial econometricians in the houseâ€”this might look like a spatially lagged X model without the original X matrix.\n\n::: aside \nSee [Spatial Data Science ch 17 on econometric models](https://r-spatial.org/book/17-Econometrics.html) for a fairly terse discussion on these models by Bivand and Pebesma.\n:::\n\nLet's first create a spatially lagged X model by hand: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Create neighborhood structure with self-loops\nnb <- include.self(guerry_nb$nb)\nlw <- spdep::nb2listw(nb, style = \"W\")\n\n# Prepare feature matrix\nx <- guerry |>\n  sf::st_drop_geometry() |>\n  select(commerce, crime_pers, donations) |>\n  scale()\n\n# calculate spatial lags for each variable\nsp_lags <- lag.listw(lw, x)\n\n# define y variable\ny <- guerry$instruction\n\n# fit slx model\nslx_mod <- lm(y ~ sp_lags)\nsummary(slx_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ sp_lags)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-45.22 -11.60   0.38  11.55  35.09 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 43.64726    1.93599  22.545  < 2e-16 ***\nsp_lags1    24.27125    2.92102   8.309 1.84e-12 ***\nsp_lags2    -1.06314    3.23981  -0.328    0.744    \nsp_lags3     0.09644    3.84151   0.025    0.980    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.75 on 81 degrees of freedom\nMultiple R-squared:  0.5086,\tAdjusted R-squared:  0.4904 \nF-statistic: 27.95 on 3 and 81 DF,  p-value: 1.663e-12\n```\n\n\n:::\n:::\n\n\nIf we treat the regression coefficients as our $W$ matrix in a GCN layer we can get the _exact_ same values from a GCN layer! \n\nHere we're using my [`{torchgnn}`](https://github.com/josiahparry/torchgnn) package to create a `layer_gcn()`.\n\n::: aside\n`{torchgnn}` is not yet on CRAN. You can install it with `pak::pak(\"josiahparry/torchgnn\")`.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(torch)\nlibrary(torchgnn)\n\n# Create sparse adjacency matrix for GCN\nadj <- adj_from_edgelist(\n  rep.int(seq_along(nb), lengths(nb)),\n  unlist(nb),\n  unlist(lw$weights), \n  # already handled by spdep\n  symmetric = FALSE\n)$coalesce()\n\n# convert X to a tensor for torch\nX <- torch_tensor(x)\n\n# don't modify the adjacency matrix\nlayer <- layer_gcn(3, 1, normalize = FALSE)\n\n# set the weights to the same as a the regression\nwith_no_grad({\n  layer$weight$copy_(torch_tensor(matrix(coef(slx_mod)[-1], nrow = 3)))\n})\n\n# set the bias to the intercept\nwith_no_grad({\n  layer$bias$copy_(torch_tensor(coef(slx_mod)[1]))\n})\n```\n:::\n\n\nIn the above code check we created a single GCN layer and manually set the bias (equivalent to an intercept term) and the weights. Typically these would be learned by a training loop. But for the sake of education, we're setting them to be the equivalent to our linear regression. \n\nNext we apply the layer to the $X$ and  $A$ matrices to get our $\\hat{Y}$ predictions. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntibble::tibble(\n  gcn_preds = as.numeric(layer(X, adj)),\n  slx_preds = unname(slx_mod$fitted.values)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 85 Ã— 2\n   gcn_preds slx_preds\n       <dbl>     <dbl>\n 1     47.9      47.9 \n 2     18.1      18.1 \n 3     64.9      64.9 \n 4     46.5      46.5 \n 5     52.0      52.0 \n 6     49.4      49.4 \n 7      6.78      6.78\n 8     45.5      45.5 \n 9     10.9      10.9 \n10     44.1      44.1 \n# â„¹ 75 more rows\n```\n\n\n:::\n:::\n\n\n## Fuller GCN models \n\nIn the above we showed that the GCN layer is literally just a fancy regression model using the spatial lags of our input variables. However, we assumed that there was not any [activation function](https://en.wikipedia.org/wiki/Activation_function).\n\nFor fuller examples we can fit a GCN with multiple hidden layers as well as an additional activation or loss functions. These GCN models can generalize well to classification or regression.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}