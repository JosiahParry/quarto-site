{
  "hash": "c8327ef8a604dfa0e918f42b9af9af87",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Encoding spatial patterns as variables\"\nsubtitle: \"Principal Coordinate Analysis & Moran Eigenvectors\"\ndate: \"2024-05-17\"\ncategories: [spatial, r]\nfreeze: true\n---\n\n\nI've begun reading [\"Spatial modelling: a comprehensive framework for principal coordinate analysis of neighbour matrices (PCNM)\"](https://www.sciencedirect.com/science/article/abs/pii/S0304380006000925) which describes the process of making \"Moran Eigenvector Maps.\"\n\nIn this case, I haven't finished reading the paper but am quite thrilled by the prospect of it. One of the biggest problems in ecological and social science modelling is that space is often a confounder in models. By this I mean that a lot of phenomena we see are **spatially dependent**.\n\nReductively, spatial dependence means that variables or outcomes are strongly linked to where things are. For example, income tends to be spatially dependent. Meaning that high income areas are typically surounded by other high income areas. \n\n## The problem\n\nWhen modelling data that exhibit spatial dependence, spatial relationships need to be accounted for. Otherwise, you will often find that model residuals (errors) _also exhibit spatial dependence_. So? How can you control for this. \n\nThere are a number of techniques that people use from more statistically sound ones, to tricks used by ML engineers. For example you may introduce the spatial lag (neighborhood average of a variable) to account for some of the spatial association. \n\n## Principal Coordinate Analysis (PCoA)\n\nOne interesting idea is using principle components analysis to encode geography into numeric variables. Conceptually, the idea is actually rather simple! \n\nWhen we do spatial statistics, we create what are called spatial weights matrices. These define which features are related to eachother. \n\n\n\n\n\nFor example we can identify the neighbors from the famous guerry dataset based on the contiguity—that is if they are touching. We create a `nb` and `wt` object. The `nb` are the neighbors and `wt` uses a gaussian kernel. The gaussian kernel assigns more weight to to locations that are closer and less weight to those that are further—essentially following the normal distribution. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sfdep)\nlibrary(dplyr)\n\ngeoms <- guerry$geometry\ncentroids <- sf::st_centroid(geoms)\n\nnb <- st_contiguity(geoms)\nwt <- st_kernel_weights(nb, centroids, \"gaussian\")\n```\n:::\n\n\nVisually, this is what the neighborhood relationship looks like: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nsfn <- st_as_graph(geoms, nb, )\n\nautoplot(sfn) +\n  geom_sf(data = geoms, fill = NA, color = \"black\", lwd = 0.2) +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](2024-05-17-moran-eigenvectors_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe weights object is a ragged array which is used to be a sparse matrix representation of the spatial weights. \n \n\n::: {.cell}\n\n```{.r .cell-code}\nhead(wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 1.553402 1.857660 2.062100 1.676694\n\n[[2]]\n[1] 1.801787 1.717777 1.439955 1.721547 1.260566 1.429496\n\n[[3]]\n[1] 1.599532 1.527097 1.376795 1.722723 1.865664 1.350771\n\n[[4]]\n[1] 2.040754 1.356645 1.871658 1.685343\n\n[[5]]\n[1] 2.040754 1.674375 1.689488\n\n[[6]]\n[1] 2.075805 1.679763 1.357435 1.308397 2.009760 1.812262 1.432539\n```\n\n\n:::\n:::\n\n\nThe spatial weights are an `n x n` square matrix. The idea behind the paper above is that we can encode the spatial relationships in this neighborhood matrix using principle components. \n\nWe can take the weights matrix and create a dense matrix from it: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- wt_as_matrix(nb, wt)\n```\n:::\n\n\nUsing this new matrix, we can perform PCA on it. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_res <- prcomp(m)\nsummary(pca_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                           PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     0.95957 0.89585 0.85836 0.81484 0.79239 0.72065 0.66135\nProportion of Variance 0.06928 0.06038 0.05543 0.04996 0.04724 0.03907 0.03291\nCumulative Proportion  0.06928 0.12966 0.18510 0.23505 0.28229 0.32137 0.35428\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.65347 0.60391 0.58993 0.54551 0.51607 0.51048 0.50266\nProportion of Variance 0.03213 0.02744 0.02618 0.02239 0.02004 0.01961 0.01901\nCumulative Proportion  0.38640 0.41385 0.44003 0.46242 0.48246 0.50206 0.52107\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.50008 0.49651 0.48004 0.47334 0.46571 0.46447 0.45886\nProportion of Variance 0.01882 0.01855 0.01734 0.01686 0.01632 0.01623 0.01584\nCumulative Proportion  0.53989 0.55844 0.57578 0.59263 0.60895 0.62518 0.64103\n                          PC22   PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.45371 0.4495 0.43495 0.43208 0.42533 0.42265 0.40912\nProportion of Variance 0.01549 0.0152 0.01423 0.01405 0.01361 0.01344 0.01259\nCumulative Proportion  0.65651 0.6717 0.68595 0.70000 0.71361 0.72705 0.73964\n                          PC29    PC30    PC31    PC32    PC33    PC34    PC35\nStandard deviation     0.40662 0.40248 0.39657 0.38949 0.38172 0.37648 0.36612\nProportion of Variance 0.01244 0.01219 0.01183 0.01141 0.01096 0.01066 0.01009\nCumulative Proportion  0.75208 0.76427 0.77610 0.78752 0.79848 0.80915 0.81923\n                          PC36    PC37    PC38    PC39    PC40    PC41    PC42\nStandard deviation     0.35885 0.35324 0.35042 0.34655 0.33906 0.33458 0.32477\nProportion of Variance 0.00969 0.00939 0.00924 0.00904 0.00865 0.00842 0.00794\nCumulative Proportion  0.82892 0.83831 0.84755 0.85658 0.86523 0.87366 0.88159\n                          PC43    PC44    PC45    PC46    PC47    PC48    PC49\nStandard deviation     0.32182 0.30859 0.30426 0.30100 0.29700 0.28072 0.27493\nProportion of Variance 0.00779 0.00716 0.00697 0.00682 0.00664 0.00593 0.00569\nCumulative Proportion  0.88938 0.89655 0.90351 0.91033 0.91697 0.92290 0.92858\n                          PC50    PC51    PC52    PC53    PC54    PC55    PC56\nStandard deviation     0.26620 0.25927 0.25817 0.25373 0.25203 0.23148 0.22505\nProportion of Variance 0.00533 0.00506 0.00501 0.00484 0.00478 0.00403 0.00381\nCumulative Proportion  0.93391 0.93897 0.94399 0.94883 0.95361 0.95764 0.96145\n                          PC57   PC58    PC59    PC60    PC61    PC62    PC63\nStandard deviation     0.21925 0.2124 0.20738 0.20542 0.20426 0.17969 0.17415\nProportion of Variance 0.00362 0.0034 0.00324 0.00318 0.00314 0.00243 0.00228\nCumulative Proportion  0.96507 0.9685 0.97170 0.97488 0.97801 0.98044 0.98273\n                          PC64    PC65    PC66    PC67    PC68    PC69    PC70\nStandard deviation     0.17330 0.16078 0.15374 0.14641 0.14201 0.13335 0.13088\nProportion of Variance 0.00226 0.00194 0.00178 0.00161 0.00152 0.00134 0.00129\nCumulative Proportion  0.98499 0.98693 0.98871 0.99032 0.99184 0.99318 0.99447\n                          PC71    PC72    PC73    PC74    PC75    PC76    PC77\nStandard deviation     0.12526 0.10810 0.10181 0.08943 0.08425 0.07410 0.07172\nProportion of Variance 0.00118 0.00088 0.00078 0.00060 0.00053 0.00041 0.00039\nCumulative Proportion  0.99565 0.99653 0.99731 0.99791 0.99844 0.99885 0.99924\n                          PC78    PC79    PC80    PC81    PC82    PC83     PC84\nStandard deviation     0.06809 0.04608 0.03760 0.03481 0.02250 0.01105 0.008143\nProportion of Variance 0.00035 0.00016 0.00011 0.00009 0.00004 0.00001 0.000000\nCumulative Proportion  0.99959 0.99975 0.99986 0.99995 0.99999 1.00000 1.000000\n                            PC85\nStandard deviation     3.921e-16\nProportion of Variance 0.000e+00\nCumulative Proportion  1.000e+00\n```\n\n\n:::\n:::\n\n\nThe spatial relationships that are embedded by the spatial weights matrix, are now encoded as components from a PCA. This means that we can use each of these components as a univariate measure of space. And, they also exhibit quite interesting patterns of spatial dependence. \n\n## Exploring PCoA \n\nThese components essentially capture spatial autocorrelation. For example we an look at the first component. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract the first component\ncomp1 <- pca_res$rotation[, 1]\n\nggplot(guerry, aes(fill = comp1)) +\n  geom_sf(color = \"black\", lwd = 0.2) +\n  scale_fill_viridis_c() +\n  theme_void() +\n  labs(fill = \"Eigenvector\")\n```\n\n::: {.cell-output-display}\n![](2024-05-17-moran-eigenvectors_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nIt displays a pattern of being near Paris (the dark purple, or negative eigenvector values) or being nearer to Aveyron, the positive eigenvector values. Clearly, this displays some interesting global spatial autocorrelation. But how much? \n\nWe can measure the global spatial autocorrelation of this component using Moran's I.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_perm(comp1, nb, wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 500 \n\nstatistic = 1.0698, observed rank = 500, p-value < 2.2e-16\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\nThe result is `1.0698` which is greater than the theoretical maximum of 1. There is a ridiculous amount of spatial autocorrelation here. \n\n## Using PCoA Eigenvectors to reduce spatial confounding\n\nPredicting crime based on population and the prostitution levels of 1830s France shows that there is a _lot_ of spatial autocorrelation in the residuals. This means that the results of the model do not appropriately account for spatial dependence. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(crime_pers ~ pop1831 + prostitutes, data = guerry)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = crime_pers ~ pop1831 + prostitutes, data = guerry)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13762.2  -4592.1   -974.6   4892.4  18672.5 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13688.932   2265.393   6.043 4.27e-08 ***\npop1831        17.676      5.881   3.006  0.00352 ** \nprostitutes    -3.197      1.665  -1.920  0.05833 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7001 on 82 degrees of freedom\nMultiple R-squared:  0.1021,\tAdjusted R-squared:  0.08016 \nF-statistic:  4.66 on 2 and 82 DF,  p-value: 0.01211\n```\n\n\n:::\n\n```{.r .cell-code}\nglobal_moran_test(resid(mod), nb, wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 5.0424, p-value = 2.298e-07\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.328493924      -0.011904762       0.004557168 \n```\n\n\n:::\n:::\n\n\n\nIf you include the first eigenvector component, the spatial autocorrelation of the residuals decrease dramatically. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(crime_pers ~ pop1831 + prostitutes + comp1, data = guerry)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = crime_pers ~ pop1831 + prostitutes + comp1, data = guerry)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14228.8  -3822.7   -893.4   4232.5  19718.8 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  14847.292   2080.269   7.137 3.66e-10 ***\npop1831         15.179      5.386   2.818  0.00607 ** \nprostitutes     -4.597      1.551  -2.964  0.00399 ** \ncomp1       -28422.828   6708.123  -4.237 5.95e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6373 on 81 degrees of freedom\nMultiple R-squared:  0.265,\tAdjusted R-squared:  0.2378 \nF-statistic: 9.733 on 3 and 81 DF,  p-value: 1.482e-05\n```\n\n\n:::\n\n```{.r .cell-code}\nglobal_moran_test(resid(mod), nb, wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 2.666, p-value = 0.003838\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.167439656      -0.011904762       0.004525529 \n```\n\n\n:::\n:::\n\n\nInterestingly, this increases the $R^2$ by 16 which is nothing to scoff at. The significance of `prostitutes` variable increases and the $\\beta$ values shrink. And the first component accounts for pretty much everything else lol!\n\n\n### What about another component? \n\nWe can plot the relationship that is capture by the second component.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract the second component\ncomp2 <- pca_res$rotation[, 2]\n\nggplot(guerry, aes(fill = comp2)) +\n  geom_sf(color = \"black\", lwd = 0.2) +\n  scale_fill_viridis_c() +\n  theme_void() +\n  labs(fill = \"Eigenvector\")\n```\n\n::: {.cell-output-display}\n![](2024-05-17-moran-eigenvectors_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThis component captures a west to east relationship rather than a north to south one. \nIs the second component spatially autocorrelated?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_perm(comp2, nb, wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 500 \n\nstatistic = 0.99864, observed rank = 500, p-value < 2.2e-16\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\nOh hell yeah it is. \n\nIf this component is included in the model instead of the first one we see something interesting. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(crime_pers ~ pop1831 + prostitutes + comp2, data = guerry)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = crime_pers ~ pop1831 + prostitutes + comp2, data = guerry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13617  -4584  -1150   4831  18360 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13730.388   2278.027   6.027 4.71e-08 ***\npop1831        17.518      5.919   2.960  0.00404 ** \nprostitutes    -3.098      1.686  -1.837  0.06989 .  \ncomp2        3303.053   7091.459   0.466  0.64262    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7034 on 81 degrees of freedom\nMultiple R-squared:  0.1045,\tAdjusted R-squared:  0.0713 \nF-statistic:  3.15 on 3 and 81 DF,  p-value: 0.02941\n```\n\n\n:::\n\n```{.r .cell-code}\nglobal_moran_test(resid(mod), nb, wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 5.0189, p-value = 2.598e-07\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.326953780      -0.011904762       0.004558477 \n```\n\n\n:::\n:::\n\n\nThe model is not impacted nor is the spatial autocorrelation. So the pattern encompassed by the second component is not confounding our variables like the first one is. \n\n## What does this mean?\n\nIf you have spatially dependent features that you're predicting you should consider using these as input features to your models. I have a hunch that they would work insanely well with computer vision tasks and things models like Random Forests and XGBoost.",
    "supporting": [
      "2024-05-17-moran-eigenvectors_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}