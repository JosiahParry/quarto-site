{
  "hash": "df7dcee7ee7b98b0599b9e448b8287d3",
  "result": {
    "markdown": "---\ntitle: \"Untitled\"\n---\n\n\nI'm working on a new video for you all on GeoHashes. As I keep working on my slides and script I keep finding new things I want to explore or that I need to write. Recently, that was to compare multiple geohashes to a known geohash. The goal of that is to count how many of the first characters matched a reference. If there are matching leading characters between a geohashes that means that they are in the same geohash at some level of precision. Knowing that number of shared characters tells us at what level of precision they coexist. The challenge is that there isn't any easy way to do that in base R or packages I could find. So, what do we do when we can't find something to do a task for us? We make it. \n\nFor these small tasks that require a lot of iteration and counting, I've been leaning on Rust a lot more. I find it actually _easier_ for the more computer sciency type tasks. \n\nHere's how I solved it. \n\nDefine two geohashes to compare:\n\n```rust\nlet x = \"drt2yyy1cxwy\";\nlet y = \"drt2yywg71qc\";\n```\n\nNext we want to iterate over each of these string slices (represented as `&str`). Typically we'd use the `.iter()` or `.into_iter()` methods to iterate over objects but these are slices and not a vector or array.\n\n:::{.aside}\n`.into_iter()` consumes the object you're iterating over whereas `.iter()` iterates over it without consuming. The former provides \"owned\" objects at each iteration while the latter provides references\"\n:::\n\nWe iterate through the characters of a slice using `.chars()`. We'll want to iterate through both of them at the same time. Then, for each iteration, we check to see if they're the same. \n\nThis will instantiate an iterator over each of the strings where each element is a `char`\n\n```rust\n  x.chars()\n  y.chars()\n```\n:::{.callout-important}\nThis will not compile, it's for illustration\n:::\n\nThese iterators will only be able to be iterated over one at a time using `.map()` and the like. We can combine them into one iterator using the `.zip()` method which _zips_ them together into one iterator. \n\n```rust\nx.chars().zip(y.chars())\n```\n\nThis is good! Now we have a single iterator to work through. Each element in the resultant iterator will be a tuple with the first element `.0` being the first character of `x` and `.1` being the first character of `y`. \n\n:::{.aside}\nTuple's look like `let z = (x, y);` and are accessed by position like `z.0` and `z.1`.\n:::\n\nThe approach I took here is to use the `.take_while()` method which takes a closure that returns a `bool` (`true` or `false`). It'll return another iterator that contains only the elements where that statement was true. \n\n```rust\nx.chars().zip(y.chars())\n    .take_while(|a| a.0 == a.1);\n```\n\n:::{.aside}\nA closure is like an anonymous function. It's arguments are defined between `| |` and the evaluated expression is to the right of it.\n:::\n\nHere, the closure has the argument `|a|` which is the tuple from `x` and `y`. It checks to see if the characters are equal. The resultant iterator now only has elements for matching characters. We don't really need to iterate over it, but rather we just need to count how many items are in the iterator. \n\nWe can use the `.count()` method for that. Shouts out to the Rust discord for helping me with this one. \n\n:::{.aside}\nPreviously I used a `fold()` method that looked like `.fold(0, |acc, _| acc + 1)` which worked but was less \"elegant\"\n:::\n\n```rust\nlet res = x.chars().zip(y.chars())\n    .take_while(|a| a.0 == a.1)\n    .count();\n```\n\n\nLet's wrap this into a function:\n\n```rust\nfn count_seq_chars(x: &str, y: &str) -> usize {\n  x.chars().zip(y.chars())\n      .take_while(|a| a.0 == a.1)\n      .count()\n}\n```\n\n\nWe can make it available in R using [`rextendr::rust_function()`](https://extendr.github.io/rextendr/reference/rust_source.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrextendr::rust_function(\"\nfn count_seq_chars(x: &str, y: &str) -> usize {\n  x.chars().zip(y.chars())\n      .take_while(|a| a.0 == a.1)\n      .count()\n}\")\n\ncount_seq_chars(\"drt2yyy1cxwy\", \"drt2yywg71qc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6\n```\n:::\n:::\n\n\nBut this isn't vectorized yet. It only works on two scalars. We can improve it by changing the `x` argument to take a vector of strings `Vec<String>`. \n\n:::{.aside}\nWe have to use `Vec<String>` instead of `Vec<&str>` because `rextendr` does not know how to take a vector of string slices.\n:::\n\nEssentially, what we do next is take this vector of strings, iterate over it, convert the string to a `&str` then just do what we did before! \n\n\nWe use `.map()` to apply an expression over each element of `x`. The closure takes a single argument `xi` which represents the ith element of `x`. We convert it to a slice, then iterate over it's characters and the rest should be similar in there! \n\nLastly, we collect the resultant `usize` objects into a vector of them `Vec<usize>`. \n\n\n```rust\n  fn count_seq_chars_to_ref(x: Vec<String>, y: &str) -> Vec<usize> {\n    x.into_iter()\n        .map(|xi| \n            xi.as_str().chars().zip(y.chars())\n            .take_while(|a| a.0 == a.1)\n            .count()\n        )\n        .collect()\n  }\n```\n\n:::{.aside}\nNote that the function definition has ` -> Vec<usize>` this defines what the ouput object type will be. Something definitely unfamiliar for Rusers! \n:::\n\nAgain, we can use `rextendr` to wrap this into a single R function that we can use. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrextendr::rust_function(\"\n  fn count_seq_chars_to_ref(x: Vec<String>, y: &str) -> Vec<usize> {\n    x.into_iter()\n        .map(|xi| \n            xi.as_str().chars().zip(y.chars())\n            .take_while(|a| a.0 == a.1)\n            .count()\n        )\n        .collect()\n  }\n\")\n\ncount_seq_chars_to_ref(\"drt2yyy1cxwy\", \"drt2yywg71qc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6\n```\n:::\n:::\n\n\nLet's test this and see how it works with a larger dataset of 100,000 strings. We create a bunch of sample strings that sample a-e and 1-5, are sorted, then pasted together. We then can compare them to the reference string `\"abcd123\"`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_strings <- replicate(100000, paste0(\n  paste0(sort(sample(letters[1:5], 4)), collapse = \"\"),\n  paste0(sample(1:5, 3), collapse = \"\"),\n  collapse = \"\"\n))\n\n\nhead(sample_strings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"acde324\" \"abde413\" \"bcde513\" \"bcde412\" \"abde531\" \"acde143\"\n```\n:::\n\n```{.r .cell-code}\ncount_seq_chars_to_ref(head(sample_strings), \"abcd123\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 2 0 0 2 1\n```\n:::\n:::\n\n\n\n[Philippe Massicotte](https://www.pmassicotte.com/) was kind enough to provide an R only example in a reply to a tweet of mine. We can compare the speed of the two implementations. A pure Rust implementation and an R native implementation.\n\n{{ tweet philmassicotte 1646191363728240642 }}\n\nHere we wrap his implementation into a function `count_seq_lapply()`. I've modified this implementation to handle the scenario where the first element is not true so we don't get a run length of `FALSE` elements.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount_seq_lapply <- function(x, ref) {\n  res <- lapply(x, \\(x) {\n    a <- unlist(strsplit(x, \"\"))\n    x <- unlist(strsplit(ref, \"\"))\n    \n    comparison <- a == x\n    \n    if (!comparison[1]) return(0)\n    \n    rle(comparison)$lengths[1]\n  })\n  \n  unlist(res)\n}\n\ncount_seq_lapply(head(sample_strings), \"abcd123\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 2 0 0 2 1\n```\n:::\n:::\n\n\nAs you can see his works just as well and frankly, better. That's because he inherits the NA handling of the base R functions he is using. If any NA are introduced into a pure Rust implementation without using [extendr](https://extendr.github.io/extendr/extendr_api/) types and proper handling you'll get a `panic!` which will cause the R function to error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n  lapply = count_seq_lapply(sample_strings, \"abcd123\"), \n  rust = count_seq_chars_to_ref(sample_strings, \"abcd123\")\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 lapply      891.4ms    891ms      1.12    1.58MB     71.8\n2 rust         56.7ms     58ms     17.3    781.3KB      0  \n```\n:::\n:::\n\n\nThe R implementation is still super fast. It's just that Rust is also super super fast! \n\n-------\n\n\n## Addendum: ChatGPT rules apparently\n\nSo I asked Chat GPT to rewrite my above function but using C++ and the results are absolutely startling! \n\n```c++\nstd::vector<size_t> count_seq_chars_to_ref_cpp(std::vector<std::string> x, const std::string& y) {\n  std::vector<size_t> result;\n  for (const auto& xi : x) {\n    size_t count = 0;\n    auto it_x = xi.begin();\n    auto it_y = y.begin();\n    while (it_x != xi.end() && it_y != y.end() && *it_x == *it_y) {\n      ++count;\n      ++it_x;\n      ++it_y;\n    }\n    result.push_back(count);\n  }\n  return result;\n}\n```\n\n\nThis is the code it wrote after only one prompt. I didn't correct it. It worked right off the rip. I did, however, provide ChatGPT with my above rust code. \n\nLet's bench mark this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRcpp::cppFunction(\"std::vector<size_t> count_seq_chars_to_ref_cpp(std::vector<std::string> x, const std::string& y) {\n  std::vector<size_t> result;\n  for (const auto& xi : x) {\n    size_t count = 0;\n    auto it_x = xi.begin();\n    auto it_y = y.begin();\n    while (it_x != xi.end() && it_y != y.end() && *it_x == *it_y) {\n      ++count;\n      ++it_x;\n      ++it_y;\n    }\n    result.push_back(count);\n  }\n  return result;\n}\")\n\nbench::mark(\n  GPT_cpp = count_seq_chars_to_ref_cpp(sample_strings, \"abcd123\"),\n  rust = count_seq_chars_to_ref(sample_strings, \"abcd123\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 GPT_cpp       1.9ms   1.96ms     495.      784KB     8.29\n2 rust         57.7ms  58.87ms      17.0     781KB     0   \n```\n:::\n:::\n\n\nAbsolutely friggin' bonkers!! If I was better at concurrency and threading I'd try to compare that but alas. I'm stopping here :) \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}