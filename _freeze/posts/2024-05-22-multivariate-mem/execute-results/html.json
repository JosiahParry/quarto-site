{
  "hash": "87aec2275a1e97132b5ae51eecb1ef7d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Univariate Spatial Dimensionality Reduction\"\nsubtitle: \"Extening PCoA and Moran Eigenvector Maps to include attributes\"\ndate: \"2024-05-22\"\ncategories: [spatial, r]\nfreeze: true\n---\n\n\nIn discussing principal coordinate analysis (PCoA), the question naturally arose of \"how could we incorporate non-spatial data into this method?\" Well, that is what I wanted to explore.\n\nIf we include an attribute e.g. crime rate, and embed that into the spatial components generated by PCoA we would truly be doing spatial dimension reduction. Not only would we be encoding spatial patterns, but we would be encoding spatial patterns as they relate to some attribute across the spatial surface. \n\nThis is already explored in the context of gene expression via [`{spatialPCA}`](https://github.com/shangll123/SpatialPCA) ([website here](https://lulushang.org/SpatialPCA_Tutorial/)). This is somewhat different to what I explored in my previous blog post. My brief review of the paper and R package tells me that the spatialPCA method applies the spatial weights matrix into the covariance matrix used in PCA. \n\nWhat I'm going to explore is a bit different than that. \n\n# Univariate spatial encoding\n\nNaturally, we want to look at how we can incorporate attributes into PCoA. Lets first start by creating a spatial weights matrix. \n\n\n\n\n\nAs always, we use the Guerry dataset since it is tried and true. Here I create a spatial weights matrix using contiguity to identify neighborhoods and we use a guassian kernel (like spatial PCA). The Gaussian kernel makes it so that locations that are closer have more weight than those further away. Since we are using contiguity, the distance is estimated by using centroids.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngeom <- guerry$geometry\npnts <- st_centroid(geom)\n\n# neighbors via contiguity\nnb <- st_contiguity(geom)\n\n# gaussian kernel weights for neighbors\nwt <- st_kernel_weights(nb, pnts, \"gaussian\")\n\n(listw <- nb2listw(nb, wt, style = \"B\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 85 \nNumber of nonzero links: 420 \nPercentage nonzero weights: 5.813149 \nAverage number of links: 4.941176 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1       S2\nB 85 7225 698.7942 2378.909 24825.15\n```\n\n\n:::\n:::\n\n\n## Revisiting the spatial lag\n\nThe spatial lag is arguably the most fundamental spatial statistic. It is, in essence, the weighted average of a variable $x$ across a neighborhood.\n\nTo calculate the spatial lag, referred often to as $Wy$, we multiply our spatial weights matrix by our neighboring values. Let's walk through how this works really quickly. \n\nWe have the index positions of our neighbors and the weights that are associated with each of them. \n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nhead(nb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 36 37 67 69\n\n[[2]]\n[1]  7 49 57 58 73 76\n\n[[3]]\n[1] 17 21 40 56 61 69\n\n[[4]]\n[1]  5 24 79 80\n\n[[5]]\n[1]  4 24 36\n\n[[6]]\n[1] 24 28 36 40 41 46 80\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 1.553402 1.857660 2.062100 1.676694\n\n[[2]]\n[1] 1.801787 1.717777 1.439955 1.721547 1.260566 1.429496\n\n[[3]]\n[1] 1.599532 1.527097 1.376795 1.722723 1.865664 1.350771\n\n[[4]]\n[1] 2.040754 1.356645 1.871658 1.685343\n\n[[5]]\n[1] 2.040754 1.674375 1.689488\n\n[[6]]\n[1] 2.075805 1.679763 1.357435 1.308397 2.009760 1.812262 1.432539\n```\n\n\n:::\n:::\n\n\nTo calculate the spatial lag we first find the neighbors' values for a vector x, multiply them by the weights and sum them up. In this case the variable is literacy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- guerry$literacy\nxj <- find_xj(x, nb)\nhead(xj)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 29 73 45 32\n\n[[2]]\n[1] 67 63 45 54 54 44\n\n[[3]]\n[1] 13 23 29 20 19 32\n\n[[4]]\n[1] 69 42 23 37\n\n[[5]]\n[1] 46 42 29\n\n[[6]]\n[1] 42 40 29 29 21 27 37\n```\n\n\n:::\n:::\n\n\nWith the neighboring values, we can multiply them by the spatial weights. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nxj_wt <- purrr::map2(xj, wt, \\(.xj, .wt) .xj * .wt)\nhead(xj_wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1]  45.04865 135.60918  92.79449  53.65420\n\n[[2]]\n[1] 120.71975 108.21994  64.79797  92.96353  68.07056  62.89782\n\n[[3]]\n[1] 20.79392 35.12323 39.92704 34.45446 35.44761 43.22467\n\n[[4]]\n[1] 140.81202  56.97907  43.04814  62.35767\n\n[[5]]\n[1] 93.87468 70.32374 48.99514\n\n[[6]]\n[1] 87.18379 67.19052 39.36560 37.94350 42.20495 48.93109 53.00394\n```\n\n\n:::\n:::\n\n\nThe last step in calculating the spatial lag is to sum that all up: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_lag <- vapply(xj_wt, sum, numeric(1))\nhead(x_lag)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 327.1065 517.6696 208.9709 303.1969 213.1936 375.8234\n```\n\n\n:::\n:::\n\nOr, simplified, that is: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(st_lag(x, nb, wt))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 327.1065 517.6696 208.9709 303.1969 213.1936 375.8234\n```\n\n\n:::\n:::\n\n\n## Stopping short of the spatial lag\n\nWe use the spatial lag to get a univariate estimate of the spatial neighborhoods value. We have a matrix of xj values that we multiply the spatial weights matrix and then perform a row summation.\n\nWhat if we _didn't_ perform that summation, and instead applied PCA onto the weighted values of `xj`? \n\nWe would then be encoding space and attributes into the components! In the below code chunk I am converting the spatial weights matrix to a sparse matrix and also creating a sparse matrix of the neighboring values. I am also scaling them. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- as(listw, \"CsparseMatrix\")\n\nxj <- as(\n  nb2listw(\n    nb, find_xj(scale(x), nb), style = \"B\"\n  ),\n  \"CsparseMatrix\"\n)\n```\n:::\n\n\nNow, we multiply them and perform PCA on the resultant matrix: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_comp <- function(comp) {\n    ggplot(guerry, aes(fill = comp)) +\n    geom_sf(color = \"black\", lwd = 0.2) +\n    scale_fill_viridis_c() +\n    theme_void() +\n    theme(legend.position = \"none\")\n}\n```\n:::\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\ncomps <- prcomp_irlba(xj * m, 4)\n\nplot_comp(comps$rotation[,1])\n```\n\n::: {.cell-output-display}\n![](2024-05-22-multivariate-mem_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot_comp(comps$rotation[,2])\n```\n\n::: {.cell-output-display}\n![](2024-05-22-multivariate-mem_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplot_comp(comps$rotation[,3])\n```\n\n::: {.cell-output-display}\n![](2024-05-22-multivariate-mem_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n\n```{.r .cell-code}\nplot_comp(comps$rotation[,4])\n```\n\n::: {.cell-output-display}\n![](2024-05-22-multivariate-mem_files/figure-html/unnamed-chunk-10-4.png){width=672}\n:::\n:::\n\n\nWe can see that there are some meaningful components in here! \n\nThe original variable's distribution: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(guerry, aes(fill = literacy)) +\n    geom_sf(color = \"black\", lwd = 0.2) +\n    scale_fill_viridis_c() +\n    labs(title = \"Literacy\") +\n    theme_void()\n```\n\n::: {.cell-output-display}\n![](2024-05-22-multivariate-mem_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nWe can see that there is some resemblance to original variable in these components. \n\n## Do they exhibit spatial autocorrelation?\n\nYes, yes they do! \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncomps_autocorr <- function(comps) {\n    apply(comps$rotation, 2, function(.x) {\n        broom::tidy(global_moran_test(.x, nb, wt))\n        }, simplify = FALSE) |>\n        dplyr::bind_rows(.id = \"component\") |>\n        dplyr::mutate(\n            estimate1 = round(estimate1, 3), p.value = rstatix::p_format(p.value)\n        ) |>\n        dplyr::select(component, \"Moran's I\" = estimate1, p_val = p.value) \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncomps_autocorr(comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 Ã— 3\n  component `Moran's I` p_val  \n  <chr>           <dbl> <chr>  \n1 PC1             0.689 <0.0001\n2 PC2             0.611 <0.0001\n3 PC3             0.409 <0.0001\n4 PC4             0.594 <0.0001\n```\n\n\n:::\n:::\n\n\n\nHow do we use this, though? That is the part I am not so clear on. We have multiple components and they all exhibit _significant_ autocorrelation. The applicability of each of these components may depend a lot upon what they are used to predict. The interpretation of them is tougher than identifying the applicability. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nregress_component <- function(z) {\n  vars <- setdiff(\n    colnames(guerry), \n    c(\"code_dept\", \"count\", \"region\", \"geometry\", \"area\", \"distance\", \"ave_id_geo\", \"main_city\", \"dept\", \"department\")\n  )\n  \n  lapply(vars, \\(var) {\n    broom::glance(summary(lm(guerry[[var]] ~ z)))\n  }) |> \n    setNames(vars) |> \n    dplyr::bind_rows(.id = \"variable\") |> \n    dplyr::arrange(-r.squared) |> \n    dplyr::select(variable, r.squared) |>\n    dplyr::filter(r.squared > 0.1)\n}\n```\n:::\n\n\nIf we regress our components upon the variables in the Guerry dataset we might be able to see which patterns they can help explain away. This helper function filters out regressions with an $R^2$ of less than 0.1.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nregress_component(comps$rotation[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 2\n  variable r.squared\n  <chr>        <dbl>\n1 literacy     0.136\n```\n\n\n:::\n\n```{.r .cell-code}\nregress_component(comps$rotation[,2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 Ã— 2\n  variable        r.squared\n  <chr>               <dbl>\n1 donation_clergy     0.276\n2 desertion           0.193\n3 instruction         0.132\n4 literacy            0.121\n5 clergy              0.112\n```\n\n\n:::\n\n```{.r .cell-code}\nregress_component(comps$rotation[,3])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 2\n  variable      r.squared\n  <chr>             <dbl>\n1 crime_parents     0.114\n```\n\n\n:::\n\n```{.r .cell-code}\nregress_component(comps$rotation[,4])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 2\n  variable    r.squared\n  <chr>           <dbl>\n1 prostitutes     0.319\n2 wealth          0.189\n```\n\n\n:::\n:::\n\n\nInterestingly the 3rd and 4th components are the most useful if we were looking for something to predict uponâ€”this is a derived use case where we're fishing for something that looks good. \n\n## Univariate Spatial Attribute Comopnent w/ Regression\n\nLet's explore this 4rd component a bit more. If we regress `prostitutes ~ literacy` we see that there is a much weaker model. Surprisingly, in fact. And the residuals are only very mildly autocorrelated. Why is the component such a strong predictor????\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(prostitutes ~ literacy, data = guerry)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = prostitutes ~ literacy, data = guerry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-415.6 -122.3  -40.5   57.9 4314.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -208.002    134.721  -1.544  0.12641   \nliteracy       8.981      3.147   2.854  0.00545 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 502.9 on 83 degrees of freedom\nMultiple R-squared:  0.08935,\tAdjusted R-squared:  0.07838 \nF-statistic: 8.144 on 1 and 83 DF,  p-value: 0.005455\n```\n\n\n:::\n\n```{.r .cell-code}\nglobal_moran_test(resid(mod), nb, wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 5.0023, p-value = 2.832e-07\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.142390596      -0.011904762       0.000951401 \n```\n\n\n:::\n:::\n\n\nAdding in the component to the model, the $R^2$ shoots right up! But why?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(prostitutes ~ literacy + comps$rotation[,4], data = guerry)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = prostitutes ~ literacy + comps$rotation[, 4], data = guerry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-766.7 -103.4  -65.6   36.5 3210.5 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -165.523    113.683  -1.456    0.149    \nliteracy               6.326      2.688   2.353    0.021 *  \ncomps$rotation[, 4] 2604.594    440.074   5.919 7.25e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 423.5 on 82 degrees of freedom\nMultiple R-squared:  0.3619,\tAdjusted R-squared:  0.3464 \nF-statistic: 23.26 on 2 and 82 DF,  p-value: 9.995e-09\n```\n\n\n:::\n\n```{.r .cell-code}\nglobal_moran_test(resid(mod), nb, wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 2.3311, p-value = 0.009874\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.100827937      -0.011904762       0.002338716 \n```\n\n\n:::\n:::\n\n\nOften when there is a spatial effect of a variable, we utilize its spatial lag in the model. This is an SLX model but simplified.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(prostitutes ~ literacy + st_lag(x, nb, wt), data = guerry)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = prostitutes ~ literacy + st_lag(x, nb, wt), data = guerry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-500.7 -138.6  -29.5   54.1 4237.2 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)   \n(Intercept)       -155.2878   149.8723  -1.036  0.30319   \nliteracy            10.7142     3.8112   2.811  0.00617 **\nst_lag(x, nb, wt)   -0.3858     0.4764  -0.810  0.42034   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 503.9 on 82 degrees of freedom\nMultiple R-squared:  0.09657,\tAdjusted R-squared:  0.07454 \nF-statistic: 4.383 on 2 and 82 DF,  p-value: 0.01554\n```\n\n\n:::\n\n```{.r .cell-code}\nglobal_moran_test(resid(mod), nb, wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 5.2839, p-value = 6.322e-08\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.167643014      -0.011904762       0.001154633 \n```\n\n\n:::\n:::\n\n\nIs this all autocorrelation? What if we look at including just a spatial component? What is interesting is that if we recreate this process using _only_ the Moran's Eigenvectors, there is nothing meaningful to be extracted that predicts prostitution as well as the spatial univariate component! \n\n\n::: {.cell layout-ncol=\"4\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nsp_comps <- irlba::prcomp_irlba(\n  scale(scale(m, T, T), T, F), 4\n)\n\ncomps_autocorr(sp_comps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 Ã— 3\n  component `Moran's I` p_val  \n  <chr>           <dbl> <chr>  \n1 PC1             1.04  <0.0001\n2 PC2             0.981 <0.0001\n3 PC3             0.934 <0.0001\n4 PC4             0.889 <0.0001\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nregress_component(sp_comps$rotation[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 Ã— 2\n   variable        r.squared\n   <chr>               <dbl>\n 1 desertion           0.366\n 2 suicides            0.258\n 3 crime_pers          0.203\n 4 literacy            0.197\n 5 crime_prop          0.178\n 6 instruction         0.171\n 7 wealth              0.158\n 8 donation_clergy     0.155\n 9 lottery             0.143\n10 commerce            0.138\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nregress_component(sp_comps$rotation[,2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 Ã— 2\n  variable    r.squared\n  <chr>           <dbl>\n1 literacy        0.256\n2 instruction     0.230\n3 donations       0.160\n4 commerce        0.151\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nregress_component(sp_comps$rotation[,3])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 2\n  variable    r.squared\n  <chr>           <dbl>\n1 instruction     0.142\n2 literacy        0.121\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nregress_component(sp_comps$rotation[,4])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 0 Ã— 2\n# â„¹ 2 variables: variable <chr>, r.squared <dbl>\n```\n\n\n:::\n:::\n",
    "supporting": [
      "2024-05-22-multivariate-mem_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}